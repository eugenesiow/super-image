{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"super-image State-of-the-art image super resolution models for PyTorch. Installation # With pip : pip install super-image Quick Start # Quickly utilise pre-trained models for upscaling your images 2x, 3x and 4x. See the full list of models below . from super_image import EdsrModel , ImageLoader from PIL import Image import requests url = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg' image = Image . open ( requests . get ( url , stream = True ) . raw ) model = EdsrModel . from_pretrained ( 'eugenesiow/edsr-base' , scale = 2 ) inputs = ImageLoader . load_image ( image ) preds = model ( inputs ) ImageLoader . save_image ( preds , './scaled_2x.png' ) ImageLoader . save_compare ( inputs , preds , './scaled_2x_compare.png' ) Pre-trained Models # Pre-trained models are available at various scales and hosted at the awesome huggingface_hub . By default the models were pretrained on DIV2K , a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of 100 validation images (images numbered 801 to 900). The leaderboard below shows the PSNR / SSIM metrics for each model at various scales on various test sets ( Set5 , Set14 , BSD100 , Urban100 ). The higher the better . All training was to 1000 epochs (some publications, like a2n, train to >1000 epochs in their experiments). Scale x2 # Rank Model Params Set5 Set14 BSD100 Urban100 1 msrn 5.9m 38.08/0.9609 33.75/0.9183 33.82/0.9258 32.14/0.9287 2 mdsr 2.7m 38.04/0.9608 33.71/0.9184 33.79/0.9256 32.14/0.9283 3 msrn-bam 5.9m 38.02/0.9608 33.73/0.9186 33.78/0.9253 32.08/0.9276 4 edsr-base 1.5m 38.02/0.9607 33.66/0.9180 33.77/0.9254 32.04/0.9276 5 mdsr-bam 2.7m 38/0.9607 33.68/0.9182 33.77/0.9253 32.04/0.9272 6 awsrn-bam 1.4m 37.99/0.9606 33.66/0.918 33.76/0.9253 31.95/0.9265 7 a2n 1.0m 37.87/0.9602 33.54/0.9171 33.67/0.9244 31.71/0.9240 8 carn 1.6m 37.89/0.9602 33.53/0.9173 33.66/0.9242 31.62/0.9229 9 carn-bam 1.6m 37.83/0.96 33.51/0.9166 33.64/0.924 31.53/0.922 10 pan 260k 37.77/0.9599 33.42/0.9162 33.6/0.9235 31.31/0.9197 11 pan-bam 260k 37.7/0.9596 33.4/0.9161 33.6/0.9234 31.35/0.92 Scale x3 # Rank Model Params Set5 Set14 BSD100 Urban100 1 msrn 6.1m 35.12/0.9409 31.08/0.8593 29.67/0.8198 29.31/0.8743 2 mdsr 2.9m 35.11/0.9406 31.06/0.8593 29.66/0.8196 29.29/0.8738 3 msrn-bam 5.9m 35.13/0.9408 31.06/0.8588 29.65/0.8196 29.26/0.8736 4 mdsr-bam 2.9m 35.07/0.9402 31.04/0.8582 29.62/0.8188 29.16/0.8717 5 edsr-base 1.5m 35.01/0.9402 31.01/0.8583 29.63/0.8190 29.19/0.8722 6 awsrn-bam 1.5m 35.05/0.9403 31.01/0.8581 29.63/0.8188 29.14/0.871 7 carn 1.6m 34.88/0.9391 30.93/0.8566 29.56/0.8173 28.95/0.867 8 a2n 1.0m 34.8/0.9387 30.94/0.8568 29.56/0.8173 28.95/0.8671 9 carn-bam 1.6m 34.82/0.9385 30.9/0.8558 29.54/0.8166 28.84/0.8648 10 pan-bam 260k 34.62/0.9371 30.83/0.8545 29.47/0.8153 28.64/0.861 11 pan 260k 34.64/0.9376 30.8/0.8544 29.47/0.815 28.61/0.8603 Scale x4 # Rank Model Params Set5 Set14 BSD100 Urban100 1 msrn 6.1m 32.19/0.8951 28.78/0.7862 28.53/0.7657 26.12/0.7866 2 msrn-bam 5.9m 32.26/0.8955 28.78/0.7859 28.51/0.7651 26.10/0.7857 3 mdsr 2.8m 32.26/0.8953 28.77/0.7856 28.53/0.7653 26.07/0.7851 4 mdsr-bam 2.9m 32.19/0.8949 28.73/0.7847 28.50/0.7645 26.02/0.7834 5 awsrn-bam 1.6m 32.13/0.8947 28.75/0.7851 28.51/0.7647 26.03/0.7838 6 edsr-base 1.5m 32.12/0.8947 28.72/0.7845 28.50/0.7644 26.02/0.7832 7 a2n 1.0m 32.07/0.8933 28.68/0.7830 28.44/0.7624 25.89/0.7787 8 carn 1.6m 32.05/0.8931 28.67/0.7828 28.44/0.7625 25.85/0.7768 9 carn-bam 1.6m 32.0/0.8923 28.62/0.7822 28.41/0.7614 25.77/0.7741 10 pan 270k 31.92/0.8915 28.57/0.7802 28.35/0.7595 25.63/0.7692 11 pan-bam 270k 31.9/0.8911 28.54/0.7795 28.32/0.7591 25.6/0.7691 You can find a notebook to easily run evaluation on pretrained models below: Train Models # We need the huggingface datasets library to download the data: pip install datasets The following code gets the data and preprocesses/augments the data. from datasets import load_dataset from super_image.data import EvalDataset , TrainDataset , augment_five_crop augmented_dataset = load_dataset ( 'eugenesiow/Div2k' , 'bicubic_x4' , split = 'train' ) \\ . map ( augment_five_crop , batched = True , desc = \"Augmenting Dataset\" ) # download and augment the data with the five_crop method train_dataset = TrainDataset ( augmented_dataset ) # prepare the train dataset for loading PyTorch DataLoader eval_dataset = EvalDataset ( load_dataset ( 'eugenesiow/Div2k' , 'bicubic_x4' , split = 'validation' )) # prepare the eval dataset for the PyTorch DataLoader The training code is provided below: from super_image import Trainer , TrainingArguments , EdsrModel , EdsrConfig training_args = TrainingArguments ( output_dir = './results' , # output directory num_train_epochs = 1000 , # total number of training epochs ) config = EdsrConfig ( scale = 4 , # train a model to upscale 4x ) model = EdsrModel ( config ) trainer = Trainer ( model = model , # the instantiated model to be trained args = training_args , # training arguments, defined above train_dataset = train_dataset , # training dataset eval_dataset = eval_dataset # evaluation dataset ) trainer . train ()","title":"Overview"},{"location":"#installation","text":"With pip : pip install super-image","title":"Installation"},{"location":"#quick-start","text":"Quickly utilise pre-trained models for upscaling your images 2x, 3x and 4x. See the full list of models below . from super_image import EdsrModel , ImageLoader from PIL import Image import requests url = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg' image = Image . open ( requests . get ( url , stream = True ) . raw ) model = EdsrModel . from_pretrained ( 'eugenesiow/edsr-base' , scale = 2 ) inputs = ImageLoader . load_image ( image ) preds = model ( inputs ) ImageLoader . save_image ( preds , './scaled_2x.png' ) ImageLoader . save_compare ( inputs , preds , './scaled_2x_compare.png' )","title":"Quick Start"},{"location":"#pre-trained-models","text":"Pre-trained models are available at various scales and hosted at the awesome huggingface_hub . By default the models were pretrained on DIV2K , a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of 100 validation images (images numbered 801 to 900). The leaderboard below shows the PSNR / SSIM metrics for each model at various scales on various test sets ( Set5 , Set14 , BSD100 , Urban100 ). The higher the better . All training was to 1000 epochs (some publications, like a2n, train to >1000 epochs in their experiments).","title":"Pre-trained Models"},{"location":"#scale-x2","text":"Rank Model Params Set5 Set14 BSD100 Urban100 1 msrn 5.9m 38.08/0.9609 33.75/0.9183 33.82/0.9258 32.14/0.9287 2 mdsr 2.7m 38.04/0.9608 33.71/0.9184 33.79/0.9256 32.14/0.9283 3 msrn-bam 5.9m 38.02/0.9608 33.73/0.9186 33.78/0.9253 32.08/0.9276 4 edsr-base 1.5m 38.02/0.9607 33.66/0.9180 33.77/0.9254 32.04/0.9276 5 mdsr-bam 2.7m 38/0.9607 33.68/0.9182 33.77/0.9253 32.04/0.9272 6 awsrn-bam 1.4m 37.99/0.9606 33.66/0.918 33.76/0.9253 31.95/0.9265 7 a2n 1.0m 37.87/0.9602 33.54/0.9171 33.67/0.9244 31.71/0.9240 8 carn 1.6m 37.89/0.9602 33.53/0.9173 33.66/0.9242 31.62/0.9229 9 carn-bam 1.6m 37.83/0.96 33.51/0.9166 33.64/0.924 31.53/0.922 10 pan 260k 37.77/0.9599 33.42/0.9162 33.6/0.9235 31.31/0.9197 11 pan-bam 260k 37.7/0.9596 33.4/0.9161 33.6/0.9234 31.35/0.92","title":"Scale x2"},{"location":"#scale-x3","text":"Rank Model Params Set5 Set14 BSD100 Urban100 1 msrn 6.1m 35.12/0.9409 31.08/0.8593 29.67/0.8198 29.31/0.8743 2 mdsr 2.9m 35.11/0.9406 31.06/0.8593 29.66/0.8196 29.29/0.8738 3 msrn-bam 5.9m 35.13/0.9408 31.06/0.8588 29.65/0.8196 29.26/0.8736 4 mdsr-bam 2.9m 35.07/0.9402 31.04/0.8582 29.62/0.8188 29.16/0.8717 5 edsr-base 1.5m 35.01/0.9402 31.01/0.8583 29.63/0.8190 29.19/0.8722 6 awsrn-bam 1.5m 35.05/0.9403 31.01/0.8581 29.63/0.8188 29.14/0.871 7 carn 1.6m 34.88/0.9391 30.93/0.8566 29.56/0.8173 28.95/0.867 8 a2n 1.0m 34.8/0.9387 30.94/0.8568 29.56/0.8173 28.95/0.8671 9 carn-bam 1.6m 34.82/0.9385 30.9/0.8558 29.54/0.8166 28.84/0.8648 10 pan-bam 260k 34.62/0.9371 30.83/0.8545 29.47/0.8153 28.64/0.861 11 pan 260k 34.64/0.9376 30.8/0.8544 29.47/0.815 28.61/0.8603","title":"Scale x3"},{"location":"#scale-x4","text":"Rank Model Params Set5 Set14 BSD100 Urban100 1 msrn 6.1m 32.19/0.8951 28.78/0.7862 28.53/0.7657 26.12/0.7866 2 msrn-bam 5.9m 32.26/0.8955 28.78/0.7859 28.51/0.7651 26.10/0.7857 3 mdsr 2.8m 32.26/0.8953 28.77/0.7856 28.53/0.7653 26.07/0.7851 4 mdsr-bam 2.9m 32.19/0.8949 28.73/0.7847 28.50/0.7645 26.02/0.7834 5 awsrn-bam 1.6m 32.13/0.8947 28.75/0.7851 28.51/0.7647 26.03/0.7838 6 edsr-base 1.5m 32.12/0.8947 28.72/0.7845 28.50/0.7644 26.02/0.7832 7 a2n 1.0m 32.07/0.8933 28.68/0.7830 28.44/0.7624 25.89/0.7787 8 carn 1.6m 32.05/0.8931 28.67/0.7828 28.44/0.7625 25.85/0.7768 9 carn-bam 1.6m 32.0/0.8923 28.62/0.7822 28.41/0.7614 25.77/0.7741 10 pan 270k 31.92/0.8915 28.57/0.7802 28.35/0.7595 25.63/0.7692 11 pan-bam 270k 31.9/0.8911 28.54/0.7795 28.32/0.7591 25.6/0.7691 You can find a notebook to easily run evaluation on pretrained models below:","title":"Scale x4"},{"location":"#train-models","text":"We need the huggingface datasets library to download the data: pip install datasets The following code gets the data and preprocesses/augments the data. from datasets import load_dataset from super_image.data import EvalDataset , TrainDataset , augment_five_crop augmented_dataset = load_dataset ( 'eugenesiow/Div2k' , 'bicubic_x4' , split = 'train' ) \\ . map ( augment_five_crop , batched = True , desc = \"Augmenting Dataset\" ) # download and augment the data with the five_crop method train_dataset = TrainDataset ( augmented_dataset ) # prepare the train dataset for loading PyTorch DataLoader eval_dataset = EvalDataset ( load_dataset ( 'eugenesiow/Div2k' , 'bicubic_x4' , split = 'validation' )) # prepare the eval dataset for the PyTorch DataLoader The training code is provided below: from super_image import Trainer , TrainingArguments , EdsrModel , EdsrConfig training_args = TrainingArguments ( output_dir = './results' , # output directory num_train_epochs = 1000 , # total number of training epochs ) config = EdsrConfig ( scale = 4 , # train a model to upscale 4x ) model = EdsrModel ( config ) trainer = Trainer ( model = model , # the instantiated model to be trained args = training_args , # training arguments, defined above train_dataset = train_dataset , # training dataset eval_dataset = eval_dataset # evaluation dataset ) trainer . train ()","title":"Train Models"},{"location":"changelog/","text":"Changelog # All notable changes to this project will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning . [Unreleased] # Added # [0.1.6] - 2021-09-06 # Updated # Update EdsrModel to fix no_upsampling bug where self.args was not stored. [0.1.5] - 2021-08-30 # Added # New model: PhysicssrModel New model: DrnModel New model: HanModel New model: AwsrnModel [0.1.4] - 2021-08-17 # Added # calculate_mean_std(dataset) to ~super_image.utils.metrics for calculating RGB pixel mean and standard deviation over a dataset. New model: DrlnModel New model: RcanModel New model: MdsrModel New model: CarnModel New model: PanModel Updated # Update EdsrModel to include no_upsampling option so it can be reused for JiifModel . [0.1.3] - 2021-07-28 # Added # TrainDataset(dataset) with support for huggingface datasets. augment_five_crop function for use with dataset.map(augment_five_crop, batched=True) [0.1.2] - 2021-07-26 # Added # Added metrics EvalMetrics().evaluate(model, eval_dataset) class to calculate PSNR and SSIM on a model and evaluation dataset. Accepts EvalDataset(dataset) with huggingface datasets. Changed # Replaced EvalDataset(dataset) to use huggingface datasets instead of H5 files. Fixed EvalDataset(dataset) to be robust to wrongly sized HR images (not equals to scaled LR image size).","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#unreleased","text":"","title":"[Unreleased]"},{"location":"changelog/#added","text":"","title":"Added"},{"location":"changelog/#016-2021-09-06","text":"","title":"[0.1.6] - 2021-09-06"},{"location":"changelog/#updated","text":"Update EdsrModel to fix no_upsampling bug where self.args was not stored.","title":"Updated"},{"location":"changelog/#015-2021-08-30","text":"","title":"[0.1.5] - 2021-08-30"},{"location":"changelog/#added_1","text":"New model: PhysicssrModel New model: DrnModel New model: HanModel New model: AwsrnModel","title":"Added"},{"location":"changelog/#014-2021-08-17","text":"","title":"[0.1.4] - 2021-08-17"},{"location":"changelog/#added_2","text":"calculate_mean_std(dataset) to ~super_image.utils.metrics for calculating RGB pixel mean and standard deviation over a dataset. New model: DrlnModel New model: RcanModel New model: MdsrModel New model: CarnModel New model: PanModel","title":"Added"},{"location":"changelog/#updated_1","text":"Update EdsrModel to include no_upsampling option so it can be reused for JiifModel .","title":"Updated"},{"location":"changelog/#013-2021-07-28","text":"","title":"[0.1.3] - 2021-07-28"},{"location":"changelog/#added_3","text":"TrainDataset(dataset) with support for huggingface datasets. augment_five_crop function for use with dataset.map(augment_five_crop, batched=True)","title":"Added"},{"location":"changelog/#012-2021-07-26","text":"","title":"[0.1.2] - 2021-07-26"},{"location":"changelog/#added_4","text":"Added metrics EvalMetrics().evaluate(model, eval_dataset) class to calculate PSNR and SSIM on a model and evaluation dataset. Accepts EvalDataset(dataset) with huggingface datasets.","title":"Added"},{"location":"changelog/#changed","text":"Replaced EvalDataset(dataset) to use huggingface datasets instead of H5 files. Fixed EvalDataset(dataset) to be robust to wrongly sized HR images (not equals to scaled LR image size).","title":"Changed"},{"location":"code_of_conduct/","text":"Contributor Covenant Code of Conduct # Our Pledge # In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards # Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities # Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope # This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement # Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at kyo116@gmail.com . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution # This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at http://contributor-covenant.org/version/1/4","title":"Code of Conduct"},{"location":"code_of_conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"code_of_conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"code_of_conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"code_of_conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"code_of_conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"code_of_conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at kyo116@gmail.com . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"code_of_conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at http://contributor-covenant.org/version/1/4","title":"Attribution"},{"location":"contributing/","text":"Contributing # Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. Environment setup # Nothing easier! Fork and clone the repository, then: cd super-image make setup Note If it fails for some reason, you'll need to install Poetry manually. You can install it with: python3 -m pip install --user pipx pipx install poetry Now you can try running make setup again, or simply poetry install . You now have the dependencies installed. You can run the application with poetry run super-image [ARGS...] . Run make help to see all the available actions! Tasks # This project uses duty to run tasks. A Makefile is also provided. The Makefile will try to run certain tasks on multiple Python versions. If for some reason you don't want to run the task on multiple Python versions, you can do one of the following: export PYTHON_VERSIONS= : this will run the task with only the current Python version run the task directly with poetry run duty TASK , or duty TASK if the environment was already activated through poetry shell The Makefile detects if the Poetry environment is activated, so make will work the same with the virtualenv activated or not. Development # As usual: create a new branch: git checkout -b feature-or-bugfix-name edit the code and/or the documentation If you updated the documentation or the project dependencies: run make docs-regen run make docs-serve , go to http://localhost:8000 and check that everything looks good Before committing: run make format to auto-format the code run make check to check everything (fix any warning) run make test to run the tests (fix any issue) follow our commit message convention If you are unsure about how to fix or ignore a warning, just let the continuous integration fail, and we will help you during review. Don't bother updating the changelog, we will take care of this. Commit message convention # Commits messages must follow the Angular style : <type>[(scope)]: Subject [Body] Scope and body are optional. Type can be: build : About packaging, building wheels, etc. chore : About packaging or repo/files management. ci : About Continuous Integration. docs : About documentation. feat : New feature. fix : Bug fix. perf : About performance. refactor : Changes which are not features nor bug fixes. style : A change in code style/format. tests : About tests. Subject (and body) must be valid Markdown. If you write a body, please add issues references at the end: Body. References: #10, #11. Fixes #15. Pull requests guidelines # Link to any related issue in the Pull Request message. During review, we recommend using fixups: # SHA is the SHA of the commit you want to fix git commit --fixup = SHA Once all the changes are approved, you can squash your commits: git rebase -i --autosquash master And force-push: git push -f If this seems all too complicated, you can push or force-push each new commit, and we will squash them ourselves if needed, before merging.","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.","title":"Contributing"},{"location":"contributing/#environment-setup","text":"Nothing easier! Fork and clone the repository, then: cd super-image make setup Note If it fails for some reason, you'll need to install Poetry manually. You can install it with: python3 -m pip install --user pipx pipx install poetry Now you can try running make setup again, or simply poetry install . You now have the dependencies installed. You can run the application with poetry run super-image [ARGS...] . Run make help to see all the available actions!","title":"Environment setup"},{"location":"contributing/#tasks","text":"This project uses duty to run tasks. A Makefile is also provided. The Makefile will try to run certain tasks on multiple Python versions. If for some reason you don't want to run the task on multiple Python versions, you can do one of the following: export PYTHON_VERSIONS= : this will run the task with only the current Python version run the task directly with poetry run duty TASK , or duty TASK if the environment was already activated through poetry shell The Makefile detects if the Poetry environment is activated, so make will work the same with the virtualenv activated or not.","title":"Tasks"},{"location":"contributing/#development","text":"As usual: create a new branch: git checkout -b feature-or-bugfix-name edit the code and/or the documentation If you updated the documentation or the project dependencies: run make docs-regen run make docs-serve , go to http://localhost:8000 and check that everything looks good Before committing: run make format to auto-format the code run make check to check everything (fix any warning) run make test to run the tests (fix any issue) follow our commit message convention If you are unsure about how to fix or ignore a warning, just let the continuous integration fail, and we will help you during review. Don't bother updating the changelog, we will take care of this.","title":"Development"},{"location":"contributing/#commit-message-convention","text":"Commits messages must follow the Angular style : <type>[(scope)]: Subject [Body] Scope and body are optional. Type can be: build : About packaging, building wheels, etc. chore : About packaging or repo/files management. ci : About Continuous Integration. docs : About documentation. feat : New feature. fix : Bug fix. perf : About performance. refactor : Changes which are not features nor bug fixes. style : A change in code style/format. tests : About tests. Subject (and body) must be valid Markdown. If you write a body, please add issues references at the end: Body. References: #10, #11. Fixes #15.","title":"Commit message convention"},{"location":"contributing/#pull-requests-guidelines","text":"Link to any related issue in the Pull Request message. During review, we recommend using fixups: # SHA is the SHA of the commit you want to fix git commit --fixup = SHA Once all the changes are approved, you can squash your commits: git rebase -i --autosquash master And force-push: git push -f If this seems all too complicated, you can push or force-push each new commit, and we will squash them ourselves if needed, before merging.","title":"Pull requests guidelines"},{"location":"evaluation/","text":"Evaluation # Evaluate pretrained super-image models with common image super resolution datasets. Setting up the Environment # Install the library # We will install the super-image and huggingface datasets library using pip install . pip install -qq datasets super-image Loading the dataset # We download the Set5 dataset using the huggingface datasets library. Note that you can change bicubic_x2 to any of [ bicubic_x2 , bicubic_x3 or bicubic_x4 ]. You can also explore more super resolution datasets here . from datasets import load_dataset dataset = load_dataset ( 'eugenesiow/Set5' , 'bicubic_x2' , split = 'validation' ) If you want to preview the first image (high resolution and the low resolution (half sized) images) from the dataset. import cv2 cv2 . imshow ( cv2 . imread ( dataset [ 0 ][ \"hr\" ])) cv2 . imshow ( cv2 . imread ( dataset [ 0 ][ \"lr\" ])) Evaluating the Model (Running Inference) # To evaluate the a model for the PSNR and SSIM metrics we run the following code: EvalDataset(dataset) converts the dataset to an evaluation dataset that can be fed in to a PyTorch dataloader. EdsrModel.from_pretrained - Download and load a small, pre-trained deep-learning model to the model variable. You can replace this with other pretrained models. EvalMetrics().evaluate(model, eval_dataset) - Run the evaluation on the eval_dataset using the model . from super_image import EdsrModel from super_image.data import EvalDataset , EvalMetrics eval_dataset = EvalDataset ( dataset ) model = EdsrModel . from_pretrained ( 'eugenesiow/edsr-base' , scale = 2 ) EvalMetrics () . evaluate ( model , eval_dataset ) We can see from the output that the PSNR for this model on this dataset is 38.02 and the SSIM is 0.9607 . Try Other Models and Datasets # You can replace the EdsrModel with other pretrained models. You can replace the Set5 dataset with other datasets here . You can try different scales: bicubic_x2 , bicubic_x3 or bicubic_x4 Compare the performance via the leaderboard .","title":"Evaluation"},{"location":"evaluation/#evaluation","text":"Evaluate pretrained super-image models with common image super resolution datasets.","title":"Evaluation"},{"location":"evaluation/#setting-up-the-environment","text":"","title":"Setting up the Environment"},{"location":"evaluation/#install-the-library","text":"We will install the super-image and huggingface datasets library using pip install . pip install -qq datasets super-image","title":"Install the library"},{"location":"evaluation/#loading-the-dataset","text":"We download the Set5 dataset using the huggingface datasets library. Note that you can change bicubic_x2 to any of [ bicubic_x2 , bicubic_x3 or bicubic_x4 ]. You can also explore more super resolution datasets here . from datasets import load_dataset dataset = load_dataset ( 'eugenesiow/Set5' , 'bicubic_x2' , split = 'validation' ) If you want to preview the first image (high resolution and the low resolution (half sized) images) from the dataset. import cv2 cv2 . imshow ( cv2 . imread ( dataset [ 0 ][ \"hr\" ])) cv2 . imshow ( cv2 . imread ( dataset [ 0 ][ \"lr\" ]))","title":"Loading the dataset"},{"location":"evaluation/#evaluating-the-model-running-inference","text":"To evaluate the a model for the PSNR and SSIM metrics we run the following code: EvalDataset(dataset) converts the dataset to an evaluation dataset that can be fed in to a PyTorch dataloader. EdsrModel.from_pretrained - Download and load a small, pre-trained deep-learning model to the model variable. You can replace this with other pretrained models. EvalMetrics().evaluate(model, eval_dataset) - Run the evaluation on the eval_dataset using the model . from super_image import EdsrModel from super_image.data import EvalDataset , EvalMetrics eval_dataset = EvalDataset ( dataset ) model = EdsrModel . from_pretrained ( 'eugenesiow/edsr-base' , scale = 2 ) EvalMetrics () . evaluate ( model , eval_dataset ) We can see from the output that the PSNR for this model on this dataset is 38.02 and the SSIM is 0.9607 .","title":"Evaluating the Model (Running Inference)"},{"location":"evaluation/#try-other-models-and-datasets","text":"You can replace the EdsrModel with other pretrained models. You can replace the Set5 dataset with other datasets here . You can try different scales: bicubic_x2 , bicubic_x3 or bicubic_x4 Compare the performance via the leaderboard .","title":"Try Other Models and Datasets"},{"location":"license/","text":"Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"prediction/","text":"Prediction # Use the super-image library to quickly upscale an image. Setting up the Environment # Install the library # We will install the super-image using pip install . pip install -qq super-image Load a Pretrained Model for Inference # Next we run a few lines of code to: Image.open and requests.get - Download an image from a URL (website) and store this as the image variable. EdsrModel.from_pretrained - Download and load a small, pre-trained deep-learning model to the model variable. ImageLoader.load_image - Load the image into the model using the ImageLoader helper. Use the model to run inference on the image ( inputs ). ImageLoader.save_image - Save the upscaled image output as a .png file using the ImageLoader helper. ImageLoader.save_compare - Save a .png that compares our upscaled image from the model with a baseline image using Bicubic upscaling. from super_image import EdsrModel , ImageLoader from PIL import Image import requests url = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg' image = Image . open ( requests . get ( url , stream = True ) . raw ) model = EdsrModel . from_pretrained ( 'eugenesiow/edsr-base' , scale = 2 ) inputs = ImageLoader . load_image ( image ) preds = model ( inputs ) ImageLoader . save_image ( preds , './scaled_2x.png' ) ImageLoader . save_compare ( inputs , preds , './scaled_2x_compare.png' ) View the comparison image to see, visually, how our model performed (on the right) against the baseline bicubic method (left). import cv2 img = cv2 . imread ( './scaled_2x_compare.png' ) cv2 . imshow ( img ) We can view the original image that we pulled from the URL/website using cv2.imshow . import numpy as np cv2 . imshow ( cv2 . cvtColor ( np . array ( image ), cv2 . COLOR_BGR2RGB )) Try Other Models # You can replace the EdsrModel with other pretrained models. You can try different scales: bicubic_x2 , bicubic_x3 or bicubic_x4 Compare the performance via the leaderboard .","title":"Prediction"},{"location":"prediction/#prediction","text":"Use the super-image library to quickly upscale an image.","title":"Prediction"},{"location":"prediction/#setting-up-the-environment","text":"","title":"Setting up the Environment"},{"location":"prediction/#install-the-library","text":"We will install the super-image using pip install . pip install -qq super-image","title":"Install the library"},{"location":"prediction/#load-a-pretrained-model-for-inference","text":"Next we run a few lines of code to: Image.open and requests.get - Download an image from a URL (website) and store this as the image variable. EdsrModel.from_pretrained - Download and load a small, pre-trained deep-learning model to the model variable. ImageLoader.load_image - Load the image into the model using the ImageLoader helper. Use the model to run inference on the image ( inputs ). ImageLoader.save_image - Save the upscaled image output as a .png file using the ImageLoader helper. ImageLoader.save_compare - Save a .png that compares our upscaled image from the model with a baseline image using Bicubic upscaling. from super_image import EdsrModel , ImageLoader from PIL import Image import requests url = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg' image = Image . open ( requests . get ( url , stream = True ) . raw ) model = EdsrModel . from_pretrained ( 'eugenesiow/edsr-base' , scale = 2 ) inputs = ImageLoader . load_image ( image ) preds = model ( inputs ) ImageLoader . save_image ( preds , './scaled_2x.png' ) ImageLoader . save_compare ( inputs , preds , './scaled_2x_compare.png' ) View the comparison image to see, visually, how our model performed (on the right) against the baseline bicubic method (left). import cv2 img = cv2 . imread ( './scaled_2x_compare.png' ) cv2 . imshow ( img ) We can view the original image that we pulled from the URL/website using cv2.imshow . import numpy as np cv2 . imshow ( cv2 . cvtColor ( np . array ( image ), cv2 . COLOR_BGR2RGB ))","title":"Load a Pretrained Model for Inference"},{"location":"prediction/#try-other-models","text":"You can replace the EdsrModel with other pretrained models. You can try different scales: bicubic_x2 , bicubic_x3 or bicubic_x4 Compare the performance via the leaderboard .","title":"Try Other Models"},{"location":"training/","text":"Training # Train super-image models for image super resolution tasks. Setting up the Environment # Install the library # We will install the super-image and huggingface datasets library using pip install . pip install -qq datasets super-image Loading and Augmenting the Dataset # We download the Div2k dataset using the huggingface datasets library. You can explore more super resolution datasets here . We then follow the pre-processing and augmentation method of Wang et al. (2021) . This will take awhile, go grab a coffee. Note that you can change bicubic_x4 to any of [ bicubic_x2 , bicubic_x3 or bicubic_x4 ]. If you don't want to do augmentation to your dataset, you can just do: train_dataset = TrainDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')) If you want eval to be faster you can use the much smaller Set5 : eval_dataset = EvalDataset(load_dataset('eugenesiow/Set5', 'bicubic_x4', split='validation')) from datasets import load_dataset from super_image.data import EvalDataset , TrainDataset , augment_five_crop augmented_dataset = load_dataset ( 'eugenesiow/Div2k' , 'bicubic_x4' , split = 'train' ) \\ . map ( augment_five_crop , batched = True , desc = \"Augmenting Dataset\" ) # download and augment the data with the five_crop method train_dataset = TrainDataset ( augmented_dataset ) # prepare the train dataset for loading PyTorch DataLoader eval_dataset = EvalDataset ( load_dataset ( 'eugenesiow/Div2k' , 'bicubic_x4' , split = 'validation' )) # prepare the eval dataset for the PyTorch DataLoader Training the Model # We then train the model. It's best if you have a GPU. from super_image import Trainer , TrainingArguments , EdsrModel , EdsrConfig training_args = TrainingArguments ( output_dir = './results' , # output directory num_train_epochs = 1000 , # total number of training epochs ) config = EdsrConfig ( scale = 4 , # train a model to upscale 4x ) model = EdsrModel ( config ) trainer = Trainer ( model = model , # the instantiated model to be trained args = training_args , # training arguments, defined above train_dataset = train_dataset , # training dataset eval_dataset = eval_dataset # evaluation dataset ) trainer . train () We see that after each epoch of training, the PSNR and SSIM scores of the epoch on the validation set is reported. The best model after 1000 epochs is saved. Try Other Architectures # You can try the other architectures in super-image . Compare the performance via the leaderboard . View the various pretrained models on huggingface hub . Here is an example on another architecture, MSRN: from super_image import Trainer , TrainingArguments , MsrnModel , MsrnConfig training_args = TrainingArguments ( output_dir = './results_msrn' , # output directory num_train_epochs = 2 , # total number of training epochs ) config = MsrnConfig ( scale = 4 , # train a model to upscale 4x bam = True , # use balanced attention ) model = MsrnModel ( config ) trainer = Trainer ( model = model , # the instantiated model to be trained args = training_args , # training arguments, defined above train_dataset = train_dataset , # training dataset eval_dataset = eval_dataset # evaluation dataset ) trainer . train ()","title":"Training"},{"location":"training/#training","text":"Train super-image models for image super resolution tasks.","title":"Training"},{"location":"training/#setting-up-the-environment","text":"","title":"Setting up the Environment"},{"location":"training/#install-the-library","text":"We will install the super-image and huggingface datasets library using pip install . pip install -qq datasets super-image","title":"Install the library"},{"location":"training/#loading-and-augmenting-the-dataset","text":"We download the Div2k dataset using the huggingface datasets library. You can explore more super resolution datasets here . We then follow the pre-processing and augmentation method of Wang et al. (2021) . This will take awhile, go grab a coffee. Note that you can change bicubic_x4 to any of [ bicubic_x2 , bicubic_x3 or bicubic_x4 ]. If you don't want to do augmentation to your dataset, you can just do: train_dataset = TrainDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')) If you want eval to be faster you can use the much smaller Set5 : eval_dataset = EvalDataset(load_dataset('eugenesiow/Set5', 'bicubic_x4', split='validation')) from datasets import load_dataset from super_image.data import EvalDataset , TrainDataset , augment_five_crop augmented_dataset = load_dataset ( 'eugenesiow/Div2k' , 'bicubic_x4' , split = 'train' ) \\ . map ( augment_five_crop , batched = True , desc = \"Augmenting Dataset\" ) # download and augment the data with the five_crop method train_dataset = TrainDataset ( augmented_dataset ) # prepare the train dataset for loading PyTorch DataLoader eval_dataset = EvalDataset ( load_dataset ( 'eugenesiow/Div2k' , 'bicubic_x4' , split = 'validation' )) # prepare the eval dataset for the PyTorch DataLoader","title":"Loading and Augmenting the Dataset"},{"location":"training/#training-the-model","text":"We then train the model. It's best if you have a GPU. from super_image import Trainer , TrainingArguments , EdsrModel , EdsrConfig training_args = TrainingArguments ( output_dir = './results' , # output directory num_train_epochs = 1000 , # total number of training epochs ) config = EdsrConfig ( scale = 4 , # train a model to upscale 4x ) model = EdsrModel ( config ) trainer = Trainer ( model = model , # the instantiated model to be trained args = training_args , # training arguments, defined above train_dataset = train_dataset , # training dataset eval_dataset = eval_dataset # evaluation dataset ) trainer . train () We see that after each epoch of training, the PSNR and SSIM scores of the epoch on the validation set is reported. The best model after 1000 epochs is saved.","title":"Training the Model"},{"location":"training/#try-other-architectures","text":"You can try the other architectures in super-image . Compare the performance via the leaderboard . View the various pretrained models on huggingface hub . Here is an example on another architecture, MSRN: from super_image import Trainer , TrainingArguments , MsrnModel , MsrnConfig training_args = TrainingArguments ( output_dir = './results_msrn' , # output directory num_train_epochs = 2 , # total number of training epochs ) config = MsrnConfig ( scale = 4 , # train a model to upscale 4x bam = True , # use balanced attention ) model = MsrnModel ( config ) trainer = Trainer ( model = model , # the instantiated model to be trained args = training_args , # training arguments, defined above train_dataset = train_dataset , # training dataset eval_dataset = eval_dataset # evaluation dataset ) trainer . train ()","title":"Try Other Architectures"},{"location":"models/a2n/","text":"Attention in Attention Network for Image Super-Resolution (A2N) # Overview # The A2N model proposes an attention in attention network (A2N) for highly accurate image SR. Specifically, the A2N consists of a non-attention branch and a coupling attention branch. Attention dropout module is proposed to generate dynamic attention weights for these two branches based on input features that can suppress unwanted attention adjustments. This allows attention modules to specialize to beneficial examples without otherwise penalties and thus greatly improve the capacity of the attention network with little parameter overhead. More importantly the model is lightweight and fast to train (~1.5m parameters, ~4mb). It was introduced in the paper Attention in Attention Network for Image Super-Resolution by Chen et al. (2021) and first released in this repository . A2nConfig # A2nModel # forward ( self , x ) # Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in super_image\\models\\a2n\\modeling_a2n.py def forward ( self , x ): fea = self . conv_first ( x ) trunk = self . trunk_conv ( self . AAB_trunk ( fea )) fea = fea + trunk if self . scale == 2 or self . scale == 3 : fea = self . upconv1 ( functional . interpolate ( fea , scale_factor = self . scale , mode = 'nearest' )) fea = self . lrelu ( self . att1 ( fea )) fea = self . lrelu ( self . HRconv1 ( fea )) elif self . scale == 4 : fea = self . upconv1 ( functional . interpolate ( fea , scale_factor = 2 , mode = 'nearest' )) fea = self . lrelu ( self . att1 ( fea )) fea = self . lrelu ( self . HRconv1 ( fea )) fea = self . upconv2 ( functional . interpolate ( fea , scale_factor = 2 , mode = 'nearest' )) fea = self . lrelu ( self . att2 ( fea )) fea = self . lrelu ( self . HRconv2 ( fea )) out = self . conv_last ( fea ) ilr = functional . interpolate ( x , scale_factor = self . scale , mode = 'bilinear' , align_corners = False ) out = out + ilr return out load_state_dict ( self , state_dict , strict = True ) # Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys Source code in super_image\\models\\a2n\\modeling_a2n.py def load_state_dict ( self , state_dict , strict = True ): own_state = self . state_dict () for name , param in state_dict . items (): if name in own_state : if isinstance ( param , nn . Parameter ): param = param . data try : own_state [ name ] . copy_ ( param ) except Exception : if name . find ( 'tail' ) == - 1 : raise RuntimeError ( f 'While copying the parameter named { name } , ' f 'whose dimensions in the model are { own_state [ name ] . size () } and ' f 'whose dimensions in the checkpoint are { param . size () } .' ) elif strict : if name . find ( 'tail' ) == - 1 : raise KeyError ( f 'unexpected key \" { name } \" in state_dict' )","title":"A2N"},{"location":"models/a2n/#attention-in-attention-network-for-image-super-resolution-a2n","text":"","title":"Attention in Attention Network for Image Super-Resolution (A2N)"},{"location":"models/a2n/#overview","text":"The A2N model proposes an attention in attention network (A2N) for highly accurate image SR. Specifically, the A2N consists of a non-attention branch and a coupling attention branch. Attention dropout module is proposed to generate dynamic attention weights for these two branches based on input features that can suppress unwanted attention adjustments. This allows attention modules to specialize to beneficial examples without otherwise penalties and thus greatly improve the capacity of the attention network with little parameter overhead. More importantly the model is lightweight and fast to train (~1.5m parameters, ~4mb). It was introduced in the paper Attention in Attention Network for Image Super-Resolution by Chen et al. (2021) and first released in this repository .","title":"Overview"},{"location":"models/a2n/#a2nconfig","text":"","title":"A2nConfig"},{"location":"models/a2n/#a2nmodel","text":"","title":"A2nModel"},{"location":"models/a2n/#super_image.models.a2n.modeling_a2n.A2nModel.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in super_image\\models\\a2n\\modeling_a2n.py def forward ( self , x ): fea = self . conv_first ( x ) trunk = self . trunk_conv ( self . AAB_trunk ( fea )) fea = fea + trunk if self . scale == 2 or self . scale == 3 : fea = self . upconv1 ( functional . interpolate ( fea , scale_factor = self . scale , mode = 'nearest' )) fea = self . lrelu ( self . att1 ( fea )) fea = self . lrelu ( self . HRconv1 ( fea )) elif self . scale == 4 : fea = self . upconv1 ( functional . interpolate ( fea , scale_factor = 2 , mode = 'nearest' )) fea = self . lrelu ( self . att1 ( fea )) fea = self . lrelu ( self . HRconv1 ( fea )) fea = self . upconv2 ( functional . interpolate ( fea , scale_factor = 2 , mode = 'nearest' )) fea = self . lrelu ( self . att2 ( fea )) fea = self . lrelu ( self . HRconv2 ( fea )) out = self . conv_last ( fea ) ilr = functional . interpolate ( x , scale_factor = self . scale , mode = 'bilinear' , align_corners = False ) out = out + ilr return out","title":"forward()"},{"location":"models/a2n/#super_image.models.a2n.modeling_a2n.A2nModel.load_state_dict","text":"Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys Source code in super_image\\models\\a2n\\modeling_a2n.py def load_state_dict ( self , state_dict , strict = True ): own_state = self . state_dict () for name , param in state_dict . items (): if name in own_state : if isinstance ( param , nn . Parameter ): param = param . data try : own_state [ name ] . copy_ ( param ) except Exception : if name . find ( 'tail' ) == - 1 : raise RuntimeError ( f 'While copying the parameter named { name } , ' f 'whose dimensions in the model are { own_state [ name ] . size () } and ' f 'whose dimensions in the checkpoint are { param . size () } .' ) elif strict : if name . find ( 'tail' ) == - 1 : raise KeyError ( f 'unexpected key \" { name } \" in state_dict' )","title":"load_state_dict()"},{"location":"models/awsrn/","text":"Lightweight Image Super-Resolution with Adaptive Weighted Learning Network (AWSRN) # Overview # Deep learning has been successfully applied to the single-image super-resolution (SISR) task with great performance in recent years. However, most convolutional neural network based SR models require heavy computation, which limit their real-world applications. In this work, a lightweight SR network, named Adaptive Weighted Super-Resolution Network (AWSRN), is proposed for SISR to address this issue. A novel local fusion block (LFB) is designed in AWSRN for efficient residual learning, which consists of stacked adaptive weighted residual units (AWRU) and a local residual fusion unit (LRFU). Moreover, an adaptive weighted multi-scale (AWMS) module is proposed to make full use of features in reconstruction layer. AWMS consists of several different scale convolutions, and the redundancy scale branch can be removed according to the contribution of adaptive weights in AWMS for lightweight network. The experimental results on the commonly used datasets show that the proposed lightweight AWSRN achieves superior performance on \u00d72, \u00d73, \u00d74, and \u00d78 scale factors to state-of-the-art methods with similar parameters and computational overhead.The PAN model proposes a a lightweig1ht convolutional neural network for image super resolution. Pixel attention (PA) is similar to channel attention and spatial attention in formulation. PA however produces 3D attention maps instead of a 1D attention vector or a 2D map. This attention scheme introduces fewer additional parameters but generates better SR results.Deep learning has been successfully applied to the single-image super-resolution (SISR) task with great performance in recent years. However, most convolutional neural network based SR models require heavy computation, which limit their real-world applications. In this work, a lightweight SR network, named Adaptive Weighted Super-Resolution Network (AWSRN), is proposed for SISR to address this issue. A novel local fusion block (LFB) is designed in AWSRN for efficient residual learning, which consists of stacked adaptive weighted residual units (AWRU) and a local residual fusion unit (LRFU). Moreover, an adaptive weighted multi-scale (AWMS) module is proposed to make full use of features in reconstruction layer. AWMS consists of several different scale convolutions, and the redundancy scale branch can be removed according to the contribution of adaptive weights in AWMS for lightweight network. The experimental results on the commonly used datasets show that the proposed lightweight AWSRN achieves superior performance on \u00d72, \u00d73, \u00d74, and \u00d78 scale factors to state-of-the-art methods with similar parameters and computational overhead. This model also applies the balanced attention (BAM) method invented by Wang et al. (2021) to further improve the results. It was introduced in the paper Lightweight Image Super-Resolution with Adaptive Weighted Learning Network by Wang et al. (2019) and first released in this repository . AwsrnConfig # This is the configuration class to store the configuration of a :class: ~super_image.AwsrnModel . It is used to instantiate the model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the AWSRN architecture. Configuration objects inherit from :class: ~super_image.PretrainedConfig and can be used to control the model outputs. Read the documentation from :class: ~super_image.PretrainedConfig for more information. Examples: from super_image import AwsrnModel , AwsrnConfig # Initializing a configuration config = AwsrnConfig ( scale = 4 , # train a model to upscale 4x ) # Initializing a model from the configuration model = AwsrnModel ( config ) # Accessing the model configuration configuration = model . config __init__ ( self , scale = None , n_resblocks = 4 , block_feats = 128 , n_colors = 3 , n_feats = 32 , res_scale = 1 , n_awru = 4 , bam = False , rgb_mean = ( 0.4488 , 0.4371 , 0.404 ), data_parallel = False , ** kwargs ) special # Parameters: Name Type Description Default scale int Scale for the model to train an upscaler/super-res model. None n_resblocks int Number of LFB blocks. 4 block_feats int Number of block features. 128 n_feats int Number of feature maps. 32 n_awru int Number of n_awru in one LFB. 4 n_colors int Number of color channels. 3 res_scale int The residual scaling. 1 bam bool Train using balanced attention. False rgb_mean tuple The RGB mean of the train dataset. You can use ~super_image.utils.metrics.calculate_mean_std to calculate it. (0.4488, 0.4371, 0.404) data_parallel bool Option to use multiple GPUs for training. False Source code in super_image\\models\\awsrn\\configuration_awsrn.py def __init__ ( self , scale : int = None , n_resblocks = 4 , block_feats = 128 , n_colors = 3 , n_feats = 32 , res_scale = 1 , n_awru = 4 , bam = False , rgb_mean = DIV2K_RGB_MEAN , data_parallel = False , ** kwargs ): \"\"\" Args: scale (int): Scale for the model to train an upscaler/super-res model. n_resblocks (int): Number of LFB blocks. block_feats (int): Number of block features. n_feats (int): Number of feature maps. n_awru (int): Number of n_awru in one LFB. n_colors (int): Number of color channels. res_scale (int): The residual scaling. bam (bool): Train using balanced attention. rgb_mean (tuple): The RGB mean of the train dataset. You can use `~super_image.utils.metrics.calculate_mean_std` to calculate it. data_parallel (bool): Option to use multiple GPUs for training. \"\"\" super () . __init__ ( ** kwargs ) self . scale = scale self . n_resblocks = n_resblocks self . block_feats = block_feats self . n_colors = n_colors self . n_feats = n_feats self . n_awru = n_awru self . res_scale = res_scale self . bam = bam self . rgb_mean = rgb_mean self . data_parallel = data_parallel AwsrnModel # config_class # This is the configuration class to store the configuration of a :class: ~super_image.AwsrnModel . It is used to instantiate the model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the AWSRN architecture. Configuration objects inherit from :class: ~super_image.PretrainedConfig and can be used to control the model outputs. Read the documentation from :class: ~super_image.PretrainedConfig for more information. Examples: from super_image import AwsrnModel , AwsrnConfig # Initializing a configuration config = AwsrnConfig ( scale = 4 , # train a model to upscale 4x ) # Initializing a model from the configuration model = AwsrnModel ( config ) # Accessing the model configuration configuration = model . config __init__ ( self , scale = None , n_resblocks = 4 , block_feats = 128 , n_colors = 3 , n_feats = 32 , res_scale = 1 , n_awru = 4 , bam = False , rgb_mean = ( 0.4488 , 0.4371 , 0.404 ), data_parallel = False , ** kwargs ) special # Parameters: Name Type Description Default scale int Scale for the model to train an upscaler/super-res model. None n_resblocks int Number of LFB blocks. 4 block_feats int Number of block features. 128 n_feats int Number of feature maps. 32 n_awru int Number of n_awru in one LFB. 4 n_colors int Number of color channels. 3 res_scale int The residual scaling. 1 bam bool Train using balanced attention. False rgb_mean tuple The RGB mean of the train dataset. You can use ~super_image.utils.metrics.calculate_mean_std to calculate it. (0.4488, 0.4371, 0.404) data_parallel bool Option to use multiple GPUs for training. False Source code in super_image\\models\\awsrn\\modeling_awsrn.py def __init__ ( self , scale : int = None , n_resblocks = 4 , block_feats = 128 , n_colors = 3 , n_feats = 32 , res_scale = 1 , n_awru = 4 , bam = False , rgb_mean = DIV2K_RGB_MEAN , data_parallel = False , ** kwargs ): \"\"\" Args: scale (int): Scale for the model to train an upscaler/super-res model. n_resblocks (int): Number of LFB blocks. block_feats (int): Number of block features. n_feats (int): Number of feature maps. n_awru (int): Number of n_awru in one LFB. n_colors (int): Number of color channels. res_scale (int): The residual scaling. bam (bool): Train using balanced attention. rgb_mean (tuple): The RGB mean of the train dataset. You can use `~super_image.utils.metrics.calculate_mean_std` to calculate it. data_parallel (bool): Option to use multiple GPUs for training. \"\"\" super () . __init__ ( ** kwargs ) self . scale = scale self . n_resblocks = n_resblocks self . block_feats = block_feats self . n_colors = n_colors self . n_feats = n_feats self . n_awru = n_awru self . res_scale = res_scale self . bam = bam self . rgb_mean = rgb_mean self . data_parallel = data_parallel forward ( self , x ) # Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in super_image\\models\\awsrn\\modeling_awsrn.py def forward ( self , x ): if not self . bam : x = ( x - self . rgb_mean . to ( self . device ) * 255 ) / 127.5 s = self . skip ( x ) x = self . head ( x ) x = self . body ( x ) if self . bam : x = self . attention ( x ) x = self . tail ( x ) x += s if not self . bam : x = x * 127.5 + self . rgb_mean . to ( self . device ) * 255 return x load_state_dict ( self , state_dict , strict = True ) # Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys Source code in super_image\\models\\awsrn\\modeling_awsrn.py def load_state_dict ( self , state_dict , strict = True ): own_state = self . state_dict () for name , param in state_dict . items (): if name in own_state : if isinstance ( param , nn . Parameter ): param = param . data try : own_state [ name ] . copy_ ( param ) except Exception : if name . find ( 'tail' ) >= 0 or name . find ( 'skip' ) >= 0 : print ( 'Replace pre-trained upsampler to new one...' ) else : raise RuntimeError ( 'While copying the parameter named {} , ' 'whose dimensions in the model are {} and ' 'whose dimensions in the checkpoint are {} .' . format ( name , own_state [ name ] . size (), param . size ())) elif strict : if name . find ( 'tail' ) == - 1 : raise KeyError ( 'unexpected key \" {} \" in state_dict' . format ( name )) if strict : missing = set ( own_state . keys ()) - set ( state_dict . keys ()) if len ( missing ) > 0 : raise KeyError ( 'missing keys in state_dict: \" {} \"' . format ( missing ))","title":"Lightweight Image Super-Resolution with Adaptive Weighted Learning Network (AWSRN)"},{"location":"models/awsrn/#lightweight-image-super-resolution-with-adaptive-weighted-learning-network-awsrn","text":"","title":"Lightweight Image Super-Resolution with Adaptive Weighted Learning Network (AWSRN)"},{"location":"models/awsrn/#overview","text":"Deep learning has been successfully applied to the single-image super-resolution (SISR) task with great performance in recent years. However, most convolutional neural network based SR models require heavy computation, which limit their real-world applications. In this work, a lightweight SR network, named Adaptive Weighted Super-Resolution Network (AWSRN), is proposed for SISR to address this issue. A novel local fusion block (LFB) is designed in AWSRN for efficient residual learning, which consists of stacked adaptive weighted residual units (AWRU) and a local residual fusion unit (LRFU). Moreover, an adaptive weighted multi-scale (AWMS) module is proposed to make full use of features in reconstruction layer. AWMS consists of several different scale convolutions, and the redundancy scale branch can be removed according to the contribution of adaptive weights in AWMS for lightweight network. The experimental results on the commonly used datasets show that the proposed lightweight AWSRN achieves superior performance on \u00d72, \u00d73, \u00d74, and \u00d78 scale factors to state-of-the-art methods with similar parameters and computational overhead.The PAN model proposes a a lightweig1ht convolutional neural network for image super resolution. Pixel attention (PA) is similar to channel attention and spatial attention in formulation. PA however produces 3D attention maps instead of a 1D attention vector or a 2D map. This attention scheme introduces fewer additional parameters but generates better SR results.Deep learning has been successfully applied to the single-image super-resolution (SISR) task with great performance in recent years. However, most convolutional neural network based SR models require heavy computation, which limit their real-world applications. In this work, a lightweight SR network, named Adaptive Weighted Super-Resolution Network (AWSRN), is proposed for SISR to address this issue. A novel local fusion block (LFB) is designed in AWSRN for efficient residual learning, which consists of stacked adaptive weighted residual units (AWRU) and a local residual fusion unit (LRFU). Moreover, an adaptive weighted multi-scale (AWMS) module is proposed to make full use of features in reconstruction layer. AWMS consists of several different scale convolutions, and the redundancy scale branch can be removed according to the contribution of adaptive weights in AWMS for lightweight network. The experimental results on the commonly used datasets show that the proposed lightweight AWSRN achieves superior performance on \u00d72, \u00d73, \u00d74, and \u00d78 scale factors to state-of-the-art methods with similar parameters and computational overhead. This model also applies the balanced attention (BAM) method invented by Wang et al. (2021) to further improve the results. It was introduced in the paper Lightweight Image Super-Resolution with Adaptive Weighted Learning Network by Wang et al. (2019) and first released in this repository .","title":"Overview"},{"location":"models/awsrn/#awsrnconfig","text":"This is the configuration class to store the configuration of a :class: ~super_image.AwsrnModel . It is used to instantiate the model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the AWSRN architecture. Configuration objects inherit from :class: ~super_image.PretrainedConfig and can be used to control the model outputs. Read the documentation from :class: ~super_image.PretrainedConfig for more information. Examples: from super_image import AwsrnModel , AwsrnConfig # Initializing a configuration config = AwsrnConfig ( scale = 4 , # train a model to upscale 4x ) # Initializing a model from the configuration model = AwsrnModel ( config ) # Accessing the model configuration configuration = model . config","title":"AwsrnConfig"},{"location":"models/awsrn/#super_image.models.awsrn.configuration_awsrn.AwsrnConfig.__init__","text":"Parameters: Name Type Description Default scale int Scale for the model to train an upscaler/super-res model. None n_resblocks int Number of LFB blocks. 4 block_feats int Number of block features. 128 n_feats int Number of feature maps. 32 n_awru int Number of n_awru in one LFB. 4 n_colors int Number of color channels. 3 res_scale int The residual scaling. 1 bam bool Train using balanced attention. False rgb_mean tuple The RGB mean of the train dataset. You can use ~super_image.utils.metrics.calculate_mean_std to calculate it. (0.4488, 0.4371, 0.404) data_parallel bool Option to use multiple GPUs for training. False Source code in super_image\\models\\awsrn\\configuration_awsrn.py def __init__ ( self , scale : int = None , n_resblocks = 4 , block_feats = 128 , n_colors = 3 , n_feats = 32 , res_scale = 1 , n_awru = 4 , bam = False , rgb_mean = DIV2K_RGB_MEAN , data_parallel = False , ** kwargs ): \"\"\" Args: scale (int): Scale for the model to train an upscaler/super-res model. n_resblocks (int): Number of LFB blocks. block_feats (int): Number of block features. n_feats (int): Number of feature maps. n_awru (int): Number of n_awru in one LFB. n_colors (int): Number of color channels. res_scale (int): The residual scaling. bam (bool): Train using balanced attention. rgb_mean (tuple): The RGB mean of the train dataset. You can use `~super_image.utils.metrics.calculate_mean_std` to calculate it. data_parallel (bool): Option to use multiple GPUs for training. \"\"\" super () . __init__ ( ** kwargs ) self . scale = scale self . n_resblocks = n_resblocks self . block_feats = block_feats self . n_colors = n_colors self . n_feats = n_feats self . n_awru = n_awru self . res_scale = res_scale self . bam = bam self . rgb_mean = rgb_mean self . data_parallel = data_parallel","title":"__init__()"},{"location":"models/awsrn/#awsrnmodel","text":"","title":"AwsrnModel"},{"location":"models/awsrn/#super_image.models.awsrn.modeling_awsrn.AwsrnModel.config_class","text":"This is the configuration class to store the configuration of a :class: ~super_image.AwsrnModel . It is used to instantiate the model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the AWSRN architecture. Configuration objects inherit from :class: ~super_image.PretrainedConfig and can be used to control the model outputs. Read the documentation from :class: ~super_image.PretrainedConfig for more information. Examples: from super_image import AwsrnModel , AwsrnConfig # Initializing a configuration config = AwsrnConfig ( scale = 4 , # train a model to upscale 4x ) # Initializing a model from the configuration model = AwsrnModel ( config ) # Accessing the model configuration configuration = model . config","title":"config_class"},{"location":"models/awsrn/#super_image.models.awsrn.modeling_awsrn.AwsrnModel.config_class.__init__","text":"Parameters: Name Type Description Default scale int Scale for the model to train an upscaler/super-res model. None n_resblocks int Number of LFB blocks. 4 block_feats int Number of block features. 128 n_feats int Number of feature maps. 32 n_awru int Number of n_awru in one LFB. 4 n_colors int Number of color channels. 3 res_scale int The residual scaling. 1 bam bool Train using balanced attention. False rgb_mean tuple The RGB mean of the train dataset. You can use ~super_image.utils.metrics.calculate_mean_std to calculate it. (0.4488, 0.4371, 0.404) data_parallel bool Option to use multiple GPUs for training. False Source code in super_image\\models\\awsrn\\modeling_awsrn.py def __init__ ( self , scale : int = None , n_resblocks = 4 , block_feats = 128 , n_colors = 3 , n_feats = 32 , res_scale = 1 , n_awru = 4 , bam = False , rgb_mean = DIV2K_RGB_MEAN , data_parallel = False , ** kwargs ): \"\"\" Args: scale (int): Scale for the model to train an upscaler/super-res model. n_resblocks (int): Number of LFB blocks. block_feats (int): Number of block features. n_feats (int): Number of feature maps. n_awru (int): Number of n_awru in one LFB. n_colors (int): Number of color channels. res_scale (int): The residual scaling. bam (bool): Train using balanced attention. rgb_mean (tuple): The RGB mean of the train dataset. You can use `~super_image.utils.metrics.calculate_mean_std` to calculate it. data_parallel (bool): Option to use multiple GPUs for training. \"\"\" super () . __init__ ( ** kwargs ) self . scale = scale self . n_resblocks = n_resblocks self . block_feats = block_feats self . n_colors = n_colors self . n_feats = n_feats self . n_awru = n_awru self . res_scale = res_scale self . bam = bam self . rgb_mean = rgb_mean self . data_parallel = data_parallel","title":"__init__()"},{"location":"models/awsrn/#super_image.models.awsrn.modeling_awsrn.AwsrnModel.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in super_image\\models\\awsrn\\modeling_awsrn.py def forward ( self , x ): if not self . bam : x = ( x - self . rgb_mean . to ( self . device ) * 255 ) / 127.5 s = self . skip ( x ) x = self . head ( x ) x = self . body ( x ) if self . bam : x = self . attention ( x ) x = self . tail ( x ) x += s if not self . bam : x = x * 127.5 + self . rgb_mean . to ( self . device ) * 255 return x","title":"forward()"},{"location":"models/awsrn/#super_image.models.awsrn.modeling_awsrn.AwsrnModel.load_state_dict","text":"Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys Source code in super_image\\models\\awsrn\\modeling_awsrn.py def load_state_dict ( self , state_dict , strict = True ): own_state = self . state_dict () for name , param in state_dict . items (): if name in own_state : if isinstance ( param , nn . Parameter ): param = param . data try : own_state [ name ] . copy_ ( param ) except Exception : if name . find ( 'tail' ) >= 0 or name . find ( 'skip' ) >= 0 : print ( 'Replace pre-trained upsampler to new one...' ) else : raise RuntimeError ( 'While copying the parameter named {} , ' 'whose dimensions in the model are {} and ' 'whose dimensions in the checkpoint are {} .' . format ( name , own_state [ name ] . size (), param . size ())) elif strict : if name . find ( 'tail' ) == - 1 : raise KeyError ( 'unexpected key \" {} \" in state_dict' . format ( name )) if strict : missing = set ( own_state . keys ()) - set ( state_dict . keys ()) if len ( missing ) > 0 : raise KeyError ( 'missing keys in state_dict: \" {} \"' . format ( missing ))","title":"load_state_dict()"},{"location":"models/carn/","text":"Cascading Residual Network (CARN) # Overview # The CARN model proposes an architecture that implements a cascading mechanism upon a residual network for accurate and lightweight image super-resolution. This model also applies the balanced attention (BAM) method invented by Wang et al. (2021) to further improve the results. It was introduced in the paper Fast, Accurate, and Lightweight Super-Resolution with Cascading Residual Network by Ahn et al. (2018) and first released in this repository . CarnConfig # This is the configuration class to store the configuration of a :class: ~super_image.CarnModel . It is used to instantiate the model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the CARN architecture. Configuration objects inherit from :class: ~super_image.PretrainedConfig and can be used to control the model outputs. Read the documentation from :class: ~super_image.PretrainedConfig for more information. Examples: from super_image import CarnModel , CarnConfig # Initializing a configuration config = CarnConfig ( scale = 4 , # train a model to upscale 4x ) # Initializing a model from the configuration model = CarnModel ( config ) # Accessing the model configuration configuration = model . config __init__ ( self , scale = None , bam = False , rgb_mean = ( 0.4488 , 0.4371 , 0.404 ), rgb_std = ( 1.0 , 1.0 , 1.0 ), data_parallel = False , ** kwargs ) special # Parameters: Name Type Description Default scale int Scale for the model to train an upscaler/super-res model. None bam bool Train using balanced attention. False rgb_mean tuple The RGB mean of the train dataset. You can use ~super_image.utils.metrics.calculate_mean_std to calculate it. (0.4488, 0.4371, 0.404) rgb_std tuple The RGB standard deviation of the train dataset. You can use ~super_image.utils.metrics.calculate_mean_std to calculate it. (1.0, 1.0, 1.0) data_parallel bool Option to use multiple GPUs for training. False Source code in super_image\\models\\carn\\configuration_carn.py def __init__ ( self , scale : int = None , bam = False , rgb_mean = DIV2K_RGB_MEAN , rgb_std = DIV2K_RGB_STD , data_parallel = False , ** kwargs ): \"\"\" Args: scale (int): Scale for the model to train an upscaler/super-res model. bam (bool): Train using balanced attention. rgb_mean (tuple): The RGB mean of the train dataset. You can use `~super_image.utils.metrics.calculate_mean_std` to calculate it. rgb_std (tuple): The RGB standard deviation of the train dataset. You can use `~super_image.utils.metrics.calculate_mean_std` to calculate it. data_parallel (bool): Option to use multiple GPUs for training. \"\"\" super () . __init__ ( ** kwargs ) self . scale = scale self . bam = bam self . rgb_mean = rgb_mean self . rgb_std = rgb_std self . data_parallel = data_parallel CarnModel # config_class # This is the configuration class to store the configuration of a :class: ~super_image.CarnModel . It is used to instantiate the model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the CARN architecture. Configuration objects inherit from :class: ~super_image.PretrainedConfig and can be used to control the model outputs. Read the documentation from :class: ~super_image.PretrainedConfig for more information. Examples: from super_image import CarnModel , CarnConfig # Initializing a configuration config = CarnConfig ( scale = 4 , # train a model to upscale 4x ) # Initializing a model from the configuration model = CarnModel ( config ) # Accessing the model configuration configuration = model . config __init__ ( self , scale = None , bam = False , rgb_mean = ( 0.4488 , 0.4371 , 0.404 ), rgb_std = ( 1.0 , 1.0 , 1.0 ), data_parallel = False , ** kwargs ) special # Parameters: Name Type Description Default scale int Scale for the model to train an upscaler/super-res model. None bam bool Train using balanced attention. False rgb_mean tuple The RGB mean of the train dataset. You can use ~super_image.utils.metrics.calculate_mean_std to calculate it. (0.4488, 0.4371, 0.404) rgb_std tuple The RGB standard deviation of the train dataset. You can use ~super_image.utils.metrics.calculate_mean_std to calculate it. (1.0, 1.0, 1.0) data_parallel bool Option to use multiple GPUs for training. False Source code in super_image\\models\\carn\\modeling_carn.py def __init__ ( self , scale : int = None , bam = False , rgb_mean = DIV2K_RGB_MEAN , rgb_std = DIV2K_RGB_STD , data_parallel = False , ** kwargs ): \"\"\" Args: scale (int): Scale for the model to train an upscaler/super-res model. bam (bool): Train using balanced attention. rgb_mean (tuple): The RGB mean of the train dataset. You can use `~super_image.utils.metrics.calculate_mean_std` to calculate it. rgb_std (tuple): The RGB standard deviation of the train dataset. You can use `~super_image.utils.metrics.calculate_mean_std` to calculate it. data_parallel (bool): Option to use multiple GPUs for training. \"\"\" super () . __init__ ( ** kwargs ) self . scale = scale self . bam = bam self . rgb_mean = rgb_mean self . rgb_std = rgb_std self . data_parallel = data_parallel forward ( self , x ) # Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in super_image\\models\\carn\\modeling_carn.py def forward ( self , x ): x = self . sub_mean ( x ) x = self . entry ( x ) c0 = o0 = x b1 = self . b1 ( o0 ) c1 = torch . cat ([ c0 , b1 ], dim = 1 ) o1 = self . c1 ( c1 ) b2 = self . b2 ( o1 ) c2 = torch . cat ([ c1 , b2 ], dim = 1 ) o2 = self . c2 ( c2 ) b3 = self . b3 ( o2 ) c3 = torch . cat ([ c2 , b3 ], dim = 1 ) o3 = self . c3 ( c3 ) out = self . upsample ( o3 , scale = self . scale ) out = self . exit ( out ) out = self . add_mean ( out ) return out load_state_dict ( self , state_dict , strict = True ) # Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys Source code in super_image\\models\\carn\\modeling_carn.py def load_state_dict ( self , state_dict , strict = True ): own_state = self . state_dict () for name , param in state_dict . items (): if name in own_state : if isinstance ( param , nn . Parameter ): param = param . data try : own_state [ name ] . copy_ ( param ) except Exception : if name . find ( 'tail' ) == - 1 : raise RuntimeError ( f 'While copying the parameter named { name } , ' f 'whose dimensions in the model are { own_state [ name ] . size () } and ' f 'whose dimensions in the checkpoint are { param . size () } .' ) elif strict : if name . find ( 'tail' ) == - 1 : raise KeyError ( f 'unexpected key \" { name } \" in state_dict' )","title":"CARN"},{"location":"models/carn/#cascading-residual-network-carn","text":"","title":"Cascading Residual Network (CARN)"},{"location":"models/carn/#overview","text":"The CARN model proposes an architecture that implements a cascading mechanism upon a residual network for accurate and lightweight image super-resolution. This model also applies the balanced attention (BAM) method invented by Wang et al. (2021) to further improve the results. It was introduced in the paper Fast, Accurate, and Lightweight Super-Resolution with Cascading Residual Network by Ahn et al. (2018) and first released in this repository .","title":"Overview"},{"location":"models/carn/#carnconfig","text":"This is the configuration class to store the configuration of a :class: ~super_image.CarnModel . It is used to instantiate the model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the CARN architecture. Configuration objects inherit from :class: ~super_image.PretrainedConfig and can be used to control the model outputs. Read the documentation from :class: ~super_image.PretrainedConfig for more information. Examples: from super_image import CarnModel , CarnConfig # Initializing a configuration config = CarnConfig ( scale = 4 , # train a model to upscale 4x ) # Initializing a model from the configuration model = CarnModel ( config ) # Accessing the model configuration configuration = model . config","title":"CarnConfig"},{"location":"models/carn/#super_image.models.carn.configuration_carn.CarnConfig.__init__","text":"Parameters: Name Type Description Default scale int Scale for the model to train an upscaler/super-res model. None bam bool Train using balanced attention. False rgb_mean tuple The RGB mean of the train dataset. You can use ~super_image.utils.metrics.calculate_mean_std to calculate it. (0.4488, 0.4371, 0.404) rgb_std tuple The RGB standard deviation of the train dataset. You can use ~super_image.utils.metrics.calculate_mean_std to calculate it. (1.0, 1.0, 1.0) data_parallel bool Option to use multiple GPUs for training. False Source code in super_image\\models\\carn\\configuration_carn.py def __init__ ( self , scale : int = None , bam = False , rgb_mean = DIV2K_RGB_MEAN , rgb_std = DIV2K_RGB_STD , data_parallel = False , ** kwargs ): \"\"\" Args: scale (int): Scale for the model to train an upscaler/super-res model. bam (bool): Train using balanced attention. rgb_mean (tuple): The RGB mean of the train dataset. You can use `~super_image.utils.metrics.calculate_mean_std` to calculate it. rgb_std (tuple): The RGB standard deviation of the train dataset. You can use `~super_image.utils.metrics.calculate_mean_std` to calculate it. data_parallel (bool): Option to use multiple GPUs for training. \"\"\" super () . __init__ ( ** kwargs ) self . scale = scale self . bam = bam self . rgb_mean = rgb_mean self . rgb_std = rgb_std self . data_parallel = data_parallel","title":"__init__()"},{"location":"models/carn/#carnmodel","text":"","title":"CarnModel"},{"location":"models/carn/#super_image.models.carn.modeling_carn.CarnModel.config_class","text":"This is the configuration class to store the configuration of a :class: ~super_image.CarnModel . It is used to instantiate the model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the CARN architecture. Configuration objects inherit from :class: ~super_image.PretrainedConfig and can be used to control the model outputs. Read the documentation from :class: ~super_image.PretrainedConfig for more information. Examples: from super_image import CarnModel , CarnConfig # Initializing a configuration config = CarnConfig ( scale = 4 , # train a model to upscale 4x ) # Initializing a model from the configuration model = CarnModel ( config ) # Accessing the model configuration configuration = model . config","title":"config_class"},{"location":"models/carn/#super_image.models.carn.modeling_carn.CarnModel.config_class.__init__","text":"Parameters: Name Type Description Default scale int Scale for the model to train an upscaler/super-res model. None bam bool Train using balanced attention. False rgb_mean tuple The RGB mean of the train dataset. You can use ~super_image.utils.metrics.calculate_mean_std to calculate it. (0.4488, 0.4371, 0.404) rgb_std tuple The RGB standard deviation of the train dataset. You can use ~super_image.utils.metrics.calculate_mean_std to calculate it. (1.0, 1.0, 1.0) data_parallel bool Option to use multiple GPUs for training. False Source code in super_image\\models\\carn\\modeling_carn.py def __init__ ( self , scale : int = None , bam = False , rgb_mean = DIV2K_RGB_MEAN , rgb_std = DIV2K_RGB_STD , data_parallel = False , ** kwargs ): \"\"\" Args: scale (int): Scale for the model to train an upscaler/super-res model. bam (bool): Train using balanced attention. rgb_mean (tuple): The RGB mean of the train dataset. You can use `~super_image.utils.metrics.calculate_mean_std` to calculate it. rgb_std (tuple): The RGB standard deviation of the train dataset. You can use `~super_image.utils.metrics.calculate_mean_std` to calculate it. data_parallel (bool): Option to use multiple GPUs for training. \"\"\" super () . __init__ ( ** kwargs ) self . scale = scale self . bam = bam self . rgb_mean = rgb_mean self . rgb_std = rgb_std self . data_parallel = data_parallel","title":"__init__()"},{"location":"models/carn/#super_image.models.carn.modeling_carn.CarnModel.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in super_image\\models\\carn\\modeling_carn.py def forward ( self , x ): x = self . sub_mean ( x ) x = self . entry ( x ) c0 = o0 = x b1 = self . b1 ( o0 ) c1 = torch . cat ([ c0 , b1 ], dim = 1 ) o1 = self . c1 ( c1 ) b2 = self . b2 ( o1 ) c2 = torch . cat ([ c1 , b2 ], dim = 1 ) o2 = self . c2 ( c2 ) b3 = self . b3 ( o2 ) c3 = torch . cat ([ c2 , b3 ], dim = 1 ) o3 = self . c3 ( c3 ) out = self . upsample ( o3 , scale = self . scale ) out = self . exit ( out ) out = self . add_mean ( out ) return out","title":"forward()"},{"location":"models/carn/#super_image.models.carn.modeling_carn.CarnModel.load_state_dict","text":"Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys Source code in super_image\\models\\carn\\modeling_carn.py def load_state_dict ( self , state_dict , strict = True ): own_state = self . state_dict () for name , param in state_dict . items (): if name in own_state : if isinstance ( param , nn . Parameter ): param = param . data try : own_state [ name ] . copy_ ( param ) except Exception : if name . find ( 'tail' ) == - 1 : raise RuntimeError ( f 'While copying the parameter named { name } , ' f 'whose dimensions in the model are { own_state [ name ] . size () } and ' f 'whose dimensions in the checkpoint are { param . size () } .' ) elif strict : if name . find ( 'tail' ) == - 1 : raise KeyError ( f 'unexpected key \" { name } \" in state_dict' )","title":"load_state_dict()"},{"location":"models/edsr/","text":"Enhanced Deep Residual Networks for Single Image Super-Resolution (EDSR) # Overview # EDSR is a model that uses both deeper and wider architecture (32 ResBlocks and 256 channels) to improve performance. It uses both global and local skip connections, and up-scaling is done at the end of the network. It doesn't use batch normalization layers (input and output have similar distributions, normalizing intermediate features may not be desirable) instead it uses constant scaling layers to ensure stable training. An L1 loss function (absolute error) is used instead of L2 (MSE), the authors showed better performance empirically and it requires less computation. The default parameters are for the base model (~5mb vs ~100mb) that includes just 16 ResBlocks and 64 channels. It was introduced in the paper Enhanced Deep Residual Networks for Single Image Super-Resolution by Lim et al. (2017) and first released in this repository . EdsrConfig # This is the configuration class to store the configuration of a :class: ~super_image.EdsrModel . It is used to instantiate the model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the EDSR base architecture. Configuration objects inherit from :class: ~super_image.PretrainedConfig and can be used to control the model outputs. Read the documentation from :class: ~super_image.PretrainedConfig for more information. Examples: from super_image import EdsrModel , EdsrConfig # Initializing a configuration config = EdsrConfig ( scale = 4 , # train a model to upscale 4x ) # Initializing a model from the configuration model = EdsrModel ( config ) # Accessing the model configuration configuration = model . config __init__ ( self , scale = None , n_resblocks = 16 , n_feats = 64 , n_colors = 3 , rgb_range = 255 , rgb_mean = ( 0.4488 , 0.4371 , 0.404 ), rgb_std = ( 1.0 , 1.0 , 1.0 ), no_upsampling = False , res_scale = 1 , data_parallel = False , ** kwargs ) special # Parameters: Name Type Description Default scale int Scale for the model to train an upscaler/super-res model. None n_resblocks int Number of residual blocks. 16 n_feats int Number of filters. 64 n_colors int Number of color channels. 3 rgb_range int Range of RGB as a multiplier to the MeanShift. 255 res_scale int The res scale multiplier. 1 rgb_mean tuple The RGB mean of the train dataset. You can use ~super_image.utils.metrics.calculate_mean_std to calculate it. (0.4488, 0.4371, 0.404) rgb_std tuple The RGB standard deviation of the train dataset. You can use ~super_image.utils.metrics.calculate_mean_std to calculate it. (1.0, 1.0, 1.0) no_upsampling bool Option to turn off upsampling. False data_parallel bool Option to use multiple GPUs for training. False Source code in super_image\\models\\edsr\\configuration_edsr.py def __init__ ( self , scale : int = None , n_resblocks = 16 , n_feats = 64 , n_colors = 3 , rgb_range = 255 , rgb_mean = DIV2K_RGB_MEAN , rgb_std = DIV2K_RGB_STD , no_upsampling = False , res_scale = 1 , data_parallel = False , ** kwargs ): \"\"\" Args: scale (int): Scale for the model to train an upscaler/super-res model. n_resblocks (int): Number of residual blocks. n_feats (int): Number of filters. n_colors (int): Number of color channels. rgb_range (int): Range of RGB as a multiplier to the MeanShift. res_scale (int): The res scale multiplier. rgb_mean (tuple): The RGB mean of the train dataset. You can use `~super_image.utils.metrics.calculate_mean_std` to calculate it. rgb_std (tuple): The RGB standard deviation of the train dataset. You can use `~super_image.utils.metrics.calculate_mean_std` to calculate it. no_upsampling (bool): Option to turn off upsampling. data_parallel (bool): Option to use multiple GPUs for training. \"\"\" super () . __init__ ( ** kwargs ) self . scale = scale self . n_resblocks = n_resblocks self . n_feats = n_feats self . n_colors = n_colors self . rgb_range = rgb_range self . res_scale = res_scale self . rgb_mean = rgb_mean self . rgb_std = rgb_std self . no_upsampling = no_upsampling self . data_parallel = data_parallel EdsrModel # config_class # This is the configuration class to store the configuration of a :class: ~super_image.EdsrModel . It is used to instantiate the model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the EDSR base architecture. Configuration objects inherit from :class: ~super_image.PretrainedConfig and can be used to control the model outputs. Read the documentation from :class: ~super_image.PretrainedConfig for more information. Examples: from super_image import EdsrModel , EdsrConfig # Initializing a configuration config = EdsrConfig ( scale = 4 , # train a model to upscale 4x ) # Initializing a model from the configuration model = EdsrModel ( config ) # Accessing the model configuration configuration = model . config __init__ ( self , scale = None , n_resblocks = 16 , n_feats = 64 , n_colors = 3 , rgb_range = 255 , rgb_mean = ( 0.4488 , 0.4371 , 0.404 ), rgb_std = ( 1.0 , 1.0 , 1.0 ), no_upsampling = False , res_scale = 1 , data_parallel = False , ** kwargs ) special # Parameters: Name Type Description Default scale int Scale for the model to train an upscaler/super-res model. None n_resblocks int Number of residual blocks. 16 n_feats int Number of filters. 64 n_colors int Number of color channels. 3 rgb_range int Range of RGB as a multiplier to the MeanShift. 255 res_scale int The res scale multiplier. 1 rgb_mean tuple The RGB mean of the train dataset. You can use ~super_image.utils.metrics.calculate_mean_std to calculate it. (0.4488, 0.4371, 0.404) rgb_std tuple The RGB standard deviation of the train dataset. You can use ~super_image.utils.metrics.calculate_mean_std to calculate it. (1.0, 1.0, 1.0) no_upsampling bool Option to turn off upsampling. False data_parallel bool Option to use multiple GPUs for training. False Source code in super_image\\models\\edsr\\modeling_edsr.py def __init__ ( self , scale : int = None , n_resblocks = 16 , n_feats = 64 , n_colors = 3 , rgb_range = 255 , rgb_mean = DIV2K_RGB_MEAN , rgb_std = DIV2K_RGB_STD , no_upsampling = False , res_scale = 1 , data_parallel = False , ** kwargs ): \"\"\" Args: scale (int): Scale for the model to train an upscaler/super-res model. n_resblocks (int): Number of residual blocks. n_feats (int): Number of filters. n_colors (int): Number of color channels. rgb_range (int): Range of RGB as a multiplier to the MeanShift. res_scale (int): The res scale multiplier. rgb_mean (tuple): The RGB mean of the train dataset. You can use `~super_image.utils.metrics.calculate_mean_std` to calculate it. rgb_std (tuple): The RGB standard deviation of the train dataset. You can use `~super_image.utils.metrics.calculate_mean_std` to calculate it. no_upsampling (bool): Option to turn off upsampling. data_parallel (bool): Option to use multiple GPUs for training. \"\"\" super () . __init__ ( ** kwargs ) self . scale = scale self . n_resblocks = n_resblocks self . n_feats = n_feats self . n_colors = n_colors self . rgb_range = rgb_range self . res_scale = res_scale self . rgb_mean = rgb_mean self . rgb_std = rgb_std self . no_upsampling = no_upsampling self . data_parallel = data_parallel forward ( self , x ) # Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in super_image\\models\\edsr\\modeling_edsr.py def forward ( self , x ): x = self . head ( x ) res = self . body ( x ) res += x if self . args . no_upsampling : x = res else : x = self . tail ( res ) return x load_state_dict ( self , state_dict , strict = True ) # Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys Source code in super_image\\models\\edsr\\modeling_edsr.py def load_state_dict ( self , state_dict , strict = True ): own_state = self . state_dict () for name , param in state_dict . items (): if name in own_state : if isinstance ( param , nn . Parameter ): param = param . data try : own_state [ name ] . copy_ ( param ) except Exception : if name . find ( 'tail' ) == - 1 : raise RuntimeError ( f 'While copying the parameter named { name } , ' f 'whose dimensions in the model are { own_state [ name ] . size () } and ' f 'whose dimensions in the checkpoint are { param . size () } .' ) elif strict : if name . find ( 'tail' ) == - 1 : raise KeyError ( f 'unexpected key \" { name } \" in state_dict' )","title":"EDSR"},{"location":"models/edsr/#enhanced-deep-residual-networks-for-single-image-super-resolution-edsr","text":"","title":"Enhanced Deep Residual Networks for Single Image Super-Resolution (EDSR)"},{"location":"models/edsr/#overview","text":"EDSR is a model that uses both deeper and wider architecture (32 ResBlocks and 256 channels) to improve performance. It uses both global and local skip connections, and up-scaling is done at the end of the network. It doesn't use batch normalization layers (input and output have similar distributions, normalizing intermediate features may not be desirable) instead it uses constant scaling layers to ensure stable training. An L1 loss function (absolute error) is used instead of L2 (MSE), the authors showed better performance empirically and it requires less computation. The default parameters are for the base model (~5mb vs ~100mb) that includes just 16 ResBlocks and 64 channels. It was introduced in the paper Enhanced Deep Residual Networks for Single Image Super-Resolution by Lim et al. (2017) and first released in this repository .","title":"Overview"},{"location":"models/edsr/#edsrconfig","text":"This is the configuration class to store the configuration of a :class: ~super_image.EdsrModel . It is used to instantiate the model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the EDSR base architecture. Configuration objects inherit from :class: ~super_image.PretrainedConfig and can be used to control the model outputs. Read the documentation from :class: ~super_image.PretrainedConfig for more information. Examples: from super_image import EdsrModel , EdsrConfig # Initializing a configuration config = EdsrConfig ( scale = 4 , # train a model to upscale 4x ) # Initializing a model from the configuration model = EdsrModel ( config ) # Accessing the model configuration configuration = model . config","title":"EdsrConfig"},{"location":"models/edsr/#super_image.models.edsr.configuration_edsr.EdsrConfig.__init__","text":"Parameters: Name Type Description Default scale int Scale for the model to train an upscaler/super-res model. None n_resblocks int Number of residual blocks. 16 n_feats int Number of filters. 64 n_colors int Number of color channels. 3 rgb_range int Range of RGB as a multiplier to the MeanShift. 255 res_scale int The res scale multiplier. 1 rgb_mean tuple The RGB mean of the train dataset. You can use ~super_image.utils.metrics.calculate_mean_std to calculate it. (0.4488, 0.4371, 0.404) rgb_std tuple The RGB standard deviation of the train dataset. You can use ~super_image.utils.metrics.calculate_mean_std to calculate it. (1.0, 1.0, 1.0) no_upsampling bool Option to turn off upsampling. False data_parallel bool Option to use multiple GPUs for training. False Source code in super_image\\models\\edsr\\configuration_edsr.py def __init__ ( self , scale : int = None , n_resblocks = 16 , n_feats = 64 , n_colors = 3 , rgb_range = 255 , rgb_mean = DIV2K_RGB_MEAN , rgb_std = DIV2K_RGB_STD , no_upsampling = False , res_scale = 1 , data_parallel = False , ** kwargs ): \"\"\" Args: scale (int): Scale for the model to train an upscaler/super-res model. n_resblocks (int): Number of residual blocks. n_feats (int): Number of filters. n_colors (int): Number of color channels. rgb_range (int): Range of RGB as a multiplier to the MeanShift. res_scale (int): The res scale multiplier. rgb_mean (tuple): The RGB mean of the train dataset. You can use `~super_image.utils.metrics.calculate_mean_std` to calculate it. rgb_std (tuple): The RGB standard deviation of the train dataset. You can use `~super_image.utils.metrics.calculate_mean_std` to calculate it. no_upsampling (bool): Option to turn off upsampling. data_parallel (bool): Option to use multiple GPUs for training. \"\"\" super () . __init__ ( ** kwargs ) self . scale = scale self . n_resblocks = n_resblocks self . n_feats = n_feats self . n_colors = n_colors self . rgb_range = rgb_range self . res_scale = res_scale self . rgb_mean = rgb_mean self . rgb_std = rgb_std self . no_upsampling = no_upsampling self . data_parallel = data_parallel","title":"__init__()"},{"location":"models/edsr/#edsrmodel","text":"","title":"EdsrModel"},{"location":"models/edsr/#super_image.models.edsr.modeling_edsr.EdsrModel.config_class","text":"This is the configuration class to store the configuration of a :class: ~super_image.EdsrModel . It is used to instantiate the model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the EDSR base architecture. Configuration objects inherit from :class: ~super_image.PretrainedConfig and can be used to control the model outputs. Read the documentation from :class: ~super_image.PretrainedConfig for more information. Examples: from super_image import EdsrModel , EdsrConfig # Initializing a configuration config = EdsrConfig ( scale = 4 , # train a model to upscale 4x ) # Initializing a model from the configuration model = EdsrModel ( config ) # Accessing the model configuration configuration = model . config","title":"config_class"},{"location":"models/edsr/#super_image.models.edsr.modeling_edsr.EdsrModel.config_class.__init__","text":"Parameters: Name Type Description Default scale int Scale for the model to train an upscaler/super-res model. None n_resblocks int Number of residual blocks. 16 n_feats int Number of filters. 64 n_colors int Number of color channels. 3 rgb_range int Range of RGB as a multiplier to the MeanShift. 255 res_scale int The res scale multiplier. 1 rgb_mean tuple The RGB mean of the train dataset. You can use ~super_image.utils.metrics.calculate_mean_std to calculate it. (0.4488, 0.4371, 0.404) rgb_std tuple The RGB standard deviation of the train dataset. You can use ~super_image.utils.metrics.calculate_mean_std to calculate it. (1.0, 1.0, 1.0) no_upsampling bool Option to turn off upsampling. False data_parallel bool Option to use multiple GPUs for training. False Source code in super_image\\models\\edsr\\modeling_edsr.py def __init__ ( self , scale : int = None , n_resblocks = 16 , n_feats = 64 , n_colors = 3 , rgb_range = 255 , rgb_mean = DIV2K_RGB_MEAN , rgb_std = DIV2K_RGB_STD , no_upsampling = False , res_scale = 1 , data_parallel = False , ** kwargs ): \"\"\" Args: scale (int): Scale for the model to train an upscaler/super-res model. n_resblocks (int): Number of residual blocks. n_feats (int): Number of filters. n_colors (int): Number of color channels. rgb_range (int): Range of RGB as a multiplier to the MeanShift. res_scale (int): The res scale multiplier. rgb_mean (tuple): The RGB mean of the train dataset. You can use `~super_image.utils.metrics.calculate_mean_std` to calculate it. rgb_std (tuple): The RGB standard deviation of the train dataset. You can use `~super_image.utils.metrics.calculate_mean_std` to calculate it. no_upsampling (bool): Option to turn off upsampling. data_parallel (bool): Option to use multiple GPUs for training. \"\"\" super () . __init__ ( ** kwargs ) self . scale = scale self . n_resblocks = n_resblocks self . n_feats = n_feats self . n_colors = n_colors self . rgb_range = rgb_range self . res_scale = res_scale self . rgb_mean = rgb_mean self . rgb_std = rgb_std self . no_upsampling = no_upsampling self . data_parallel = data_parallel","title":"__init__()"},{"location":"models/edsr/#super_image.models.edsr.modeling_edsr.EdsrModel.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in super_image\\models\\edsr\\modeling_edsr.py def forward ( self , x ): x = self . head ( x ) res = self . body ( x ) res += x if self . args . no_upsampling : x = res else : x = self . tail ( res ) return x","title":"forward()"},{"location":"models/edsr/#super_image.models.edsr.modeling_edsr.EdsrModel.load_state_dict","text":"Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys Source code in super_image\\models\\edsr\\modeling_edsr.py def load_state_dict ( self , state_dict , strict = True ): own_state = self . state_dict () for name , param in state_dict . items (): if name in own_state : if isinstance ( param , nn . Parameter ): param = param . data try : own_state [ name ] . copy_ ( param ) except Exception : if name . find ( 'tail' ) == - 1 : raise RuntimeError ( f 'While copying the parameter named { name } , ' f 'whose dimensions in the model are { own_state [ name ] . size () } and ' f 'whose dimensions in the checkpoint are { param . size () } .' ) elif strict : if name . find ( 'tail' ) == - 1 : raise KeyError ( f 'unexpected key \" { name } \" in state_dict' )","title":"load_state_dict()"},{"location":"models/msrn/","text":"Multi-scale Residual Network for Image Super-Resolution (MSRN) # Overview # The MSRN model proposes a feature extraction structure called the multi-scale residual block. This module can \"adaptively detect image features at different scales\" and \"exploit the potential features of the image\". This model also applies the balanced attention (BAM) method invented by Wang et al. (2021) to further improve the results. It was introduced in the paper Multi-scale Residual Network for Image Super-Resolution by Li et al. (2018) and first released in this repository . MsrnConfig # This is the configuration class to store the configuration of a :class: ~super_image.MsrnModel . It is used to instantiate the model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the MSRN BAM architecture. Configuration objects inherit from :class: ~super_image.PretrainedConfig and can be used to control the model outputs. Read the documentation from :class: ~super_image.PretrainedConfig for more information. Examples: from super_image import MsrnModel , MsrnConfig # Initializing a configuration config = MsrnConfig ( scale = 4 , # train a model to upscale 4x bam = True , # use balanced attention (BAM) ) # Initializing a model from the configuration model = MsrnModel ( config ) # Accessing the model configuration configuration = model . config __init__ ( self , scale = None , n_blocks = 8 , n_feats = 64 , rgb_range = 255 , bam = False , rgb_mean = ( 0.4488 , 0.4371 , 0.404 ), rgb_std = ( 1.0 , 1.0 , 1.0 ), data_parallel = False , ** kwargs ) special # Parameters: Name Type Description Default scale int Scale for the model to train an upscaler/super-res model. None n_blocks int Number of blocks. 8 n_feats int Number of filters. 64 rgb_range int Range of RGB as a multiplier to the MeanShift. 255 data_parallel bool Option to use multiple GPUs for training. False bam bool Option to use balanced attention modules instead (BAM) False Source code in super_image\\models\\msrn\\configuration_msrn.py def __init__ ( self , scale = None , n_blocks = 8 , n_feats = 64 , rgb_range = 255 , bam = False , rgb_mean = DIV2K_RGB_MEAN , rgb_std = DIV2K_RGB_STD , data_parallel = False , ** kwargs ): \"\"\" Args: scale (int): Scale for the model to train an upscaler/super-res model. n_blocks (int): Number of blocks. n_feats (int): Number of filters. rgb_range (int): Range of RGB as a multiplier to the MeanShift. data_parallel (bool): Option to use multiple GPUs for training. bam (bool): Option to use balanced attention modules instead (BAM) \"\"\" super () . __init__ ( ** kwargs ) self . scale = scale self . n_blocks = n_blocks self . n_feats = n_feats self . rgb_range = rgb_range self . rgb_mean = rgb_mean self . rgb_std = rgb_std self . data_parallel = data_parallel self . bam = bam MsrnModel # config_class # This is the configuration class to store the configuration of a :class: ~super_image.MsrnModel . It is used to instantiate the model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the MSRN BAM architecture. Configuration objects inherit from :class: ~super_image.PretrainedConfig and can be used to control the model outputs. Read the documentation from :class: ~super_image.PretrainedConfig for more information. Examples: from super_image import MsrnModel , MsrnConfig # Initializing a configuration config = MsrnConfig ( scale = 4 , # train a model to upscale 4x bam = True , # use balanced attention (BAM) ) # Initializing a model from the configuration model = MsrnModel ( config ) # Accessing the model configuration configuration = model . config __init__ ( self , scale = None , n_blocks = 8 , n_feats = 64 , rgb_range = 255 , bam = False , rgb_mean = ( 0.4488 , 0.4371 , 0.404 ), rgb_std = ( 1.0 , 1.0 , 1.0 ), data_parallel = False , ** kwargs ) special # Parameters: Name Type Description Default scale int Scale for the model to train an upscaler/super-res model. None n_blocks int Number of blocks. 8 n_feats int Number of filters. 64 rgb_range int Range of RGB as a multiplier to the MeanShift. 255 data_parallel bool Option to use multiple GPUs for training. False bam bool Option to use balanced attention modules instead (BAM) False Source code in super_image\\models\\msrn\\modeling_msrn.py def __init__ ( self , scale = None , n_blocks = 8 , n_feats = 64 , rgb_range = 255 , bam = False , rgb_mean = DIV2K_RGB_MEAN , rgb_std = DIV2K_RGB_STD , data_parallel = False , ** kwargs ): \"\"\" Args: scale (int): Scale for the model to train an upscaler/super-res model. n_blocks (int): Number of blocks. n_feats (int): Number of filters. rgb_range (int): Range of RGB as a multiplier to the MeanShift. data_parallel (bool): Option to use multiple GPUs for training. bam (bool): Option to use balanced attention modules instead (BAM) \"\"\" super () . __init__ ( ** kwargs ) self . scale = scale self . n_blocks = n_blocks self . n_feats = n_feats self . rgb_range = rgb_range self . rgb_mean = rgb_mean self . rgb_std = rgb_std self . data_parallel = data_parallel self . bam = bam forward ( self , x ) # Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in super_image\\models\\msrn\\modeling_msrn.py def forward ( self , x ): # x = self.sub_mean(x) x = self . head ( x ) res = x MSRB_out = [] for i in range ( self . n_blocks ): x = self . body [ i ]( x ) MSRB_out . append ( x ) MSRB_out . append ( res ) res = torch . cat ( MSRB_out , 1 ) x = self . tail ( res ) # x = self.add_mean(x) return x load_state_dict ( self , state_dict , strict = True ) # Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys Source code in super_image\\models\\msrn\\modeling_msrn.py def load_state_dict ( self , state_dict , strict = True ): own_state = self . state_dict () for name , param in state_dict . items (): if name in own_state : if isinstance ( param , nn . Parameter ): param = param . data try : own_state [ name ] . copy_ ( param ) except Exception : if name . find ( 'tail' ) >= 0 : print ( 'Replace pre-trained upsampler to new one...' ) else : raise RuntimeError ( f 'While copying the parameter named { name } , ' f 'whose dimensions in the model are { own_state [ name ] . size () } and ' f 'whose dimensions in the checkpoint are { param . size () } .' ) elif strict : if name . find ( 'tail' ) == - 1 : raise KeyError ( f 'unexpected key \" { name } \" in state_dict' ) if strict : missing = set ( own_state . keys ()) - set ( state_dict . keys ()) if len ( missing ) > 0 : raise KeyError ( f 'missing keys in state_dict: \" { missing } \"' )","title":"MSRN"},{"location":"models/msrn/#multi-scale-residual-network-for-image-super-resolution-msrn","text":"","title":"Multi-scale Residual Network for Image Super-Resolution (MSRN)"},{"location":"models/msrn/#overview","text":"The MSRN model proposes a feature extraction structure called the multi-scale residual block. This module can \"adaptively detect image features at different scales\" and \"exploit the potential features of the image\". This model also applies the balanced attention (BAM) method invented by Wang et al. (2021) to further improve the results. It was introduced in the paper Multi-scale Residual Network for Image Super-Resolution by Li et al. (2018) and first released in this repository .","title":"Overview"},{"location":"models/msrn/#msrnconfig","text":"This is the configuration class to store the configuration of a :class: ~super_image.MsrnModel . It is used to instantiate the model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the MSRN BAM architecture. Configuration objects inherit from :class: ~super_image.PretrainedConfig and can be used to control the model outputs. Read the documentation from :class: ~super_image.PretrainedConfig for more information. Examples: from super_image import MsrnModel , MsrnConfig # Initializing a configuration config = MsrnConfig ( scale = 4 , # train a model to upscale 4x bam = True , # use balanced attention (BAM) ) # Initializing a model from the configuration model = MsrnModel ( config ) # Accessing the model configuration configuration = model . config","title":"MsrnConfig"},{"location":"models/msrn/#super_image.models.msrn.configuration_msrn.MsrnConfig.__init__","text":"Parameters: Name Type Description Default scale int Scale for the model to train an upscaler/super-res model. None n_blocks int Number of blocks. 8 n_feats int Number of filters. 64 rgb_range int Range of RGB as a multiplier to the MeanShift. 255 data_parallel bool Option to use multiple GPUs for training. False bam bool Option to use balanced attention modules instead (BAM) False Source code in super_image\\models\\msrn\\configuration_msrn.py def __init__ ( self , scale = None , n_blocks = 8 , n_feats = 64 , rgb_range = 255 , bam = False , rgb_mean = DIV2K_RGB_MEAN , rgb_std = DIV2K_RGB_STD , data_parallel = False , ** kwargs ): \"\"\" Args: scale (int): Scale for the model to train an upscaler/super-res model. n_blocks (int): Number of blocks. n_feats (int): Number of filters. rgb_range (int): Range of RGB as a multiplier to the MeanShift. data_parallel (bool): Option to use multiple GPUs for training. bam (bool): Option to use balanced attention modules instead (BAM) \"\"\" super () . __init__ ( ** kwargs ) self . scale = scale self . n_blocks = n_blocks self . n_feats = n_feats self . rgb_range = rgb_range self . rgb_mean = rgb_mean self . rgb_std = rgb_std self . data_parallel = data_parallel self . bam = bam","title":"__init__()"},{"location":"models/msrn/#msrnmodel","text":"","title":"MsrnModel"},{"location":"models/msrn/#super_image.models.msrn.modeling_msrn.MsrnModel.config_class","text":"This is the configuration class to store the configuration of a :class: ~super_image.MsrnModel . It is used to instantiate the model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the MSRN BAM architecture. Configuration objects inherit from :class: ~super_image.PretrainedConfig and can be used to control the model outputs. Read the documentation from :class: ~super_image.PretrainedConfig for more information. Examples: from super_image import MsrnModel , MsrnConfig # Initializing a configuration config = MsrnConfig ( scale = 4 , # train a model to upscale 4x bam = True , # use balanced attention (BAM) ) # Initializing a model from the configuration model = MsrnModel ( config ) # Accessing the model configuration configuration = model . config","title":"config_class"},{"location":"models/msrn/#super_image.models.msrn.modeling_msrn.MsrnModel.config_class.__init__","text":"Parameters: Name Type Description Default scale int Scale for the model to train an upscaler/super-res model. None n_blocks int Number of blocks. 8 n_feats int Number of filters. 64 rgb_range int Range of RGB as a multiplier to the MeanShift. 255 data_parallel bool Option to use multiple GPUs for training. False bam bool Option to use balanced attention modules instead (BAM) False Source code in super_image\\models\\msrn\\modeling_msrn.py def __init__ ( self , scale = None , n_blocks = 8 , n_feats = 64 , rgb_range = 255 , bam = False , rgb_mean = DIV2K_RGB_MEAN , rgb_std = DIV2K_RGB_STD , data_parallel = False , ** kwargs ): \"\"\" Args: scale (int): Scale for the model to train an upscaler/super-res model. n_blocks (int): Number of blocks. n_feats (int): Number of filters. rgb_range (int): Range of RGB as a multiplier to the MeanShift. data_parallel (bool): Option to use multiple GPUs for training. bam (bool): Option to use balanced attention modules instead (BAM) \"\"\" super () . __init__ ( ** kwargs ) self . scale = scale self . n_blocks = n_blocks self . n_feats = n_feats self . rgb_range = rgb_range self . rgb_mean = rgb_mean self . rgb_std = rgb_std self . data_parallel = data_parallel self . bam = bam","title":"__init__()"},{"location":"models/msrn/#super_image.models.msrn.modeling_msrn.MsrnModel.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in super_image\\models\\msrn\\modeling_msrn.py def forward ( self , x ): # x = self.sub_mean(x) x = self . head ( x ) res = x MSRB_out = [] for i in range ( self . n_blocks ): x = self . body [ i ]( x ) MSRB_out . append ( x ) MSRB_out . append ( res ) res = torch . cat ( MSRB_out , 1 ) x = self . tail ( res ) # x = self.add_mean(x) return x","title":"forward()"},{"location":"models/msrn/#super_image.models.msrn.modeling_msrn.MsrnModel.load_state_dict","text":"Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys Source code in super_image\\models\\msrn\\modeling_msrn.py def load_state_dict ( self , state_dict , strict = True ): own_state = self . state_dict () for name , param in state_dict . items (): if name in own_state : if isinstance ( param , nn . Parameter ): param = param . data try : own_state [ name ] . copy_ ( param ) except Exception : if name . find ( 'tail' ) >= 0 : print ( 'Replace pre-trained upsampler to new one...' ) else : raise RuntimeError ( f 'While copying the parameter named { name } , ' f 'whose dimensions in the model are { own_state [ name ] . size () } and ' f 'whose dimensions in the checkpoint are { param . size () } .' ) elif strict : if name . find ( 'tail' ) == - 1 : raise KeyError ( f 'unexpected key \" { name } \" in state_dict' ) if strict : missing = set ( own_state . keys ()) - set ( state_dict . keys ()) if len ( missing ) > 0 : raise KeyError ( f 'missing keys in state_dict: \" { missing } \"' )","title":"load_state_dict()"},{"location":"models/pan/","text":"Pixel Attention Network (PAN) # Overview # The PAN model proposes a a lightweight convolutional neural network for image super resolution. Pixel attention (PA) is similar to channel attention and spatial attention in formulation. PA however produces 3D attention maps instead of a 1D attention vector or a 2D map. This attention scheme introduces fewer additional parameters but generates better SR results. This model also applies the balanced attention (BAM) method invented by Wang et al. (2021) to further improve the results. It was introduced in the paper Efficient Image Super-Resolution Using Pixel Attention by Zhao et al. (2020) and first released in this repository . PanConfig # PanModel # forward ( self , x ) # Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in super_image\\models\\pan\\modeling_pan.py def forward ( self , x ): fea = self . conv_first ( x ) trunk = self . trunk_conv ( self . SCPA_trunk ( fea )) fea = fea + trunk if self . scale == 2 or self . scale == 3 : fea = self . upconv1 ( functional . interpolate ( fea , scale_factor = self . scale , mode = 'nearest' )) fea = self . lrelu ( self . att1 ( fea )) fea = self . lrelu ( self . HRconv1 ( fea )) elif self . scale == 4 : fea = self . upconv1 ( functional . interpolate ( fea , scale_factor = 2 , mode = 'nearest' )) fea = self . lrelu ( self . att1 ( fea )) fea = self . lrelu ( self . HRconv1 ( fea )) fea = self . upconv2 ( functional . interpolate ( fea , scale_factor = 2 , mode = 'nearest' )) fea = self . lrelu ( self . att2 ( fea )) fea = self . lrelu ( self . HRconv2 ( fea )) out = self . conv_last ( fea ) ilr = functional . interpolate ( x , scale_factor = self . scale , mode = 'bilinear' , align_corners = False ) out = out + ilr return out load_state_dict ( self , state_dict , strict = True ) # Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys Source code in super_image\\models\\pan\\modeling_pan.py def load_state_dict ( self , state_dict , strict = True ): own_state = self . state_dict () for name , param in state_dict . items (): if name in own_state : if isinstance ( param , nn . Parameter ): param = param . data try : own_state [ name ] . copy_ ( param ) except Exception : if name . find ( 'tail' ) >= 0 : print ( 'Replace pre-trained upsampler to new one...' ) else : raise RuntimeError ( f 'While copying the parameter named { name } , ' f 'whose dimensions in the model are { own_state [ name ] . size () } and ' f 'whose dimensions in the checkpoint are { param . size () } .' ) elif strict : if name . find ( 'tail' ) == - 1 : raise KeyError ( f 'unexpected key \" { name } \" in state_dict' ) if strict : missing = set ( own_state . keys ()) - set ( state_dict . keys ()) if len ( missing ) > 0 : raise KeyError ( f 'missing keys in state_dict: \" { missing } \"' )","title":"PAN"},{"location":"models/pan/#pixel-attention-network-pan","text":"","title":"Pixel Attention Network (PAN)"},{"location":"models/pan/#overview","text":"The PAN model proposes a a lightweight convolutional neural network for image super resolution. Pixel attention (PA) is similar to channel attention and spatial attention in formulation. PA however produces 3D attention maps instead of a 1D attention vector or a 2D map. This attention scheme introduces fewer additional parameters but generates better SR results. This model also applies the balanced attention (BAM) method invented by Wang et al. (2021) to further improve the results. It was introduced in the paper Efficient Image Super-Resolution Using Pixel Attention by Zhao et al. (2020) and first released in this repository .","title":"Overview"},{"location":"models/pan/#panconfig","text":"","title":"PanConfig"},{"location":"models/pan/#panmodel","text":"","title":"PanModel"},{"location":"models/pan/#super_image.models.pan.modeling_pan.PanModel.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in super_image\\models\\pan\\modeling_pan.py def forward ( self , x ): fea = self . conv_first ( x ) trunk = self . trunk_conv ( self . SCPA_trunk ( fea )) fea = fea + trunk if self . scale == 2 or self . scale == 3 : fea = self . upconv1 ( functional . interpolate ( fea , scale_factor = self . scale , mode = 'nearest' )) fea = self . lrelu ( self . att1 ( fea )) fea = self . lrelu ( self . HRconv1 ( fea )) elif self . scale == 4 : fea = self . upconv1 ( functional . interpolate ( fea , scale_factor = 2 , mode = 'nearest' )) fea = self . lrelu ( self . att1 ( fea )) fea = self . lrelu ( self . HRconv1 ( fea )) fea = self . upconv2 ( functional . interpolate ( fea , scale_factor = 2 , mode = 'nearest' )) fea = self . lrelu ( self . att2 ( fea )) fea = self . lrelu ( self . HRconv2 ( fea )) out = self . conv_last ( fea ) ilr = functional . interpolate ( x , scale_factor = self . scale , mode = 'bilinear' , align_corners = False ) out = out + ilr return out","title":"forward()"},{"location":"models/pan/#super_image.models.pan.modeling_pan.PanModel.load_state_dict","text":"Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys Source code in super_image\\models\\pan\\modeling_pan.py def load_state_dict ( self , state_dict , strict = True ): own_state = self . state_dict () for name , param in state_dict . items (): if name in own_state : if isinstance ( param , nn . Parameter ): param = param . data try : own_state [ name ] . copy_ ( param ) except Exception : if name . find ( 'tail' ) >= 0 : print ( 'Replace pre-trained upsampler to new one...' ) else : raise RuntimeError ( f 'While copying the parameter named { name } , ' f 'whose dimensions in the model are { own_state [ name ] . size () } and ' f 'whose dimensions in the checkpoint are { param . size () } .' ) elif strict : if name . find ( 'tail' ) == - 1 : raise KeyError ( f 'unexpected key \" { name } \" in state_dict' ) if strict : missing = set ( own_state . keys ()) - set ( state_dict . keys ()) if len ( missing ) > 0 : raise KeyError ( f 'missing keys in state_dict: \" { missing } \"' )","title":"load_state_dict()"},{"location":"reference/cli/","text":"Module that contains the command line application. get_parser () # Return the CLI argument parser. Returns: Type Description ArgumentParser An argparse parser. Source code in super_image\\cli.py def get_parser () -> argparse . ArgumentParser : \"\"\" Return the CLI argument parser. Returns: An argparse parser. \"\"\" return argparse . ArgumentParser ( prog = \"super-image\" ) main ( args = None ) # Run the main program. This function is executed when you type super-image or python -m super_image . Parameters: Name Type Description Default args Optional[List[str]] Arguments passed from the command line. None Returns: Type Description int An exit code. Source code in super_image\\cli.py def main ( args : Optional [ List [ str ]] = None ) -> int : \"\"\" Run the main program. This function is executed when you type `super-image` or `python -m super_image`. Arguments: args: Arguments passed from the command line. Returns: An exit code. \"\"\" parser = get_parser () opts = parser . parse_args ( args = args ) print ( opts ) # noqa: WPS421 (side-effect in main is fine) return 0","title":"CLI"},{"location":"reference/cli/#super_image.cli.get_parser","text":"Return the CLI argument parser. Returns: Type Description ArgumentParser An argparse parser. Source code in super_image\\cli.py def get_parser () -> argparse . ArgumentParser : \"\"\" Return the CLI argument parser. Returns: An argparse parser. \"\"\" return argparse . ArgumentParser ( prog = \"super-image\" )","title":"get_parser()"},{"location":"reference/cli/#super_image.cli.main","text":"Run the main program. This function is executed when you type super-image or python -m super_image . Parameters: Name Type Description Default args Optional[List[str]] Arguments passed from the command line. None Returns: Type Description int An exit code. Source code in super_image\\cli.py def main ( args : Optional [ List [ str ]] = None ) -> int : \"\"\" Run the main program. This function is executed when you type `super-image` or `python -m super_image`. Arguments: args: Arguments passed from the command line. Returns: An exit code. \"\"\" parser = get_parser () opts = parser . parse_args ( args = args ) print ( opts ) # noqa: WPS421 (side-effect in main is fine) return 0","title":"main()"},{"location":"reference/configuration/","text":"Configuration # The base class PretrainedConfig implements the common methods for loading/saving a configuration either from a local file or directory, or from a pretrained model configuration provided by the library (downloaded from HuggingFace's Hub ). PretrainedConfig # to_json_string : str property readonly # Serializes this instance to a JSON string. Returns: Type Description str :obj: str : String containing all the attributes that make up this configuration instance in JSON format. from_dict ( config_dict , ** kwargs ) classmethod # Instantiates a :class: ~super_image.PretrainedConfig from a Python dictionary of parameters. Parameters: Name Type Description Default config_dict Dict[str, Any] obj: Dict[str, Any] ): Dictionary that will be used to instantiate the configuration object. Such a dictionary can be retrieved from a pretrained checkpoint by leveraging the :func: ~super_image.PretrainedConfig.get_config_dict method. required kwargs obj: Dict[str, Any] ): Additional parameters from which to initialize the configuration object. {} Returns: Type Description Tuple[PretrainedConfig, Dict[str, Any]] :class: PretrainedConfig : The configuration object instantiated from those parameters. Source code in super_image\\configuration_utils.py @classmethod def from_dict ( cls , config_dict : Dict [ str , Any ], ** kwargs ) -> Tuple [ \"PretrainedConfig\" , Dict [ str , Any ]]: \"\"\" Instantiates a :class:`~super_image.PretrainedConfig` from a Python dictionary of parameters. Args: config_dict (:obj:`Dict[str, Any]`): Dictionary that will be used to instantiate the configuration object. Such a dictionary can be retrieved from a pretrained checkpoint by leveraging the :func:`~super_image.PretrainedConfig.get_config_dict` method. kwargs (:obj:`Dict[str, Any]`): Additional parameters from which to initialize the configuration object. Returns: :class:`PretrainedConfig`: The configuration object instantiated from those parameters. \"\"\" config = cls ( ** config_dict ) # Update config with kwargs if needed to_remove = [] for key , value in kwargs . items (): if hasattr ( config , key ): setattr ( config , key , value ) to_remove . append ( key ) for key in to_remove : kwargs . pop ( key , None ) logger . info ( f \"Model config { config } \" ) return config , kwargs from_json_file ( json_file ) classmethod # Instantiates a :class: ~super_image.PretrainedConfig from the path to a JSON file of parameters. Parameters: Name Type Description Default json_file Union[str, os.PathLike] obj: str or :obj: os.PathLike ): Path to the JSON file containing the parameters. required Returns: Type Description PretrainedConfig :class: PretrainedConfig : The configuration object instantiated from that JSON file. Source code in super_image\\configuration_utils.py @classmethod def from_json_file ( cls , json_file : Union [ str , os . PathLike ]) -> \"PretrainedConfig\" : \"\"\" Instantiates a :class:`~super_image.PretrainedConfig` from the path to a JSON file of parameters. Args: json_file (:obj:`str` or :obj:`os.PathLike`): Path to the JSON file containing the parameters. Returns: :class:`PretrainedConfig`: The configuration object instantiated from that JSON file. \"\"\" config_dict = cls . _dict_from_json_file ( json_file ) return cls ( ** config_dict ) get_config_dict ( pretrained_model_name_or_path , ** kwargs ) classmethod # From a pretrained_model_name_or_path , resolve to a dictionary of parameters, to be used for instantiating a :class: ~super_image.PretrainedConfig using from_dict . Parameters: Name Type Description Default pretrained_model_name_or_path Union[str, os.PathLike] obj: str or :obj: os.PathLike ): The identifier of the pre-trained checkpoint from which we want the dictionary of parameters. required Returns: Type Description Tuple[Dict[str, Any], Dict[str, Any]] :obj: Tuple[Dict, Dict] : The dictionary(ies) that will be used to instantiate the configuration object. Source code in super_image\\configuration_utils.py @classmethod def get_config_dict ( cls , pretrained_model_name_or_path : Union [ str , os . PathLike ], ** kwargs ) -> Tuple [ Dict [ str , Any ], Dict [ str , Any ]]: \"\"\" From a ``pretrained_model_name_or_path``, resolve to a dictionary of parameters, to be used for instantiating a :class:`~super_image.PretrainedConfig` using ``from_dict``. Parameters: pretrained_model_name_or_path (:obj:`str` or :obj:`os.PathLike`): The identifier of the pre-trained checkpoint from which we want the dictionary of parameters. Returns: :obj:`Tuple[Dict, Dict]`: The dictionary(ies) that will be used to instantiate the configuration object. \"\"\" scale = kwargs . pop ( \"scale\" , None ) cache_dir = kwargs . pop ( \"cache_dir\" , None ) revision = kwargs . pop ( \"revision\" , None ) pretrained_model_name_or_path = str ( pretrained_model_name_or_path ) if os . path . isdir ( pretrained_model_name_or_path ): config_file = os . path . join ( pretrained_model_name_or_path , CONFIG_NAME ) elif os . path . isfile ( pretrained_model_name_or_path ) or is_remote_url ( pretrained_model_name_or_path ): config_file = pretrained_model_name_or_path else : config_file = get_model_url ( pretrained_model_name_or_path , filename = CONFIG_NAME , revision = revision ) try : # Load from URL or cache if already cached resolved_config_file = get_model_path ( config_file , cache_dir = cache_dir ) # Load config dict config_dict = cls . _dict_from_json_file ( resolved_config_file ) if scale is not None : config_dict [ 'scale' ] = scale except EnvironmentError as err : logger . error ( err ) msg = ( f \"Can't load config for ' { pretrained_model_name_or_path } '. Make sure that: \\n\\n \" f \"- ' { pretrained_model_name_or_path } ' is a correct model identifier \\n\\n \" f \"- or ' { pretrained_model_name_or_path } ' is the correct path to a directory containing a { CONFIG_NAME } file \\n\\n \" ) raise EnvironmentError ( msg ) except json . JSONDecodeError : msg = ( f \"Couldn't reach server at ' { config_file } ' to download configuration file or \" \"configuration file is not a valid JSON file. \" f \"Please check network or file content here: { resolved_config_file } .\" ) raise EnvironmentError ( msg ) if resolved_config_file == config_file : logger . info ( f \"loading configuration file { config_file } \" ) else : logger . info ( f \"loading configuration file { config_file } from cache at { resolved_config_file } \" ) return config_dict , kwargs save_pretrained ( self , save_directory ) # Save a configuration object to the directory save_directory , so that it can be re-loaded using the :func: ~super_image.PretrainedConfig.from_pretrained class method. Parameters: Name Type Description Default save_directory Union[str, os.PathLike] obj: str or :obj: os.PathLike ): Directory where the configuration JSON file will be saved (will be created if it does not exist). required Source code in super_image\\configuration_utils.py def save_pretrained ( self , save_directory : Union [ str , os . PathLike ]): \"\"\" Save a configuration object to the directory ``save_directory``, so that it can be re-loaded using the :func:`~super_image.PretrainedConfig.from_pretrained` class method. Args: save_directory (:obj:`str` or :obj:`os.PathLike`): Directory where the configuration JSON file will be saved (will be created if it does not exist). \"\"\" if os . path . isfile ( save_directory ): raise AssertionError ( f \"Provided path ( { save_directory } ) should be a directory, not a file\" ) os . makedirs ( save_directory , exist_ok = True ) # If we save using the predefined names, we can load using `from_pretrained` output_config_file = os . path . join ( save_directory , CONFIG_NAME ) self . to_json_file ( output_config_file ) logger . info ( f \"Configuration saved in { output_config_file } \" ) to_dict ( self ) # Serializes this instance to a Python dictionary. Returns: Type Description Dict[str, Any] :obj: Dict[str, Any] : Dictionary of all the attributes that make up this configuration instance. Source code in super_image\\configuration_utils.py def to_dict ( self ) -> Dict [ str , Any ]: \"\"\" Serializes this instance to a Python dictionary. Returns: :obj:`Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance. \"\"\" output = copy . deepcopy ( self . __dict__ ) if hasattr ( self . __class__ , \"model_type\" ): output [ \"model_type\" ] = self . __class__ . model_type return output to_json_file ( self , json_file_path ) # Save this instance to a JSON file. Parameters: Name Type Description Default json_file_path Union[str, os.PathLike] obj: str or :obj: os.PathLike ): Path to the JSON file in which this configuration instance's parameters will be saved. required Source code in super_image\\configuration_utils.py def to_json_file ( self , json_file_path : Union [ str , os . PathLike ]): \"\"\" Save this instance to a JSON file. Args: json_file_path (:obj:`str` or :obj:`os.PathLike`): Path to the JSON file in which this configuration instance's parameters will be saved. \"\"\" with open ( json_file_path , \"w\" , encoding = \"utf-8\" ) as writer : writer . write ( self . to_json_string )","title":"Configuration"},{"location":"reference/configuration/#configuration","text":"The base class PretrainedConfig implements the common methods for loading/saving a configuration either from a local file or directory, or from a pretrained model configuration provided by the library (downloaded from HuggingFace's Hub ).","title":"Configuration"},{"location":"reference/configuration/#pretrainedconfig","text":"","title":"PretrainedConfig"},{"location":"reference/configuration/#super_image.configuration_utils.PretrainedConfig.to_json_string","text":"Serializes this instance to a JSON string. Returns: Type Description str :obj: str : String containing all the attributes that make up this configuration instance in JSON format.","title":"to_json_string"},{"location":"reference/configuration/#super_image.configuration_utils.PretrainedConfig.from_dict","text":"Instantiates a :class: ~super_image.PretrainedConfig from a Python dictionary of parameters. Parameters: Name Type Description Default config_dict Dict[str, Any] obj: Dict[str, Any] ): Dictionary that will be used to instantiate the configuration object. Such a dictionary can be retrieved from a pretrained checkpoint by leveraging the :func: ~super_image.PretrainedConfig.get_config_dict method. required kwargs obj: Dict[str, Any] ): Additional parameters from which to initialize the configuration object. {} Returns: Type Description Tuple[PretrainedConfig, Dict[str, Any]] :class: PretrainedConfig : The configuration object instantiated from those parameters. Source code in super_image\\configuration_utils.py @classmethod def from_dict ( cls , config_dict : Dict [ str , Any ], ** kwargs ) -> Tuple [ \"PretrainedConfig\" , Dict [ str , Any ]]: \"\"\" Instantiates a :class:`~super_image.PretrainedConfig` from a Python dictionary of parameters. Args: config_dict (:obj:`Dict[str, Any]`): Dictionary that will be used to instantiate the configuration object. Such a dictionary can be retrieved from a pretrained checkpoint by leveraging the :func:`~super_image.PretrainedConfig.get_config_dict` method. kwargs (:obj:`Dict[str, Any]`): Additional parameters from which to initialize the configuration object. Returns: :class:`PretrainedConfig`: The configuration object instantiated from those parameters. \"\"\" config = cls ( ** config_dict ) # Update config with kwargs if needed to_remove = [] for key , value in kwargs . items (): if hasattr ( config , key ): setattr ( config , key , value ) to_remove . append ( key ) for key in to_remove : kwargs . pop ( key , None ) logger . info ( f \"Model config { config } \" ) return config , kwargs","title":"from_dict()"},{"location":"reference/configuration/#super_image.configuration_utils.PretrainedConfig.from_json_file","text":"Instantiates a :class: ~super_image.PretrainedConfig from the path to a JSON file of parameters. Parameters: Name Type Description Default json_file Union[str, os.PathLike] obj: str or :obj: os.PathLike ): Path to the JSON file containing the parameters. required Returns: Type Description PretrainedConfig :class: PretrainedConfig : The configuration object instantiated from that JSON file. Source code in super_image\\configuration_utils.py @classmethod def from_json_file ( cls , json_file : Union [ str , os . PathLike ]) -> \"PretrainedConfig\" : \"\"\" Instantiates a :class:`~super_image.PretrainedConfig` from the path to a JSON file of parameters. Args: json_file (:obj:`str` or :obj:`os.PathLike`): Path to the JSON file containing the parameters. Returns: :class:`PretrainedConfig`: The configuration object instantiated from that JSON file. \"\"\" config_dict = cls . _dict_from_json_file ( json_file ) return cls ( ** config_dict )","title":"from_json_file()"},{"location":"reference/configuration/#super_image.configuration_utils.PretrainedConfig.get_config_dict","text":"From a pretrained_model_name_or_path , resolve to a dictionary of parameters, to be used for instantiating a :class: ~super_image.PretrainedConfig using from_dict . Parameters: Name Type Description Default pretrained_model_name_or_path Union[str, os.PathLike] obj: str or :obj: os.PathLike ): The identifier of the pre-trained checkpoint from which we want the dictionary of parameters. required Returns: Type Description Tuple[Dict[str, Any], Dict[str, Any]] :obj: Tuple[Dict, Dict] : The dictionary(ies) that will be used to instantiate the configuration object. Source code in super_image\\configuration_utils.py @classmethod def get_config_dict ( cls , pretrained_model_name_or_path : Union [ str , os . PathLike ], ** kwargs ) -> Tuple [ Dict [ str , Any ], Dict [ str , Any ]]: \"\"\" From a ``pretrained_model_name_or_path``, resolve to a dictionary of parameters, to be used for instantiating a :class:`~super_image.PretrainedConfig` using ``from_dict``. Parameters: pretrained_model_name_or_path (:obj:`str` or :obj:`os.PathLike`): The identifier of the pre-trained checkpoint from which we want the dictionary of parameters. Returns: :obj:`Tuple[Dict, Dict]`: The dictionary(ies) that will be used to instantiate the configuration object. \"\"\" scale = kwargs . pop ( \"scale\" , None ) cache_dir = kwargs . pop ( \"cache_dir\" , None ) revision = kwargs . pop ( \"revision\" , None ) pretrained_model_name_or_path = str ( pretrained_model_name_or_path ) if os . path . isdir ( pretrained_model_name_or_path ): config_file = os . path . join ( pretrained_model_name_or_path , CONFIG_NAME ) elif os . path . isfile ( pretrained_model_name_or_path ) or is_remote_url ( pretrained_model_name_or_path ): config_file = pretrained_model_name_or_path else : config_file = get_model_url ( pretrained_model_name_or_path , filename = CONFIG_NAME , revision = revision ) try : # Load from URL or cache if already cached resolved_config_file = get_model_path ( config_file , cache_dir = cache_dir ) # Load config dict config_dict = cls . _dict_from_json_file ( resolved_config_file ) if scale is not None : config_dict [ 'scale' ] = scale except EnvironmentError as err : logger . error ( err ) msg = ( f \"Can't load config for ' { pretrained_model_name_or_path } '. Make sure that: \\n\\n \" f \"- ' { pretrained_model_name_or_path } ' is a correct model identifier \\n\\n \" f \"- or ' { pretrained_model_name_or_path } ' is the correct path to a directory containing a { CONFIG_NAME } file \\n\\n \" ) raise EnvironmentError ( msg ) except json . JSONDecodeError : msg = ( f \"Couldn't reach server at ' { config_file } ' to download configuration file or \" \"configuration file is not a valid JSON file. \" f \"Please check network or file content here: { resolved_config_file } .\" ) raise EnvironmentError ( msg ) if resolved_config_file == config_file : logger . info ( f \"loading configuration file { config_file } \" ) else : logger . info ( f \"loading configuration file { config_file } from cache at { resolved_config_file } \" ) return config_dict , kwargs","title":"get_config_dict()"},{"location":"reference/configuration/#super_image.configuration_utils.PretrainedConfig.save_pretrained","text":"Save a configuration object to the directory save_directory , so that it can be re-loaded using the :func: ~super_image.PretrainedConfig.from_pretrained class method. Parameters: Name Type Description Default save_directory Union[str, os.PathLike] obj: str or :obj: os.PathLike ): Directory where the configuration JSON file will be saved (will be created if it does not exist). required Source code in super_image\\configuration_utils.py def save_pretrained ( self , save_directory : Union [ str , os . PathLike ]): \"\"\" Save a configuration object to the directory ``save_directory``, so that it can be re-loaded using the :func:`~super_image.PretrainedConfig.from_pretrained` class method. Args: save_directory (:obj:`str` or :obj:`os.PathLike`): Directory where the configuration JSON file will be saved (will be created if it does not exist). \"\"\" if os . path . isfile ( save_directory ): raise AssertionError ( f \"Provided path ( { save_directory } ) should be a directory, not a file\" ) os . makedirs ( save_directory , exist_ok = True ) # If we save using the predefined names, we can load using `from_pretrained` output_config_file = os . path . join ( save_directory , CONFIG_NAME ) self . to_json_file ( output_config_file ) logger . info ( f \"Configuration saved in { output_config_file } \" )","title":"save_pretrained()"},{"location":"reference/configuration/#super_image.configuration_utils.PretrainedConfig.to_dict","text":"Serializes this instance to a Python dictionary. Returns: Type Description Dict[str, Any] :obj: Dict[str, Any] : Dictionary of all the attributes that make up this configuration instance. Source code in super_image\\configuration_utils.py def to_dict ( self ) -> Dict [ str , Any ]: \"\"\" Serializes this instance to a Python dictionary. Returns: :obj:`Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance. \"\"\" output = copy . deepcopy ( self . __dict__ ) if hasattr ( self . __class__ , \"model_type\" ): output [ \"model_type\" ] = self . __class__ . model_type return output","title":"to_dict()"},{"location":"reference/configuration/#super_image.configuration_utils.PretrainedConfig.to_json_file","text":"Save this instance to a JSON file. Parameters: Name Type Description Default json_file_path Union[str, os.PathLike] obj: str or :obj: os.PathLike ): Path to the JSON file in which this configuration instance's parameters will be saved. required Source code in super_image\\configuration_utils.py def to_json_file ( self , json_file_path : Union [ str , os . PathLike ]): \"\"\" Save this instance to a JSON file. Args: json_file_path (:obj:`str` or :obj:`os.PathLike`): Path to the JSON file in which this configuration instance's parameters will be saved. \"\"\" with open ( json_file_path , \"w\" , encoding = \"utf-8\" ) as writer : writer . write ( self . to_json_string )","title":"to_json_file()"},{"location":"reference/models/","text":"Models # The base class PreTrainedModel implements the common methods for loading/saving a model either from a local file or directory, or from a pretrained model configuration provided by the library (downloaded from HuggingFace's Hub ). PreTrainedModel # save_pretrained ( self , save_directory , save_config = True , state_dict = None , save_function =< function save at 0x000001C1C08B4318 > ) # Save a model and its configuration file to a directory, so that it can be re-loaded using the :func: ~super_image.PreTrainedModel.from_pretrained`` class method. Parameters: Name Type Description Default save_directory Union[str, os.PathLike] obj: str or :obj: os.PathLike ): Directory to which to save. Will be created if it doesn't exist. required save_config bool obj: bool , optional , defaults to :obj: True ): Whether or not to save the config of the model. Useful when in distributed training like TPUs and need to call this function on all processes. In this case, set :obj: save_config=True only on the main process to avoid race conditions. True state_dict Optional[dict] obj: torch.Tensor ): The state dictionary of the model to save. Will default to :obj: self.state_dict() , but can be used to only save parts of the model or if special precautions need to be taken when recovering the state dictionary of a model (like when using model parallelism). None save_function Callable obj: Callable ): The function to use to save the state dictionary. When we need to replace :obj: torch.save by another method. <function save at 0x000001C1C08B4318> Source code in super_image\\modeling_utils.py def save_pretrained ( self , save_directory : Union [ str , os . PathLike ], save_config : bool = True , state_dict : Optional [ dict ] = None , save_function : Callable = torch . save , ): \"\"\" Save a model and its configuration file to a directory, so that it can be re-loaded using the `:func:`~super_image.PreTrainedModel.from_pretrained`` class method. Arguments: save_directory (:obj:`str` or :obj:`os.PathLike`): Directory to which to save. Will be created if it doesn't exist. save_config (:obj:`bool`, `optional`, defaults to :obj:`True`): Whether or not to save the config of the model. Useful when in distributed training like TPUs and need to call this function on all processes. In this case, set :obj:`save_config=True` only on the main process to avoid race conditions. state_dict (nested dictionary of :obj:`torch.Tensor`): The state dictionary of the model to save. Will default to :obj:`self.state_dict()`, but can be used to only save parts of the model or if special precautions need to be taken when recovering the state dictionary of a model (like when using model parallelism). save_function (:obj:`Callable`): The function to use to save the state dictionary. When we need to replace :obj:`torch.save` by another method. \"\"\" if os . path . isfile ( save_directory ): logger . error ( f \"Provided path ( { save_directory } ) should be a directory, not a file\" ) return os . makedirs ( save_directory , exist_ok = True ) model_to_save = self # Setup scale scale = self . config . scale if scale is not None : weights_name = WEIGHTS_NAME_SCALE . format ( scale = scale ) else : weights_name = WEIGHTS_NAME # Save the config if save_config : model_to_save . config . save_pretrained ( save_directory ) # Save the model if state_dict is None : state_dict = model_to_save . state_dict () # If we save using the predefined names, we can load using `from_pretrained` output_model_file = os . path . join ( save_directory , weights_name ) save_function ( state_dict , output_model_file ) logger . info ( f \"Model weights saved in { output_model_file } \" )","title":"Models"},{"location":"reference/models/#models","text":"The base class PreTrainedModel implements the common methods for loading/saving a model either from a local file or directory, or from a pretrained model configuration provided by the library (downloaded from HuggingFace's Hub ).","title":"Models"},{"location":"reference/models/#pretrainedmodel","text":"","title":"PreTrainedModel"},{"location":"reference/models/#super_image.modeling_utils.PreTrainedModel.save_pretrained","text":"Save a model and its configuration file to a directory, so that it can be re-loaded using the :func: ~super_image.PreTrainedModel.from_pretrained`` class method. Parameters: Name Type Description Default save_directory Union[str, os.PathLike] obj: str or :obj: os.PathLike ): Directory to which to save. Will be created if it doesn't exist. required save_config bool obj: bool , optional , defaults to :obj: True ): Whether or not to save the config of the model. Useful when in distributed training like TPUs and need to call this function on all processes. In this case, set :obj: save_config=True only on the main process to avoid race conditions. True state_dict Optional[dict] obj: torch.Tensor ): The state dictionary of the model to save. Will default to :obj: self.state_dict() , but can be used to only save parts of the model or if special precautions need to be taken when recovering the state dictionary of a model (like when using model parallelism). None save_function Callable obj: Callable ): The function to use to save the state dictionary. When we need to replace :obj: torch.save by another method. <function save at 0x000001C1C08B4318> Source code in super_image\\modeling_utils.py def save_pretrained ( self , save_directory : Union [ str , os . PathLike ], save_config : bool = True , state_dict : Optional [ dict ] = None , save_function : Callable = torch . save , ): \"\"\" Save a model and its configuration file to a directory, so that it can be re-loaded using the `:func:`~super_image.PreTrainedModel.from_pretrained`` class method. Arguments: save_directory (:obj:`str` or :obj:`os.PathLike`): Directory to which to save. Will be created if it doesn't exist. save_config (:obj:`bool`, `optional`, defaults to :obj:`True`): Whether or not to save the config of the model. Useful when in distributed training like TPUs and need to call this function on all processes. In this case, set :obj:`save_config=True` only on the main process to avoid race conditions. state_dict (nested dictionary of :obj:`torch.Tensor`): The state dictionary of the model to save. Will default to :obj:`self.state_dict()`, but can be used to only save parts of the model or if special precautions need to be taken when recovering the state dictionary of a model (like when using model parallelism). save_function (:obj:`Callable`): The function to use to save the state dictionary. When we need to replace :obj:`torch.save` by another method. \"\"\" if os . path . isfile ( save_directory ): logger . error ( f \"Provided path ( { save_directory } ) should be a directory, not a file\" ) return os . makedirs ( save_directory , exist_ok = True ) model_to_save = self # Setup scale scale = self . config . scale if scale is not None : weights_name = WEIGHTS_NAME_SCALE . format ( scale = scale ) else : weights_name = WEIGHTS_NAME # Save the config if save_config : model_to_save . config . save_pretrained ( save_directory ) # Save the model if state_dict is None : state_dict = model_to_save . state_dict () # If we save using the predefined names, we can load using `from_pretrained` output_model_file = os . path . join ( save_directory , weights_name ) save_function ( state_dict , output_model_file ) logger . info ( f \"Model weights saved in { output_model_file } \" )","title":"save_pretrained()"},{"location":"reference/trainer/","text":"Trainer # The Trainer class provides an API for feature-complete training in most standard use cases. Before instantiating your Trainer , create a TrainingArguments to access all the points of customization during training. Trainer # Trainer is a simple class implementing the training and eval loop for PyTorch to train a super-image model. Parameters: Name Type Description Default model class: ~super_image.PreTrainedModel or :obj: torch.nn.Module , optional ): The model to train, evaluate or use for predictions. If not provided, a model_init must be passed. .. note:: :class: ~super_image.Trainer is optimized to work with the :class: ~super_image.PreTrainedModel provided by the library. You can still use your own models defined as :obj: torch.nn.Module as long as they work the same way as the super_image models. required args class: ~super_image.TrainingArguments , optional ): The arguments to tweak for training. Will default to a basic instance of :class: ~super_image.TrainingArguments with the output_dir set to a directory named tmp_trainer in the current directory if not provided. required train_dataset obj: torch.utils.data.dataset.Dataset or :obj: torch.utils.data.dataset.IterableDataset ): The dataset to use for training. required eval_dataset obj: torch.utils.data.dataset.Dataset , optional ): The dataset to use for evaluation. required get_eval_dataloader ( self ) # Returns the evaluation :class: ~torch.utils.data.DataLoader . Source code in super_image\\trainer.py def get_eval_dataloader ( self ) -> DataLoader : \"\"\" Returns the evaluation :class:`~torch.utils.data.DataLoader`. \"\"\" eval_dataset = self . eval_dataset if eval_dataset is None : eval_dataset = self . train_dataset return DataLoader ( dataset = eval_dataset , batch_size = 1 , ) get_train_dataloader ( self ) # Returns the training :class: ~torch.utils.data.DataLoader . Source code in super_image\\trainer.py def get_train_dataloader ( self ) -> DataLoader : \"\"\" Returns the training :class:`~torch.utils.data.DataLoader`. \"\"\" if self . train_dataset is None : raise ValueError ( \"Trainer: training requires a train_dataset.\" ) train_dataset = self . train_dataset return DataLoader ( dataset = train_dataset , batch_size = self . args . train_batch_size , shuffle = True , num_workers = self . args . dataloader_num_workers , pin_memory = self . args . dataloader_pin_memory , ) save_model ( self , output_dir = None ) # Will save the model, so you can reload it using :obj: from_pretrained() . Will only save from the main process. Source code in super_image\\trainer.py def save_model ( self , output_dir : Optional [ str ] = None ): \"\"\" Will save the model, so you can reload it using :obj:`from_pretrained()`. Will only save from the main process. \"\"\" output_dir = output_dir if output_dir is not None else self . args . output_dir os . makedirs ( output_dir , exist_ok = True ) if not isinstance ( self . model , PreTrainedModel ): # Setup scale scale = self . model . config . scale if scale is not None : weights_name = WEIGHTS_NAME_SCALE . format ( scale = scale ) else : weights_name = WEIGHTS_NAME weights = copy . deepcopy ( self . model . state_dict ()) torch . save ( weights , os . path . join ( output_dir , weights_name )) else : self . model . save_pretrained ( output_dir ) train ( self , resume_from_checkpoint = None , ** kwargs ) # Main training entry point. Parameters: Name Type Description Default resume_from_checkpoint Union[bool, str] obj: str or :obj: bool , optional ): If a :obj: str , local path to a saved checkpoint as saved by a previous instance of :class: ~super_image.Trainer . If a :obj: bool and equals True , load the last checkpoint in args.output_dir as saved by a previous instance of :class: ~super_image.Trainer . If present, training will resume from the model/optimizer/scheduler states loaded here. None kwargs Additional keyword arguments used to hide deprecated arguments {} Source code in super_image\\trainer.py def train ( self , resume_from_checkpoint : Optional [ Union [ str , bool ]] = None , ** kwargs , ): \"\"\" Main training entry point. Args: resume_from_checkpoint (:obj:`str` or :obj:`bool`, `optional`): If a :obj:`str`, local path to a saved checkpoint as saved by a previous instance of :class:`~super_image.Trainer`. If a :obj:`bool` and equals `True`, load the last checkpoint in `args.output_dir` as saved by a previous instance of :class:`~super_image.Trainer`. If present, training will resume from the model/optimizer/scheduler states loaded here. kwargs: Additional keyword arguments used to hide deprecated arguments \"\"\" args = self . args epochs_trained = 0 device = args . device num_train_epochs = args . num_train_epochs learning_rate = args . learning_rate train_batch_size = args . train_batch_size train_dataset = self . train_dataset train_dataloader = self . get_train_dataloader () step_size = int ( len ( train_dataset ) / train_batch_size * 200 ) # # Load potential model checkpoint # if isinstance(resume_from_checkpoint, bool) and resume_from_checkpoint: # resume_from_checkpoint = get_last_checkpoint(args.output_dir) # if resume_from_checkpoint is None: # raise ValueError(f\"No valid checkpoint found in output directory ({args.output_dir})\") # # if resume_from_checkpoint is not None: # if not os.path.isfile(os.path.join(resume_from_checkpoint, WEIGHTS_NAME)): # raise ValueError(f\"Can't find a valid checkpoint at {resume_from_checkpoint}\") # # logger.info(f\"Loading model from {resume_from_checkpoint}).\") # # if os.path.isfile(os.path.join(resume_from_checkpoint, CONFIG_NAME)): # config = PretrainedConfig.from_json_file(os.path.join(resume_from_checkpoint, CONFIG_NAME)) # # state_dict = torch.load(os.path.join(resume_from_checkpoint, WEIGHTS_NAME), map_location=\"cpu\") # # If the model is on the GPU, it still works! # self._load_state_dict_in_model(state_dict) # # # release memory # del state_dict optimizer = Adam ( self . model . parameters (), lr = learning_rate ) scheduler = lr_scheduler . StepLR ( optimizer , step_size = step_size , gamma = self . args . gamma ) for epoch in range ( epochs_trained , num_train_epochs ): for param_group in optimizer . param_groups : param_group [ 'lr' ] = learning_rate * ( 0.1 ** ( epoch // int ( num_train_epochs * 0.8 ))) self . model . train () epoch_losses = AverageMeter () with tqdm ( total = ( len ( train_dataset ) - len ( train_dataset ) % train_batch_size )) as t : t . set_description ( f 'epoch: { epoch } / { num_train_epochs - 1 } ' ) for data in train_dataloader : inputs , labels = data inputs = inputs . to ( device ) labels = labels . to ( device ) if self . model . config . model_type == 'SMSR' : # update tau for gumbel softmax tau = max ( 1 - ( epoch - 1 ) / 500 , 0.4 ) for m in self . model . modules (): if hasattr ( m , '_set_tau' ): m . _set_tau ( tau ) preds = self . model ( inputs ) criterion = nn . L1Loss () loss = criterion ( preds , labels ) epoch_losses . update ( loss . item (), len ( inputs )) optimizer . zero_grad () loss . backward () optimizer . step () scheduler . step () t . set_postfix ( loss = f ' { epoch_losses . avg : .6f } ' ) t . update ( len ( inputs )) self . eval ( epoch ) TrainingArguments # TrainingArguments is the data class of arguments which relate to the training loop itself . Parameters: Name Type Description Default output_dir obj: str ): The output directory where the model predictions and checkpoints will be written. required overwrite_output_dir obj: bool , optional , defaults to :obj: False ): If :obj: True , overwrite the content of the output directory. Use this to continue training if :obj: output_dir points to a checkpoint directory. required learning_rate obj: float , optional , defaults to 1e-4): The initial learning rate for :class: torch.optim.Adam optimizer. required gamma obj: float , optional , defaults to 0.5): The weight decay gamma to apply to the :class: torch.optim.Adam optimizer. required num_train_epochs( obj: int , optional , defaults to 1000): Total number of training epochs to perform. required save_strategy obj: str or :class: ~transformers.trainer_utils.IntervalStrategy , optional , defaults to :obj: \"steps\" ): The checkpoint save strategy to adopt during training. Possible values are: * :obj: \"no\" : No save is done during training. * :obj: \"epoch\" : Save is done at the end of each epoch. * :obj: \"steps\" : Save is done every :obj: save_steps . required save_steps obj: int , optional , defaults to 500): Number of updates steps before two checkpoint saves if :obj: save_strategy=\"steps\" . required save_total_limit obj: int , optional ): If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in :obj: output_dir . required no_cuda obj: bool , optional , defaults to :obj: False ): Whether to not use CUDA even when it is available or not. required seed obj: int , optional , defaults to 42): Random seed that will be set at the beginning of training. required fp16 obj: bool , optional , defaults to :obj: False ): Whether to use 16-bit (mixed) precision training instead of 32-bit training. required per_device_train_batch_size obj: int , optional , defaults to 16): The batch size per GPU/CPU for training. required local_rank obj: int , optional , defaults to -1): Rank of the process during distributed training. required dataloader_num_workers obj: int , optional , defaults to 0): Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. required dataloader_pin_memory obj: bool , optional , defaults to :obj: True ): Whether you want to pin memory in data loaders or not. Will default to :obj: True . required device : torch . device property readonly # The device used by this process. n_gpu property readonly # The number of GPUs used by this process. Note This will only be greater than one when you have multiple GPUs available but are not using distributed training. For distributed training, it will always be 1. train_batch_size : int property readonly # The actual batch size for training (may differ from :obj: per_device_train_batch_size in distributed training).","title":"Trainer"},{"location":"reference/trainer/#trainer","text":"The Trainer class provides an API for feature-complete training in most standard use cases. Before instantiating your Trainer , create a TrainingArguments to access all the points of customization during training.","title":"Trainer"},{"location":"reference/trainer/#trainer_1","text":"Trainer is a simple class implementing the training and eval loop for PyTorch to train a super-image model. Parameters: Name Type Description Default model class: ~super_image.PreTrainedModel or :obj: torch.nn.Module , optional ): The model to train, evaluate or use for predictions. If not provided, a model_init must be passed. .. note:: :class: ~super_image.Trainer is optimized to work with the :class: ~super_image.PreTrainedModel provided by the library. You can still use your own models defined as :obj: torch.nn.Module as long as they work the same way as the super_image models. required args class: ~super_image.TrainingArguments , optional ): The arguments to tweak for training. Will default to a basic instance of :class: ~super_image.TrainingArguments with the output_dir set to a directory named tmp_trainer in the current directory if not provided. required train_dataset obj: torch.utils.data.dataset.Dataset or :obj: torch.utils.data.dataset.IterableDataset ): The dataset to use for training. required eval_dataset obj: torch.utils.data.dataset.Dataset , optional ): The dataset to use for evaluation. required","title":"Trainer"},{"location":"reference/trainer/#super_image.trainer.Trainer.get_eval_dataloader","text":"Returns the evaluation :class: ~torch.utils.data.DataLoader . Source code in super_image\\trainer.py def get_eval_dataloader ( self ) -> DataLoader : \"\"\" Returns the evaluation :class:`~torch.utils.data.DataLoader`. \"\"\" eval_dataset = self . eval_dataset if eval_dataset is None : eval_dataset = self . train_dataset return DataLoader ( dataset = eval_dataset , batch_size = 1 , )","title":"get_eval_dataloader()"},{"location":"reference/trainer/#super_image.trainer.Trainer.get_train_dataloader","text":"Returns the training :class: ~torch.utils.data.DataLoader . Source code in super_image\\trainer.py def get_train_dataloader ( self ) -> DataLoader : \"\"\" Returns the training :class:`~torch.utils.data.DataLoader`. \"\"\" if self . train_dataset is None : raise ValueError ( \"Trainer: training requires a train_dataset.\" ) train_dataset = self . train_dataset return DataLoader ( dataset = train_dataset , batch_size = self . args . train_batch_size , shuffle = True , num_workers = self . args . dataloader_num_workers , pin_memory = self . args . dataloader_pin_memory , )","title":"get_train_dataloader()"},{"location":"reference/trainer/#super_image.trainer.Trainer.save_model","text":"Will save the model, so you can reload it using :obj: from_pretrained() . Will only save from the main process. Source code in super_image\\trainer.py def save_model ( self , output_dir : Optional [ str ] = None ): \"\"\" Will save the model, so you can reload it using :obj:`from_pretrained()`. Will only save from the main process. \"\"\" output_dir = output_dir if output_dir is not None else self . args . output_dir os . makedirs ( output_dir , exist_ok = True ) if not isinstance ( self . model , PreTrainedModel ): # Setup scale scale = self . model . config . scale if scale is not None : weights_name = WEIGHTS_NAME_SCALE . format ( scale = scale ) else : weights_name = WEIGHTS_NAME weights = copy . deepcopy ( self . model . state_dict ()) torch . save ( weights , os . path . join ( output_dir , weights_name )) else : self . model . save_pretrained ( output_dir )","title":"save_model()"},{"location":"reference/trainer/#super_image.trainer.Trainer.train","text":"Main training entry point. Parameters: Name Type Description Default resume_from_checkpoint Union[bool, str] obj: str or :obj: bool , optional ): If a :obj: str , local path to a saved checkpoint as saved by a previous instance of :class: ~super_image.Trainer . If a :obj: bool and equals True , load the last checkpoint in args.output_dir as saved by a previous instance of :class: ~super_image.Trainer . If present, training will resume from the model/optimizer/scheduler states loaded here. None kwargs Additional keyword arguments used to hide deprecated arguments {} Source code in super_image\\trainer.py def train ( self , resume_from_checkpoint : Optional [ Union [ str , bool ]] = None , ** kwargs , ): \"\"\" Main training entry point. Args: resume_from_checkpoint (:obj:`str` or :obj:`bool`, `optional`): If a :obj:`str`, local path to a saved checkpoint as saved by a previous instance of :class:`~super_image.Trainer`. If a :obj:`bool` and equals `True`, load the last checkpoint in `args.output_dir` as saved by a previous instance of :class:`~super_image.Trainer`. If present, training will resume from the model/optimizer/scheduler states loaded here. kwargs: Additional keyword arguments used to hide deprecated arguments \"\"\" args = self . args epochs_trained = 0 device = args . device num_train_epochs = args . num_train_epochs learning_rate = args . learning_rate train_batch_size = args . train_batch_size train_dataset = self . train_dataset train_dataloader = self . get_train_dataloader () step_size = int ( len ( train_dataset ) / train_batch_size * 200 ) # # Load potential model checkpoint # if isinstance(resume_from_checkpoint, bool) and resume_from_checkpoint: # resume_from_checkpoint = get_last_checkpoint(args.output_dir) # if resume_from_checkpoint is None: # raise ValueError(f\"No valid checkpoint found in output directory ({args.output_dir})\") # # if resume_from_checkpoint is not None: # if not os.path.isfile(os.path.join(resume_from_checkpoint, WEIGHTS_NAME)): # raise ValueError(f\"Can't find a valid checkpoint at {resume_from_checkpoint}\") # # logger.info(f\"Loading model from {resume_from_checkpoint}).\") # # if os.path.isfile(os.path.join(resume_from_checkpoint, CONFIG_NAME)): # config = PretrainedConfig.from_json_file(os.path.join(resume_from_checkpoint, CONFIG_NAME)) # # state_dict = torch.load(os.path.join(resume_from_checkpoint, WEIGHTS_NAME), map_location=\"cpu\") # # If the model is on the GPU, it still works! # self._load_state_dict_in_model(state_dict) # # # release memory # del state_dict optimizer = Adam ( self . model . parameters (), lr = learning_rate ) scheduler = lr_scheduler . StepLR ( optimizer , step_size = step_size , gamma = self . args . gamma ) for epoch in range ( epochs_trained , num_train_epochs ): for param_group in optimizer . param_groups : param_group [ 'lr' ] = learning_rate * ( 0.1 ** ( epoch // int ( num_train_epochs * 0.8 ))) self . model . train () epoch_losses = AverageMeter () with tqdm ( total = ( len ( train_dataset ) - len ( train_dataset ) % train_batch_size )) as t : t . set_description ( f 'epoch: { epoch } / { num_train_epochs - 1 } ' ) for data in train_dataloader : inputs , labels = data inputs = inputs . to ( device ) labels = labels . to ( device ) if self . model . config . model_type == 'SMSR' : # update tau for gumbel softmax tau = max ( 1 - ( epoch - 1 ) / 500 , 0.4 ) for m in self . model . modules (): if hasattr ( m , '_set_tau' ): m . _set_tau ( tau ) preds = self . model ( inputs ) criterion = nn . L1Loss () loss = criterion ( preds , labels ) epoch_losses . update ( loss . item (), len ( inputs )) optimizer . zero_grad () loss . backward () optimizer . step () scheduler . step () t . set_postfix ( loss = f ' { epoch_losses . avg : .6f } ' ) t . update ( len ( inputs )) self . eval ( epoch )","title":"train()"},{"location":"reference/trainer/#trainingarguments","text":"TrainingArguments is the data class of arguments which relate to the training loop itself . Parameters: Name Type Description Default output_dir obj: str ): The output directory where the model predictions and checkpoints will be written. required overwrite_output_dir obj: bool , optional , defaults to :obj: False ): If :obj: True , overwrite the content of the output directory. Use this to continue training if :obj: output_dir points to a checkpoint directory. required learning_rate obj: float , optional , defaults to 1e-4): The initial learning rate for :class: torch.optim.Adam optimizer. required gamma obj: float , optional , defaults to 0.5): The weight decay gamma to apply to the :class: torch.optim.Adam optimizer. required num_train_epochs( obj: int , optional , defaults to 1000): Total number of training epochs to perform. required save_strategy obj: str or :class: ~transformers.trainer_utils.IntervalStrategy , optional , defaults to :obj: \"steps\" ): The checkpoint save strategy to adopt during training. Possible values are: * :obj: \"no\" : No save is done during training. * :obj: \"epoch\" : Save is done at the end of each epoch. * :obj: \"steps\" : Save is done every :obj: save_steps . required save_steps obj: int , optional , defaults to 500): Number of updates steps before two checkpoint saves if :obj: save_strategy=\"steps\" . required save_total_limit obj: int , optional ): If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in :obj: output_dir . required no_cuda obj: bool , optional , defaults to :obj: False ): Whether to not use CUDA even when it is available or not. required seed obj: int , optional , defaults to 42): Random seed that will be set at the beginning of training. required fp16 obj: bool , optional , defaults to :obj: False ): Whether to use 16-bit (mixed) precision training instead of 32-bit training. required per_device_train_batch_size obj: int , optional , defaults to 16): The batch size per GPU/CPU for training. required local_rank obj: int , optional , defaults to -1): Rank of the process during distributed training. required dataloader_num_workers obj: int , optional , defaults to 0): Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. required dataloader_pin_memory obj: bool , optional , defaults to :obj: True ): Whether you want to pin memory in data loaders or not. Will default to :obj: True . required","title":"TrainingArguments"},{"location":"reference/trainer/#super_image.training_args.TrainingArguments.device","text":"The device used by this process.","title":"device"},{"location":"reference/trainer/#super_image.training_args.TrainingArguments.n_gpu","text":"The number of GPUs used by this process. Note This will only be greater than one when you have multiple GPUs available but are not using distributed training. For distributed training, it will always be 1.","title":"n_gpu"},{"location":"reference/trainer/#super_image.training_args.TrainingArguments.train_batch_size","text":"The actual batch size for training (may differ from :obj: per_device_train_batch_size in distributed training).","title":"train_batch_size"},{"location":"coverage/","text":".md-content { max-width: none !important; } article h1, article > a { display: none; } var coviframe = document.getElementById(\"coviframe\"); function resizeIframe() { coviframe.style.height = coviframe.contentWindow.document.documentElement.offsetHeight + 'px'; } coviframe.contentWindow.document.body.onclick = function() { coviframe.contentWindow.location.reload(); }","title":"Coverage report"}]}