{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"super-image State-of-the-art image super resolution models for PyTorch. Requirements \ufffd super-image requires Python 3.6 or above. Installation \ufffd With pip : pip install super-image Quick Start \ufffd Quickly utilise pre-trained models for upscaling your images 2x, 3x and 4x. See the full list of models below . from super_image import EdsrModel , ImageLoader from PIL import Image import requests url = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg' image = Image . open ( requests . get ( url , stream = True ) . raw ) model = EdsrModel . from_pretrained ( 'eugenesiow/edsr-base' , scale = 2 ) inputs = ImageLoader . load_image ( image ) preds = model ( inputs ) ImageLoader . save_image ( preds , './scaled_2x.png' ) ImageLoader . save_compare ( inputs , preds , './scaled_2x_compare.png' ) Pre-trained Models \ufffd Pre-trained models are available at various scales and hosted at the awesome huggingface_hub . By default the models were pretrained on DIV2K , a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of 100 validation images (images numbered 801 to 900). The leaderboard below shows the PSNR / SSIM metrics for each model at various scales on various test sets ( Set5 , Set14 , BSD100 , Urban100 ). The higher the better. Scale x2 \ufffd Rank Model Params Set5 Set14 BSD100 Urban100 1 msrn-bam 5.9m 38.02/0.9607 33.63/0.9177 32.20/0.8998 32.08/0.9276 2 edsr-base 1.5m 38.02/0.9607 33.57/0.9172 32.21/0.8999 32.04/0.9276 Scale x3 \ufffd Rank Model Params Set5 Set14 BSD100 Urban100 1 msrn-bam 5.9m 35.16/0.9410 30.97/0.8574 29.67/0.8209 29.31/0.8737 2 edsr-base 1.5m 35.04/0.9403 30.93/0.8567 29.65/0.8204 29.23/0.8723 Scale x4 \ufffd Rank Model Params Set5 Set14 BSD100 Urban100 1 msrn-bam 5.9m 32.26/0.8955 28.66/0.7829 27.61/0.7369 26.10/0.7857 2 edsr-base 1.5m 32.12/0.8947 28.60/0.7815 27.61/0.7363 26.02/0.7832 3 a2n 1.0m 32.07/0.8933 28.56/0.7801 27.54/0.7342 25.89/0.7787","title":"Overview"},{"location":"#requirements","text":"super-image requires Python 3.6 or above.","title":"Requirements"},{"location":"#installation","text":"With pip : pip install super-image","title":"Installation"},{"location":"#quick-start","text":"Quickly utilise pre-trained models for upscaling your images 2x, 3x and 4x. See the full list of models below . from super_image import EdsrModel , ImageLoader from PIL import Image import requests url = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg' image = Image . open ( requests . get ( url , stream = True ) . raw ) model = EdsrModel . from_pretrained ( 'eugenesiow/edsr-base' , scale = 2 ) inputs = ImageLoader . load_image ( image ) preds = model ( inputs ) ImageLoader . save_image ( preds , './scaled_2x.png' ) ImageLoader . save_compare ( inputs , preds , './scaled_2x_compare.png' )","title":"Quick Start"},{"location":"#pre-trained-models","text":"Pre-trained models are available at various scales and hosted at the awesome huggingface_hub . By default the models were pretrained on DIV2K , a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of 100 validation images (images numbered 801 to 900). The leaderboard below shows the PSNR / SSIM metrics for each model at various scales on various test sets ( Set5 , Set14 , BSD100 , Urban100 ). The higher the better.","title":"Pre-trained Models"},{"location":"#scale-x2","text":"Rank Model Params Set5 Set14 BSD100 Urban100 1 msrn-bam 5.9m 38.02/0.9607 33.63/0.9177 32.20/0.8998 32.08/0.9276 2 edsr-base 1.5m 38.02/0.9607 33.57/0.9172 32.21/0.8999 32.04/0.9276","title":"Scale x2"},{"location":"#scale-x3","text":"Rank Model Params Set5 Set14 BSD100 Urban100 1 msrn-bam 5.9m 35.16/0.9410 30.97/0.8574 29.67/0.8209 29.31/0.8737 2 edsr-base 1.5m 35.04/0.9403 30.93/0.8567 29.65/0.8204 29.23/0.8723","title":"Scale x3"},{"location":"#scale-x4","text":"Rank Model Params Set5 Set14 BSD100 Urban100 1 msrn-bam 5.9m 32.26/0.8955 28.66/0.7829 27.61/0.7369 26.10/0.7857 2 edsr-base 1.5m 32.12/0.8947 28.60/0.7815 27.61/0.7363 26.02/0.7832 3 a2n 1.0m 32.07/0.8933 28.56/0.7801 27.54/0.7342 25.89/0.7787","title":"Scale x4"},{"location":"changelog/","text":"Changelog \ufffd All notable changes to this project will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"code_of_conduct/","text":"Contributor Covenant Code of Conduct \ufffd Our Pledge \ufffd In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards \ufffd Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities \ufffd Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope \ufffd This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement \ufffd Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at kyo116@gmail.com . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution \ufffd This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at http://contributor-covenant.org/version/1/4","title":"Code of Conduct"},{"location":"code_of_conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"code_of_conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"code_of_conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"code_of_conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"code_of_conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"code_of_conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at kyo116@gmail.com . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"code_of_conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at http://contributor-covenant.org/version/1/4","title":"Attribution"},{"location":"contributing/","text":"Contributing \ufffd Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. Environment setup \ufffd Nothing easier! Fork and clone the repository, then: cd super-image make setup Note If it fails for some reason, you'll need to install Poetry manually. You can install it with: python3 -m pip install --user pipx pipx install poetry Now you can try running make setup again, or simply poetry install . You now have the dependencies installed. You can run the application with poetry run super-image [ARGS...] . Run make help to see all the available actions! Tasks \ufffd This project uses duty to run tasks. A Makefile is also provided. The Makefile will try to run certain tasks on multiple Python versions. If for some reason you don't want to run the task on multiple Python versions, you can do one of the following: export PYTHON_VERSIONS= : this will run the task with only the current Python version run the task directly with poetry run duty TASK , or duty TASK if the environment was already activated through poetry shell The Makefile detects if the Poetry environment is activated, so make will work the same with the virtualenv activated or not. Development \ufffd As usual: create a new branch: git checkout -b feature-or-bugfix-name edit the code and/or the documentation If you updated the documentation or the project dependencies: run make docs-regen run make docs-serve , go to http://localhost:8000 and check that everything looks good Before committing: run make format to auto-format the code run make check to check everything (fix any warning) run make test to run the tests (fix any issue) follow our commit message convention If you are unsure about how to fix or ignore a warning, just let the continuous integration fail, and we will help you during review. Don't bother updating the changelog, we will take care of this. Commit message convention \ufffd Commits messages must follow the Angular style : <type>[(scope)]: Subject [Body] Scope and body are optional. Type can be: build : About packaging, building wheels, etc. chore : About packaging or repo/files management. ci : About Continuous Integration. docs : About documentation. feat : New feature. fix : Bug fix. perf : About performance. refactor : Changes which are not features nor bug fixes. style : A change in code style/format. tests : About tests. Subject (and body) must be valid Markdown. If you write a body, please add issues references at the end: Body. References: #10, #11. Fixes #15. Pull requests guidelines \ufffd Link to any related issue in the Pull Request message. During review, we recommend using fixups: # SHA is the SHA of the commit you want to fix git commit --fixup = SHA Once all the changes are approved, you can squash your commits: git rebase -i --autosquash master And force-push: git push -f If this seems all too complicated, you can push or force-push each new commit, and we will squash them ourselves if needed, before merging.","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.","title":"Contributing"},{"location":"contributing/#environment-setup","text":"Nothing easier! Fork and clone the repository, then: cd super-image make setup Note If it fails for some reason, you'll need to install Poetry manually. You can install it with: python3 -m pip install --user pipx pipx install poetry Now you can try running make setup again, or simply poetry install . You now have the dependencies installed. You can run the application with poetry run super-image [ARGS...] . Run make help to see all the available actions!","title":"Environment setup"},{"location":"contributing/#tasks","text":"This project uses duty to run tasks. A Makefile is also provided. The Makefile will try to run certain tasks on multiple Python versions. If for some reason you don't want to run the task on multiple Python versions, you can do one of the following: export PYTHON_VERSIONS= : this will run the task with only the current Python version run the task directly with poetry run duty TASK , or duty TASK if the environment was already activated through poetry shell The Makefile detects if the Poetry environment is activated, so make will work the same with the virtualenv activated or not.","title":"Tasks"},{"location":"contributing/#development","text":"As usual: create a new branch: git checkout -b feature-or-bugfix-name edit the code and/or the documentation If you updated the documentation or the project dependencies: run make docs-regen run make docs-serve , go to http://localhost:8000 and check that everything looks good Before committing: run make format to auto-format the code run make check to check everything (fix any warning) run make test to run the tests (fix any issue) follow our commit message convention If you are unsure about how to fix or ignore a warning, just let the continuous integration fail, and we will help you during review. Don't bother updating the changelog, we will take care of this.","title":"Development"},{"location":"contributing/#commit-message-convention","text":"Commits messages must follow the Angular style : <type>[(scope)]: Subject [Body] Scope and body are optional. Type can be: build : About packaging, building wheels, etc. chore : About packaging or repo/files management. ci : About Continuous Integration. docs : About documentation. feat : New feature. fix : Bug fix. perf : About performance. refactor : Changes which are not features nor bug fixes. style : A change in code style/format. tests : About tests. Subject (and body) must be valid Markdown. If you write a body, please add issues references at the end: Body. References: #10, #11. Fixes #15.","title":"Commit message convention"},{"location":"contributing/#pull-requests-guidelines","text":"Link to any related issue in the Pull Request message. During review, we recommend using fixups: # SHA is the SHA of the commit you want to fix git commit --fixup = SHA Once all the changes are approved, you can squash your commits: git rebase -i --autosquash master And force-push: git push -f If this seems all too complicated, you can push or force-push each new commit, and we will squash them ourselves if needed, before merging.","title":"Pull requests guidelines"},{"location":"credits/","text":"Credits \ufffd These projects were used to build super-image . Thank you! python | poetry | copier-poetry Direct dependencies \ufffd autoflake | black | darglint | duty | flake8-bandit | flake8-black | flake8-bugbear | flake8-builtins | flake8-comprehensions | flake8-docstrings | flake8-pytest-style | flake8-string-format | flake8-tidy-imports | flake8-variables-names | git-changelog | h5py | httpx | huggingface-hub | isort | jinja2-cli | mkdocs | mkdocs-coverage | mkdocs-macros-plugin | mkdocs-material | mkdocstrings | mypy | opencv-python | pep8-naming | pytest | pytest-cov | pytest-randomly | pytest-sugar | pytest-xdist | toml | torch | torchvision | tqdm | wps-light Indirect dependencies \ufffd ansimarkup | appdirs | astor | astunparse | atomicwrites | attrs | bandit | cached-property | certifi | chardet | click | colorama | contextvars | coverage | dataclasses | execnet | failprint | filelock | flake8 | flake8-plugin-utils | flake8-polyfill | ghp-import | gitdb | GitPython | h11 | httpcore | idna | immutables | importlib-metadata | iniconfig | Jinja2 | Markdown | MarkupSafe | mccabe | mergedeep | mkdocs-autorefs | mkdocs-material-extensions | mypy-extensions | numpy | packaging | pathspec | pbr | Pillow | pluggy | ptyprocess | py | pycodestyle | pydocstyle | pyflakes | Pygments | pymdown-extensions | pyparsing | pytest-forked | python-dateutil | pytkdocs | PyYAML | pyyaml-env-tag | regex | requests | rfc3986 | six | smmap | sniffio | snowballstemmer | stevedore | termcolor | typed-ast | typing-extensions | urllib3 | watchdog | zipp More credits from the author","title":"Credits"},{"location":"credits/#credits","text":"These projects were used to build super-image . Thank you! python | poetry | copier-poetry","title":"Credits"},{"location":"credits/#direct-dependencies","text":"autoflake | black | darglint | duty | flake8-bandit | flake8-black | flake8-bugbear | flake8-builtins | flake8-comprehensions | flake8-docstrings | flake8-pytest-style | flake8-string-format | flake8-tidy-imports | flake8-variables-names | git-changelog | h5py | httpx | huggingface-hub | isort | jinja2-cli | mkdocs | mkdocs-coverage | mkdocs-macros-plugin | mkdocs-material | mkdocstrings | mypy | opencv-python | pep8-naming | pytest | pytest-cov | pytest-randomly | pytest-sugar | pytest-xdist | toml | torch | torchvision | tqdm | wps-light","title":"Direct dependencies"},{"location":"credits/#indirect-dependencies","text":"ansimarkup | appdirs | astor | astunparse | atomicwrites | attrs | bandit | cached-property | certifi | chardet | click | colorama | contextvars | coverage | dataclasses | execnet | failprint | filelock | flake8 | flake8-plugin-utils | flake8-polyfill | ghp-import | gitdb | GitPython | h11 | httpcore | idna | immutables | importlib-metadata | iniconfig | Jinja2 | Markdown | MarkupSafe | mccabe | mergedeep | mkdocs-autorefs | mkdocs-material-extensions | mypy-extensions | numpy | packaging | pathspec | pbr | Pillow | pluggy | ptyprocess | py | pycodestyle | pydocstyle | pyflakes | Pygments | pymdown-extensions | pyparsing | pytest-forked | python-dateutil | pytkdocs | PyYAML | pyyaml-env-tag | regex | requests | rfc3986 | six | smmap | sniffio | snowballstemmer | stevedore | termcolor | typed-ast | typing-extensions | urllib3 | watchdog | zipp More credits from the author","title":"Indirect dependencies"},{"location":"license/","text":"Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"reference/cli/","text":"Module that contains the command line application. get_parser () \ufffd Return the CLI argument parser. Returns: Type Description ArgumentParser An argparse parser. Source code in super_image\\cli.py def get_parser () -> argparse . ArgumentParser : \"\"\" Return the CLI argument parser. Returns: An argparse parser. \"\"\" return argparse . ArgumentParser ( prog = \"super-image\" ) main ( args = None ) \ufffd Run the main program. This function is executed when you type super-image or python -m super_image . Parameters: Name Type Description Default args Optional[List[str]] Arguments passed from the command line. None Returns: Type Description int An exit code. Source code in super_image\\cli.py def main ( args : Optional [ List [ str ]] = None ) -> int : \"\"\" Run the main program. This function is executed when you type `super-image` or `python -m super_image`. Arguments: args: Arguments passed from the command line. Returns: An exit code. \"\"\" parser = get_parser () opts = parser . parse_args ( args = args ) print ( opts ) # noqa: WPS421 (side-effect in main is fine) return 0","title":"CLI"},{"location":"reference/cli/#super_image.cli.get_parser","text":"Return the CLI argument parser. Returns: Type Description ArgumentParser An argparse parser. Source code in super_image\\cli.py def get_parser () -> argparse . ArgumentParser : \"\"\" Return the CLI argument parser. Returns: An argparse parser. \"\"\" return argparse . ArgumentParser ( prog = \"super-image\" )","title":"get_parser()"},{"location":"reference/cli/#super_image.cli.main","text":"Run the main program. This function is executed when you type super-image or python -m super_image . Parameters: Name Type Description Default args Optional[List[str]] Arguments passed from the command line. None Returns: Type Description int An exit code. Source code in super_image\\cli.py def main ( args : Optional [ List [ str ]] = None ) -> int : \"\"\" Run the main program. This function is executed when you type `super-image` or `python -m super_image`. Arguments: args: Arguments passed from the command line. Returns: An exit code. \"\"\" parser = get_parser () opts = parser . parse_args ( args = args ) print ( opts ) # noqa: WPS421 (side-effect in main is fine) return 0","title":"main()"},{"location":"reference/trainer/","text":"Trainer is a simple class implementing the training and eval loop for PyTorch to train a super-image model. Parameters: Name Type Description Default model class: ~super_image.PreTrainedModel or :obj: torch.nn.Module , optional ): The model to train, evaluate or use for predictions. If not provided, a model_init must be passed. .. note:: :class: ~super_image.Trainer is optimized to work with the :class: ~super_image.PreTrainedModel provided by the library. You can still use your own models defined as :obj: torch.nn.Module as long as they work the same way as the super_image models. required args class: ~super_image.TrainingArguments , optional ): The arguments to tweak for training. Will default to a basic instance of :class: ~super_image.TrainingArguments with the output_dir set to a directory named tmp_trainer in the current directory if not provided. required train_dataset obj: torch.utils.data.dataset.Dataset or :obj: torch.utils.data.dataset.IterableDataset ): The dataset to use for training. required eval_dataset obj: torch.utils.data.dataset.Dataset , optional ): The dataset to use for evaluation. required get_eval_dataloader ( self ) \ufffd Returns the evaluation :class: ~torch.utils.data.DataLoader . Source code in super_image\\trainer.py def get_eval_dataloader ( self ) -> DataLoader : \"\"\" Returns the evaluation :class:`~torch.utils.data.DataLoader`. \"\"\" eval_dataset = self . eval_dataset if eval_dataset is None : eval_dataset = self . train_dataset return DataLoader ( dataset = eval_dataset , batch_size = 1 , ) get_train_dataloader ( self ) \ufffd Returns the training :class: ~torch.utils.data.DataLoader . Source code in super_image\\trainer.py def get_train_dataloader ( self ) -> DataLoader : \"\"\" Returns the training :class:`~torch.utils.data.DataLoader`. \"\"\" if self . train_dataset is None : raise ValueError ( \"Trainer: training requires a train_dataset.\" ) train_dataset = self . train_dataset return DataLoader ( dataset = train_dataset , batch_size = self . args . train_batch_size , shuffle = True , num_workers = self . args . dataloader_num_workers , pin_memory = self . args . dataloader_pin_memory , ) save_model ( self , output_dir = None ) \ufffd Will save the model, so you can reload it using :obj: from_pretrained() . Will only save from the main process. Source code in super_image\\trainer.py def save_model ( self , output_dir : Optional [ str ] = None ): \"\"\" Will save the model, so you can reload it using :obj:`from_pretrained()`. Will only save from the main process. \"\"\" output_dir = output_dir if output_dir is not None else self . args . output_dir os . makedirs ( output_dir , exist_ok = True ) if not isinstance ( self . model , PreTrainedModel ): # Setup scale scale = self . model . config . scale if scale is not None : weights_name = WEIGHTS_NAME_SCALE . format ( scale = scale ) else : weights_name = WEIGHTS_NAME weights = copy . deepcopy ( self . model . state_dict ()) torch . save ( weights , os . path . join ( output_dir , weights_name )) else : self . model . save_pretrained ( output_dir ) train ( self , resume_from_checkpoint = None , ** kwargs ) \ufffd Main training entry point. Parameters: Name Type Description Default resume_from_checkpoint Union[bool, str] obj: str or :obj: bool , optional ): If a :obj: str , local path to a saved checkpoint as saved by a previous instance of :class: ~super_image.Trainer . If a :obj: bool and equals True , load the last checkpoint in args.output_dir as saved by a previous instance of :class: ~super_image.Trainer . If present, training will resume from the model/optimizer/scheduler states loaded here. None kwargs Additional keyword arguments used to hide deprecated arguments {} Source code in super_image\\trainer.py def train ( self , resume_from_checkpoint : Optional [ Union [ str , bool ]] = None , ** kwargs , ): \"\"\" Main training entry point. Args: resume_from_checkpoint (:obj:`str` or :obj:`bool`, `optional`): If a :obj:`str`, local path to a saved checkpoint as saved by a previous instance of :class:`~super_image.Trainer`. If a :obj:`bool` and equals `True`, load the last checkpoint in `args.output_dir` as saved by a previous instance of :class:`~super_image.Trainer`. If present, training will resume from the model/optimizer/scheduler states loaded here. kwargs: Additional keyword arguments used to hide deprecated arguments \"\"\" args = self . args epochs_trained = 0 device = args . device num_train_epochs = args . num_train_epochs learning_rate = args . learning_rate train_batch_size = args . train_batch_size train_dataset = self . train_dataset train_dataloader = self . get_train_dataloader () step_size = int ( len ( train_dataset ) / train_batch_size * 200 ) # # Load potential model checkpoint # if isinstance(resume_from_checkpoint, bool) and resume_from_checkpoint: # resume_from_checkpoint = get_last_checkpoint(args.output_dir) # if resume_from_checkpoint is None: # raise ValueError(f\"No valid checkpoint found in output directory ({args.output_dir})\") # # if resume_from_checkpoint is not None: # if not os.path.isfile(os.path.join(resume_from_checkpoint, WEIGHTS_NAME)): # raise ValueError(f\"Can't find a valid checkpoint at {resume_from_checkpoint}\") # # logger.info(f\"Loading model from {resume_from_checkpoint}).\") # # if os.path.isfile(os.path.join(resume_from_checkpoint, CONFIG_NAME)): # config = PretrainedConfig.from_json_file(os.path.join(resume_from_checkpoint, CONFIG_NAME)) # # state_dict = torch.load(os.path.join(resume_from_checkpoint, WEIGHTS_NAME), map_location=\"cpu\") # # If the model is on the GPU, it still works! # self._load_state_dict_in_model(state_dict) # # # release memory # del state_dict optimizer = Adam ( self . model . parameters (), lr = learning_rate ) scheduler = lr_scheduler . StepLR ( optimizer , step_size = step_size , gamma = self . args . gamma ) for epoch in range ( epochs_trained , num_train_epochs ): for param_group in optimizer . param_groups : param_group [ 'lr' ] = learning_rate * ( 0.1 ** ( epoch // int ( num_train_epochs * 0.8 ))) self . model . train () epoch_losses = AverageMeter () with tqdm ( total = ( len ( train_dataset ) - len ( train_dataset ) % train_batch_size )) as t : t . set_description ( f 'epoch: { epoch } / { num_train_epochs - 1 } ' ) for data in train_dataloader : inputs , labels = data inputs = inputs . to ( device ) labels = labels . to ( device ) preds = self . model ( inputs ) criterion = nn . L1Loss () loss = criterion ( preds , labels ) epoch_losses . update ( loss . item (), len ( inputs )) optimizer . zero_grad () loss . backward () optimizer . step () scheduler . step () t . set_postfix ( loss = f ' { epoch_losses . avg : .6f } ' ) t . update ( len ( inputs )) self . eval ( epoch )","title":"Trainer"},{"location":"reference/trainer/#super_image.trainer.Trainer.get_eval_dataloader","text":"Returns the evaluation :class: ~torch.utils.data.DataLoader . Source code in super_image\\trainer.py def get_eval_dataloader ( self ) -> DataLoader : \"\"\" Returns the evaluation :class:`~torch.utils.data.DataLoader`. \"\"\" eval_dataset = self . eval_dataset if eval_dataset is None : eval_dataset = self . train_dataset return DataLoader ( dataset = eval_dataset , batch_size = 1 , )","title":"get_eval_dataloader()"},{"location":"reference/trainer/#super_image.trainer.Trainer.get_train_dataloader","text":"Returns the training :class: ~torch.utils.data.DataLoader . Source code in super_image\\trainer.py def get_train_dataloader ( self ) -> DataLoader : \"\"\" Returns the training :class:`~torch.utils.data.DataLoader`. \"\"\" if self . train_dataset is None : raise ValueError ( \"Trainer: training requires a train_dataset.\" ) train_dataset = self . train_dataset return DataLoader ( dataset = train_dataset , batch_size = self . args . train_batch_size , shuffle = True , num_workers = self . args . dataloader_num_workers , pin_memory = self . args . dataloader_pin_memory , )","title":"get_train_dataloader()"},{"location":"reference/trainer/#super_image.trainer.Trainer.save_model","text":"Will save the model, so you can reload it using :obj: from_pretrained() . Will only save from the main process. Source code in super_image\\trainer.py def save_model ( self , output_dir : Optional [ str ] = None ): \"\"\" Will save the model, so you can reload it using :obj:`from_pretrained()`. Will only save from the main process. \"\"\" output_dir = output_dir if output_dir is not None else self . args . output_dir os . makedirs ( output_dir , exist_ok = True ) if not isinstance ( self . model , PreTrainedModel ): # Setup scale scale = self . model . config . scale if scale is not None : weights_name = WEIGHTS_NAME_SCALE . format ( scale = scale ) else : weights_name = WEIGHTS_NAME weights = copy . deepcopy ( self . model . state_dict ()) torch . save ( weights , os . path . join ( output_dir , weights_name )) else : self . model . save_pretrained ( output_dir )","title":"save_model()"},{"location":"reference/trainer/#super_image.trainer.Trainer.train","text":"Main training entry point. Parameters: Name Type Description Default resume_from_checkpoint Union[bool, str] obj: str or :obj: bool , optional ): If a :obj: str , local path to a saved checkpoint as saved by a previous instance of :class: ~super_image.Trainer . If a :obj: bool and equals True , load the last checkpoint in args.output_dir as saved by a previous instance of :class: ~super_image.Trainer . If present, training will resume from the model/optimizer/scheduler states loaded here. None kwargs Additional keyword arguments used to hide deprecated arguments {} Source code in super_image\\trainer.py def train ( self , resume_from_checkpoint : Optional [ Union [ str , bool ]] = None , ** kwargs , ): \"\"\" Main training entry point. Args: resume_from_checkpoint (:obj:`str` or :obj:`bool`, `optional`): If a :obj:`str`, local path to a saved checkpoint as saved by a previous instance of :class:`~super_image.Trainer`. If a :obj:`bool` and equals `True`, load the last checkpoint in `args.output_dir` as saved by a previous instance of :class:`~super_image.Trainer`. If present, training will resume from the model/optimizer/scheduler states loaded here. kwargs: Additional keyword arguments used to hide deprecated arguments \"\"\" args = self . args epochs_trained = 0 device = args . device num_train_epochs = args . num_train_epochs learning_rate = args . learning_rate train_batch_size = args . train_batch_size train_dataset = self . train_dataset train_dataloader = self . get_train_dataloader () step_size = int ( len ( train_dataset ) / train_batch_size * 200 ) # # Load potential model checkpoint # if isinstance(resume_from_checkpoint, bool) and resume_from_checkpoint: # resume_from_checkpoint = get_last_checkpoint(args.output_dir) # if resume_from_checkpoint is None: # raise ValueError(f\"No valid checkpoint found in output directory ({args.output_dir})\") # # if resume_from_checkpoint is not None: # if not os.path.isfile(os.path.join(resume_from_checkpoint, WEIGHTS_NAME)): # raise ValueError(f\"Can't find a valid checkpoint at {resume_from_checkpoint}\") # # logger.info(f\"Loading model from {resume_from_checkpoint}).\") # # if os.path.isfile(os.path.join(resume_from_checkpoint, CONFIG_NAME)): # config = PretrainedConfig.from_json_file(os.path.join(resume_from_checkpoint, CONFIG_NAME)) # # state_dict = torch.load(os.path.join(resume_from_checkpoint, WEIGHTS_NAME), map_location=\"cpu\") # # If the model is on the GPU, it still works! # self._load_state_dict_in_model(state_dict) # # # release memory # del state_dict optimizer = Adam ( self . model . parameters (), lr = learning_rate ) scheduler = lr_scheduler . StepLR ( optimizer , step_size = step_size , gamma = self . args . gamma ) for epoch in range ( epochs_trained , num_train_epochs ): for param_group in optimizer . param_groups : param_group [ 'lr' ] = learning_rate * ( 0.1 ** ( epoch // int ( num_train_epochs * 0.8 ))) self . model . train () epoch_losses = AverageMeter () with tqdm ( total = ( len ( train_dataset ) - len ( train_dataset ) % train_batch_size )) as t : t . set_description ( f 'epoch: { epoch } / { num_train_epochs - 1 } ' ) for data in train_dataloader : inputs , labels = data inputs = inputs . to ( device ) labels = labels . to ( device ) preds = self . model ( inputs ) criterion = nn . L1Loss () loss = criterion ( preds , labels ) epoch_losses . update ( loss . item (), len ( inputs )) optimizer . zero_grad () loss . backward () optimizer . step () scheduler . step () t . set_postfix ( loss = f ' { epoch_losses . avg : .6f } ' ) t . update ( len ( inputs )) self . eval ( epoch )","title":"train()"},{"location":"coverage/","text":".md-content { max-width: none !important; } article h1, article > a { display: none; } var coviframe = document.getElementById(\"coviframe\"); function resizeIframe() { coviframe.style.height = coviframe.contentWindow.document.documentElement.offsetHeight + 'px'; } coviframe.contentWindow.document.body.onclick = function() { coviframe.contentWindow.location.reload(); }","title":"Coverage report"}]}