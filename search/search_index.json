{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"super-image State-of-the-art image super resolution models for PyTorch. Installation # With pip : pip install super-image Quick Start # Quickly utilise pre-trained models for upscaling your images 2x, 3x and 4x. See the full list of models below . from super_image import EdsrModel , ImageLoader from PIL import Image import requests url = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg' image = Image . open ( requests . get ( url , stream = True ) . raw ) model = EdsrModel . from_pretrained ( 'eugenesiow/edsr-base' , scale = 2 ) inputs = ImageLoader . load_image ( image ) preds = model ( inputs ) ImageLoader . save_image ( preds , './scaled_2x.png' ) ImageLoader . save_compare ( inputs , preds , './scaled_2x_compare.png' ) Pre-trained Models # Pre-trained models are available at various scales and hosted at the awesome huggingface_hub . By default the models were pretrained on DIV2K , a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of 100 validation images (images numbered 801 to 900). The leaderboard below shows the PSNR / SSIM metrics for each model at various scales on various test sets ( Set5 , Set14 , BSD100 , Urban100 ). The higher the better . All training was to 1000 epochs (some publications, like a2n, train to >1000 epochs in their experiments). Scale x2 # Rank Model Params Set5 Set14 BSD100 Urban100 1 msrn-bam 5.9m 38.02/0.9608 33.73/0.9186 33.78/0.9253 32.08/0.9276 2 edsr-base 1.5m 38.02/0.9607 33.66/0.9180 33.77/0.9254 32.04/0.9276 3 a2n 1.0m 37.87/0.9602 33.54/0.9171 33.67/0.9244 31.71/0.9240 Scale x3 # Rank Model Params Set5 Set14 BSD100 Urban100 1 msrn-bam 5.9m 35.13/0.9408 31.06/0.8588 29.65/0.8196 29.26/0.8736 2 edsr-base 1.5m 35.01/0.9402 31.01/0.8583 29.63/0.8190 29.19/0.8722 Scale x4 # Rank Model Params Set5 Set14 BSD100 Urban100 1 msrn 6.1m 32.19/0.8951 28.78/0.7862 28.53/0.7657 26.12/0.7866 2 msrn-bam 5.9m 32.26/0.8955 28.78/0.7859 28.51/0.7651 26.10/0.7857 3 edsr-base 1.5m 32.12/0.8947 28.72/0.7845 28.50/0.7644 26.02/0.7832 4 a2n 1.0m 32.07/0.8933 28.68/0.7830 28.44/0.7624 25.89/0.7787 You can find a notebook to easily run evaluation on pretrained models below: Train Models # We need the huggingface datasets library to download the data: pip install datasets The following code gets the data and preprocesses/augments the data. from datasets import load_dataset from super_image.data import EvalDataset , TrainDataset , augment_five_crop augmented_dataset = load_dataset ( 'eugenesiow/Div2k' , 'bicubic_x4' , split = 'train' ) \\ . map ( augment_five_crop , batched = True , desc = \"Augmenting Dataset\" ) # download and augment the data with the five_crop method train_dataset = TrainDataset ( augmented_dataset ) # prepare the train dataset for loading PyTorch DataLoader eval_dataset = EvalDataset ( load_dataset ( 'eugenesiow/Div2k' , 'bicubic_x4' , split = 'validation' )) # prepare the eval dataset for the PyTorch DataLoader The training code is provided below: from super_image import Trainer , TrainingArguments , EdsrModel , EdsrConfig training_args = TrainingArguments ( output_dir = './results' , # output directory num_train_epochs = 1000 , # total number of training epochs ) config = EdsrConfig ( scale = 4 , # train a model to upscale 4x ) model = EdsrModel ( config ) trainer = Trainer ( model = model , # the instantiated model to be trained args = training_args , # training arguments, defined above train_dataset = train_dataset , # training dataset eval_dataset = eval_dataset # evaluation dataset ) trainer . train ()","title":"Overview"},{"location":"#installation","text":"With pip : pip install super-image","title":"Installation"},{"location":"#quick-start","text":"Quickly utilise pre-trained models for upscaling your images 2x, 3x and 4x. See the full list of models below . from super_image import EdsrModel , ImageLoader from PIL import Image import requests url = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg' image = Image . open ( requests . get ( url , stream = True ) . raw ) model = EdsrModel . from_pretrained ( 'eugenesiow/edsr-base' , scale = 2 ) inputs = ImageLoader . load_image ( image ) preds = model ( inputs ) ImageLoader . save_image ( preds , './scaled_2x.png' ) ImageLoader . save_compare ( inputs , preds , './scaled_2x_compare.png' )","title":"Quick Start"},{"location":"#pre-trained-models","text":"Pre-trained models are available at various scales and hosted at the awesome huggingface_hub . By default the models were pretrained on DIV2K , a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of 100 validation images (images numbered 801 to 900). The leaderboard below shows the PSNR / SSIM metrics for each model at various scales on various test sets ( Set5 , Set14 , BSD100 , Urban100 ). The higher the better . All training was to 1000 epochs (some publications, like a2n, train to >1000 epochs in their experiments).","title":"Pre-trained Models"},{"location":"#scale-x2","text":"Rank Model Params Set5 Set14 BSD100 Urban100 1 msrn-bam 5.9m 38.02/0.9608 33.73/0.9186 33.78/0.9253 32.08/0.9276 2 edsr-base 1.5m 38.02/0.9607 33.66/0.9180 33.77/0.9254 32.04/0.9276 3 a2n 1.0m 37.87/0.9602 33.54/0.9171 33.67/0.9244 31.71/0.9240","title":"Scale x2"},{"location":"#scale-x3","text":"Rank Model Params Set5 Set14 BSD100 Urban100 1 msrn-bam 5.9m 35.13/0.9408 31.06/0.8588 29.65/0.8196 29.26/0.8736 2 edsr-base 1.5m 35.01/0.9402 31.01/0.8583 29.63/0.8190 29.19/0.8722","title":"Scale x3"},{"location":"#scale-x4","text":"Rank Model Params Set5 Set14 BSD100 Urban100 1 msrn 6.1m 32.19/0.8951 28.78/0.7862 28.53/0.7657 26.12/0.7866 2 msrn-bam 5.9m 32.26/0.8955 28.78/0.7859 28.51/0.7651 26.10/0.7857 3 edsr-base 1.5m 32.12/0.8947 28.72/0.7845 28.50/0.7644 26.02/0.7832 4 a2n 1.0m 32.07/0.8933 28.68/0.7830 28.44/0.7624 25.89/0.7787 You can find a notebook to easily run evaluation on pretrained models below:","title":"Scale x4"},{"location":"#train-models","text":"We need the huggingface datasets library to download the data: pip install datasets The following code gets the data and preprocesses/augments the data. from datasets import load_dataset from super_image.data import EvalDataset , TrainDataset , augment_five_crop augmented_dataset = load_dataset ( 'eugenesiow/Div2k' , 'bicubic_x4' , split = 'train' ) \\ . map ( augment_five_crop , batched = True , desc = \"Augmenting Dataset\" ) # download and augment the data with the five_crop method train_dataset = TrainDataset ( augmented_dataset ) # prepare the train dataset for loading PyTorch DataLoader eval_dataset = EvalDataset ( load_dataset ( 'eugenesiow/Div2k' , 'bicubic_x4' , split = 'validation' )) # prepare the eval dataset for the PyTorch DataLoader The training code is provided below: from super_image import Trainer , TrainingArguments , EdsrModel , EdsrConfig training_args = TrainingArguments ( output_dir = './results' , # output directory num_train_epochs = 1000 , # total number of training epochs ) config = EdsrConfig ( scale = 4 , # train a model to upscale 4x ) model = EdsrModel ( config ) trainer = Trainer ( model = model , # the instantiated model to be trained args = training_args , # training arguments, defined above train_dataset = train_dataset , # training dataset eval_dataset = eval_dataset # evaluation dataset ) trainer . train ()","title":"Train Models"},{"location":"changelog/","text":"Changelog # All notable changes to this project will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning . [Unreleased] # [0.1.3] - 2021-07-28 # Added # TrainDataset(dataset) with support for huggingface datasets. augment_five_crop function for use with dataset.map(augment_five_crop, batched=True) [0.1.2] - 2021-07-26 # Added # Added metrics EvalMetrics().evaluate(model, eval_dataset) class to calculate PSNR and SSIM on a model and evaluation dataset. Accepts EvalDataset(dataset) with huggingface datasets. Changed # Replaced EvalDataset(dataset) to use huggingface datasets instead of H5 files. Fixed EvalDataset(dataset) to be robust to wrongly sized HR images (not equals to scaled LR image size).","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#unreleased","text":"","title":"[Unreleased]"},{"location":"changelog/#013-2021-07-28","text":"","title":"[0.1.3] - 2021-07-28"},{"location":"changelog/#added","text":"TrainDataset(dataset) with support for huggingface datasets. augment_five_crop function for use with dataset.map(augment_five_crop, batched=True)","title":"Added"},{"location":"changelog/#012-2021-07-26","text":"","title":"[0.1.2] - 2021-07-26"},{"location":"changelog/#added_1","text":"Added metrics EvalMetrics().evaluate(model, eval_dataset) class to calculate PSNR and SSIM on a model and evaluation dataset. Accepts EvalDataset(dataset) with huggingface datasets.","title":"Added"},{"location":"changelog/#changed","text":"Replaced EvalDataset(dataset) to use huggingface datasets instead of H5 files. Fixed EvalDataset(dataset) to be robust to wrongly sized HR images (not equals to scaled LR image size).","title":"Changed"},{"location":"code_of_conduct/","text":"Contributor Covenant Code of Conduct # Our Pledge # In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards # Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities # Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope # This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement # Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at kyo116@gmail.com . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution # This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at http://contributor-covenant.org/version/1/4","title":"Code of Conduct"},{"location":"code_of_conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"code_of_conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"code_of_conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"code_of_conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"code_of_conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"code_of_conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at kyo116@gmail.com . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"code_of_conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at http://contributor-covenant.org/version/1/4","title":"Attribution"},{"location":"contributing/","text":"Contributing # Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. Environment setup # Nothing easier! Fork and clone the repository, then: cd super-image make setup Note If it fails for some reason, you'll need to install Poetry manually. You can install it with: python3 -m pip install --user pipx pipx install poetry Now you can try running make setup again, or simply poetry install . You now have the dependencies installed. You can run the application with poetry run super-image [ARGS...] . Run make help to see all the available actions! Tasks # This project uses duty to run tasks. A Makefile is also provided. The Makefile will try to run certain tasks on multiple Python versions. If for some reason you don't want to run the task on multiple Python versions, you can do one of the following: export PYTHON_VERSIONS= : this will run the task with only the current Python version run the task directly with poetry run duty TASK , or duty TASK if the environment was already activated through poetry shell The Makefile detects if the Poetry environment is activated, so make will work the same with the virtualenv activated or not. Development # As usual: create a new branch: git checkout -b feature-or-bugfix-name edit the code and/or the documentation If you updated the documentation or the project dependencies: run make docs-regen run make docs-serve , go to http://localhost:8000 and check that everything looks good Before committing: run make format to auto-format the code run make check to check everything (fix any warning) run make test to run the tests (fix any issue) follow our commit message convention If you are unsure about how to fix or ignore a warning, just let the continuous integration fail, and we will help you during review. Don't bother updating the changelog, we will take care of this. Commit message convention # Commits messages must follow the Angular style : <type>[(scope)]: Subject [Body] Scope and body are optional. Type can be: build : About packaging, building wheels, etc. chore : About packaging or repo/files management. ci : About Continuous Integration. docs : About documentation. feat : New feature. fix : Bug fix. perf : About performance. refactor : Changes which are not features nor bug fixes. style : A change in code style/format. tests : About tests. Subject (and body) must be valid Markdown. If you write a body, please add issues references at the end: Body. References: #10, #11. Fixes #15. Pull requests guidelines # Link to any related issue in the Pull Request message. During review, we recommend using fixups: # SHA is the SHA of the commit you want to fix git commit --fixup = SHA Once all the changes are approved, you can squash your commits: git rebase -i --autosquash master And force-push: git push -f If this seems all too complicated, you can push or force-push each new commit, and we will squash them ourselves if needed, before merging.","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.","title":"Contributing"},{"location":"contributing/#environment-setup","text":"Nothing easier! Fork and clone the repository, then: cd super-image make setup Note If it fails for some reason, you'll need to install Poetry manually. You can install it with: python3 -m pip install --user pipx pipx install poetry Now you can try running make setup again, or simply poetry install . You now have the dependencies installed. You can run the application with poetry run super-image [ARGS...] . Run make help to see all the available actions!","title":"Environment setup"},{"location":"contributing/#tasks","text":"This project uses duty to run tasks. A Makefile is also provided. The Makefile will try to run certain tasks on multiple Python versions. If for some reason you don't want to run the task on multiple Python versions, you can do one of the following: export PYTHON_VERSIONS= : this will run the task with only the current Python version run the task directly with poetry run duty TASK , or duty TASK if the environment was already activated through poetry shell The Makefile detects if the Poetry environment is activated, so make will work the same with the virtualenv activated or not.","title":"Tasks"},{"location":"contributing/#development","text":"As usual: create a new branch: git checkout -b feature-or-bugfix-name edit the code and/or the documentation If you updated the documentation or the project dependencies: run make docs-regen run make docs-serve , go to http://localhost:8000 and check that everything looks good Before committing: run make format to auto-format the code run make check to check everything (fix any warning) run make test to run the tests (fix any issue) follow our commit message convention If you are unsure about how to fix or ignore a warning, just let the continuous integration fail, and we will help you during review. Don't bother updating the changelog, we will take care of this.","title":"Development"},{"location":"contributing/#commit-message-convention","text":"Commits messages must follow the Angular style : <type>[(scope)]: Subject [Body] Scope and body are optional. Type can be: build : About packaging, building wheels, etc. chore : About packaging or repo/files management. ci : About Continuous Integration. docs : About documentation. feat : New feature. fix : Bug fix. perf : About performance. refactor : Changes which are not features nor bug fixes. style : A change in code style/format. tests : About tests. Subject (and body) must be valid Markdown. If you write a body, please add issues references at the end: Body. References: #10, #11. Fixes #15.","title":"Commit message convention"},{"location":"contributing/#pull-requests-guidelines","text":"Link to any related issue in the Pull Request message. During review, we recommend using fixups: # SHA is the SHA of the commit you want to fix git commit --fixup = SHA Once all the changes are approved, you can squash your commits: git rebase -i --autosquash master And force-push: git push -f If this seems all too complicated, you can push or force-push each new commit, and we will squash them ourselves if needed, before merging.","title":"Pull requests guidelines"},{"location":"credits/","text":"Credits # These projects were used to build super-image . Thank you! python | poetry | copier-poetry Direct dependencies # autoflake | black | darglint | duty | flake8-bandit | flake8-black | flake8-bugbear | flake8-builtins | flake8-comprehensions | flake8-docstrings | flake8-pytest-style | flake8-string-format | flake8-tidy-imports | flake8-variables-names | git-changelog | h5py | httpx | huggingface-hub | isort | jinja2-cli | mkdocs | mkdocs-coverage | mkdocs-macros-plugin | mkdocs-material | mkdocstrings | mypy | opencv-python | pep8-naming | pytest | pytest-cov | pytest-randomly | pytest-sugar | pytest-xdist | toml | torch | torchvision | tqdm | wps-light Indirect dependencies # ansimarkup | appdirs | astor | astunparse | atomicwrites | attrs | bandit | cached-property | certifi | chardet | click | colorama | contextvars | coverage | dataclasses | execnet | failprint | filelock | flake8 | flake8-plugin-utils | flake8-polyfill | ghp-import | gitdb | GitPython | h11 | httpcore | idna | immutables | importlib-metadata | iniconfig | Jinja2 | Markdown | MarkupSafe | mccabe | mergedeep | mkdocs-autorefs | mkdocs-material-extensions | mypy-extensions | numpy | packaging | pathspec | pbr | Pillow | pluggy | ptyprocess | py | pycodestyle | pydocstyle | pyflakes | Pygments | pymdown-extensions | pyparsing | pytest-forked | python-dateutil | pytkdocs | PyYAML | pyyaml-env-tag | regex | requests | rfc3986 | six | smmap | sniffio | snowballstemmer | stevedore | termcolor | typed-ast | typing-extensions | urllib3 | watchdog | zipp More credits from the author","title":"Credits"},{"location":"credits/#credits","text":"These projects were used to build super-image . Thank you! python | poetry | copier-poetry","title":"Credits"},{"location":"credits/#direct-dependencies","text":"autoflake | black | darglint | duty | flake8-bandit | flake8-black | flake8-bugbear | flake8-builtins | flake8-comprehensions | flake8-docstrings | flake8-pytest-style | flake8-string-format | flake8-tidy-imports | flake8-variables-names | git-changelog | h5py | httpx | huggingface-hub | isort | jinja2-cli | mkdocs | mkdocs-coverage | mkdocs-macros-plugin | mkdocs-material | mkdocstrings | mypy | opencv-python | pep8-naming | pytest | pytest-cov | pytest-randomly | pytest-sugar | pytest-xdist | toml | torch | torchvision | tqdm | wps-light","title":"Direct dependencies"},{"location":"credits/#indirect-dependencies","text":"ansimarkup | appdirs | astor | astunparse | atomicwrites | attrs | bandit | cached-property | certifi | chardet | click | colorama | contextvars | coverage | dataclasses | execnet | failprint | filelock | flake8 | flake8-plugin-utils | flake8-polyfill | ghp-import | gitdb | GitPython | h11 | httpcore | idna | immutables | importlib-metadata | iniconfig | Jinja2 | Markdown | MarkupSafe | mccabe | mergedeep | mkdocs-autorefs | mkdocs-material-extensions | mypy-extensions | numpy | packaging | pathspec | pbr | Pillow | pluggy | ptyprocess | py | pycodestyle | pydocstyle | pyflakes | Pygments | pymdown-extensions | pyparsing | pytest-forked | python-dateutil | pytkdocs | PyYAML | pyyaml-env-tag | regex | requests | rfc3986 | six | smmap | sniffio | snowballstemmer | stevedore | termcolor | typed-ast | typing-extensions | urllib3 | watchdog | zipp More credits from the author","title":"Indirect dependencies"},{"location":"license/","text":"Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"models/edsr/","text":"Enhanced Deep Residual Networks for Single Image Super-Resolution (EDSR) # Overview # EDSR is a model that uses both deeper and wider architecture (32 ResBlocks and 256 channels) to improve performance. It uses both global and local skip connections, and up-scaling is done at the end of the network. It doesn't use batch normalization layers (input and output have similar distributions, normalizing intermediate features may not be desirable) instead it uses constant scaling layers to ensure stable training. An L1 loss function (absolute error) is used instead of L2 (MSE), the authors showed better performance empirically and it requires less computation. The default parameters are for the base model (~5mb vs ~100mb) that includes just 16 ResBlocks and 64 channels. It was introduced in the paper Enhanced Deep Residual Networks for Single Image Super-Resolution by Lim et al. (2017) and first released in this repository . EdsrConfig # This is the configuration class to store the configuration of a :class: ~super_image.EdsrModel . It is used to instantiate the model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the EDSR base architecture. Configuration objects inherit from :class: ~super_image.PretrainedConfig and can be used to control the model outputs. Read the documentation from :class: ~super_image.PretrainedConfig for more information. Examples: from super_image import EdsrModel , EdsrConfig # Initializing a configuration config = EdsrConfig ( scale = 4 , # train a model to upscale 4x ) # Initializing a model from the configuration model = EdsrModel ( config ) # Accessing the model configuration configuration = model . config __init__ ( self , scale = None , n_resblocks = 16 , n_feats = 64 , n_colors = 3 , rgb_range = 255 , res_scale = 1 , data_parallel = False , ** kwargs ) special # Parameters: Name Type Description Default scale int Scale for the model to train an upscaler/super-res model. None n_resblocks int Number of residual blocks. 16 n_feats int Number of features. 64 n_colors int Number of color channels. 3 rgb_range int Range of RGB as a multiplier to the MeanShift. 255 res_scale int The res scale multiplier. 1 Source code in super_image\\models\\edsr\\configuration_edsr.py def __init__ ( self , scale : int = None , n_resblocks = 16 , n_feats = 64 , n_colors = 3 , rgb_range = 255 , res_scale = 1 , data_parallel = False , ** kwargs ): \"\"\" Args: scale (int): Scale for the model to train an upscaler/super-res model. n_resblocks (int): Number of residual blocks. n_feats (int): Number of features. n_colors (int): Number of color channels. rgb_range (int): Range of RGB as a multiplier to the MeanShift. res_scale (int): The res scale multiplier. \"\"\" super () . __init__ ( ** kwargs ) self . scale = scale self . n_resblocks = n_resblocks self . n_feats = n_feats self . n_colors = n_colors self . rgb_range = rgb_range self . res_scale = res_scale self . data_parallel = data_parallel EdsrModel # config_class # This is the configuration class to store the configuration of a :class: ~super_image.EdsrModel . It is used to instantiate the model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the EDSR base architecture. Configuration objects inherit from :class: ~super_image.PretrainedConfig and can be used to control the model outputs. Read the documentation from :class: ~super_image.PretrainedConfig for more information. Examples: from super_image import EdsrModel , EdsrConfig # Initializing a configuration config = EdsrConfig ( scale = 4 , # train a model to upscale 4x ) # Initializing a model from the configuration model = EdsrModel ( config ) # Accessing the model configuration configuration = model . config __init__ ( self , scale = None , n_resblocks = 16 , n_feats = 64 , n_colors = 3 , rgb_range = 255 , res_scale = 1 , data_parallel = False , ** kwargs ) special # Parameters: Name Type Description Default scale int Scale for the model to train an upscaler/super-res model. None n_resblocks int Number of residual blocks. 16 n_feats int Number of features. 64 n_colors int Number of color channels. 3 rgb_range int Range of RGB as a multiplier to the MeanShift. 255 res_scale int The res scale multiplier. 1 Source code in super_image\\models\\edsr\\modeling_edsr.py def __init__ ( self , scale : int = None , n_resblocks = 16 , n_feats = 64 , n_colors = 3 , rgb_range = 255 , res_scale = 1 , data_parallel = False , ** kwargs ): \"\"\" Args: scale (int): Scale for the model to train an upscaler/super-res model. n_resblocks (int): Number of residual blocks. n_feats (int): Number of features. n_colors (int): Number of color channels. rgb_range (int): Range of RGB as a multiplier to the MeanShift. res_scale (int): The res scale multiplier. \"\"\" super () . __init__ ( ** kwargs ) self . scale = scale self . n_resblocks = n_resblocks self . n_feats = n_feats self . n_colors = n_colors self . rgb_range = rgb_range self . res_scale = res_scale self . data_parallel = data_parallel forward ( self , x ) # Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in super_image\\models\\edsr\\modeling_edsr.py def forward ( self , x ): x = self . head ( x ) res = self . body ( x ) res += x x = self . tail ( res ) return x load_state_dict ( self , state_dict , strict = True ) # Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys Source code in super_image\\models\\edsr\\modeling_edsr.py def load_state_dict ( self , state_dict , strict = True ): own_state = self . state_dict () for name , param in state_dict . items (): if name in own_state : if isinstance ( param , nn . Parameter ): param = param . data try : own_state [ name ] . copy_ ( param ) except Exception : if name . find ( 'tail' ) == - 1 : raise RuntimeError ( f 'While copying the parameter named { name } , ' f 'whose dimensions in the model are { own_state [ name ] . size () } and ' f 'whose dimensions in the checkpoint are { param . size () } .' ) elif strict : if name . find ( 'tail' ) == - 1 : raise KeyError ( f 'unexpected key \" { name } \" in state_dict' )","title":"EDSR"},{"location":"models/edsr/#enhanced-deep-residual-networks-for-single-image-super-resolution-edsr","text":"","title":"Enhanced Deep Residual Networks for Single Image Super-Resolution (EDSR)"},{"location":"models/edsr/#overview","text":"EDSR is a model that uses both deeper and wider architecture (32 ResBlocks and 256 channels) to improve performance. It uses both global and local skip connections, and up-scaling is done at the end of the network. It doesn't use batch normalization layers (input and output have similar distributions, normalizing intermediate features may not be desirable) instead it uses constant scaling layers to ensure stable training. An L1 loss function (absolute error) is used instead of L2 (MSE), the authors showed better performance empirically and it requires less computation. The default parameters are for the base model (~5mb vs ~100mb) that includes just 16 ResBlocks and 64 channels. It was introduced in the paper Enhanced Deep Residual Networks for Single Image Super-Resolution by Lim et al. (2017) and first released in this repository .","title":"Overview"},{"location":"models/edsr/#edsrconfig","text":"This is the configuration class to store the configuration of a :class: ~super_image.EdsrModel . It is used to instantiate the model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the EDSR base architecture. Configuration objects inherit from :class: ~super_image.PretrainedConfig and can be used to control the model outputs. Read the documentation from :class: ~super_image.PretrainedConfig for more information. Examples: from super_image import EdsrModel , EdsrConfig # Initializing a configuration config = EdsrConfig ( scale = 4 , # train a model to upscale 4x ) # Initializing a model from the configuration model = EdsrModel ( config ) # Accessing the model configuration configuration = model . config","title":"EdsrConfig"},{"location":"models/edsr/#super_image.models.edsr.configuration_edsr.EdsrConfig.__init__","text":"Parameters: Name Type Description Default scale int Scale for the model to train an upscaler/super-res model. None n_resblocks int Number of residual blocks. 16 n_feats int Number of features. 64 n_colors int Number of color channels. 3 rgb_range int Range of RGB as a multiplier to the MeanShift. 255 res_scale int The res scale multiplier. 1 Source code in super_image\\models\\edsr\\configuration_edsr.py def __init__ ( self , scale : int = None , n_resblocks = 16 , n_feats = 64 , n_colors = 3 , rgb_range = 255 , res_scale = 1 , data_parallel = False , ** kwargs ): \"\"\" Args: scale (int): Scale for the model to train an upscaler/super-res model. n_resblocks (int): Number of residual blocks. n_feats (int): Number of features. n_colors (int): Number of color channels. rgb_range (int): Range of RGB as a multiplier to the MeanShift. res_scale (int): The res scale multiplier. \"\"\" super () . __init__ ( ** kwargs ) self . scale = scale self . n_resblocks = n_resblocks self . n_feats = n_feats self . n_colors = n_colors self . rgb_range = rgb_range self . res_scale = res_scale self . data_parallel = data_parallel","title":"__init__()"},{"location":"models/edsr/#edsrmodel","text":"","title":"EdsrModel"},{"location":"models/edsr/#super_image.models.edsr.modeling_edsr.EdsrModel.config_class","text":"This is the configuration class to store the configuration of a :class: ~super_image.EdsrModel . It is used to instantiate the model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the EDSR base architecture. Configuration objects inherit from :class: ~super_image.PretrainedConfig and can be used to control the model outputs. Read the documentation from :class: ~super_image.PretrainedConfig for more information. Examples: from super_image import EdsrModel , EdsrConfig # Initializing a configuration config = EdsrConfig ( scale = 4 , # train a model to upscale 4x ) # Initializing a model from the configuration model = EdsrModel ( config ) # Accessing the model configuration configuration = model . config","title":"config_class"},{"location":"models/edsr/#super_image.models.edsr.modeling_edsr.EdsrModel.config_class.__init__","text":"Parameters: Name Type Description Default scale int Scale for the model to train an upscaler/super-res model. None n_resblocks int Number of residual blocks. 16 n_feats int Number of features. 64 n_colors int Number of color channels. 3 rgb_range int Range of RGB as a multiplier to the MeanShift. 255 res_scale int The res scale multiplier. 1 Source code in super_image\\models\\edsr\\modeling_edsr.py def __init__ ( self , scale : int = None , n_resblocks = 16 , n_feats = 64 , n_colors = 3 , rgb_range = 255 , res_scale = 1 , data_parallel = False , ** kwargs ): \"\"\" Args: scale (int): Scale for the model to train an upscaler/super-res model. n_resblocks (int): Number of residual blocks. n_feats (int): Number of features. n_colors (int): Number of color channels. rgb_range (int): Range of RGB as a multiplier to the MeanShift. res_scale (int): The res scale multiplier. \"\"\" super () . __init__ ( ** kwargs ) self . scale = scale self . n_resblocks = n_resblocks self . n_feats = n_feats self . n_colors = n_colors self . rgb_range = rgb_range self . res_scale = res_scale self . data_parallel = data_parallel","title":"__init__()"},{"location":"models/edsr/#super_image.models.edsr.modeling_edsr.EdsrModel.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in super_image\\models\\edsr\\modeling_edsr.py def forward ( self , x ): x = self . head ( x ) res = self . body ( x ) res += x x = self . tail ( res ) return x","title":"forward()"},{"location":"models/edsr/#super_image.models.edsr.modeling_edsr.EdsrModel.load_state_dict","text":"Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys Source code in super_image\\models\\edsr\\modeling_edsr.py def load_state_dict ( self , state_dict , strict = True ): own_state = self . state_dict () for name , param in state_dict . items (): if name in own_state : if isinstance ( param , nn . Parameter ): param = param . data try : own_state [ name ] . copy_ ( param ) except Exception : if name . find ( 'tail' ) == - 1 : raise RuntimeError ( f 'While copying the parameter named { name } , ' f 'whose dimensions in the model are { own_state [ name ] . size () } and ' f 'whose dimensions in the checkpoint are { param . size () } .' ) elif strict : if name . find ( 'tail' ) == - 1 : raise KeyError ( f 'unexpected key \" { name } \" in state_dict' )","title":"load_state_dict()"},{"location":"models/msrn/","text":"Multi-scale Residual Network for Image Super-Resolution (MSRN) # Overview # The MSRN model proposes a feature extraction structure called the multi-scale residual block. This module can \"adaptively detect image features at different scales\" and \"exploit the potential features of the image\". This model also applies the balanced attention (BAM) method invented by Wang et al. (2021) to further improve the results. It was introduced in the paper Multi-scale Residual Network for Image Super-Resolution by Li et al. (2018) and first released in this repository . MsrnConfig # MsrnModel # forward ( self , x ) # Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in super_image\\models\\msrn\\modeling_msrn.py def forward ( self , x ): # x = self.sub_mean(x) x = self . head ( x ) res = x MSRB_out = [] for i in range ( self . n_blocks ): x = self . body [ i ]( x ) MSRB_out . append ( x ) MSRB_out . append ( res ) res = torch . cat ( MSRB_out , 1 ) x = self . tail ( res ) # x = self.add_mean(x) return x load_state_dict ( self , state_dict , strict = True ) # Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys Source code in super_image\\models\\msrn\\modeling_msrn.py def load_state_dict ( self , state_dict , strict = True ): own_state = self . state_dict () for name , param in state_dict . items (): if name in own_state : if isinstance ( param , nn . Parameter ): param = param . data try : own_state [ name ] . copy_ ( param ) except Exception : if name . find ( 'tail' ) >= 0 : print ( 'Replace pre-trained upsampler to new one...' ) else : raise RuntimeError ( f 'While copying the parameter named { name } , ' f 'whose dimensions in the model are { own_state [ name ] . size () } and ' f 'whose dimensions in the checkpoint are { param . size () } .' ) elif strict : if name . find ( 'tail' ) == - 1 : raise KeyError ( f 'unexpected key \" { name } \" in state_dict' ) if strict : missing = set ( own_state . keys ()) - set ( state_dict . keys ()) if len ( missing ) > 0 : raise KeyError ( f 'missing keys in state_dict: \" { missing } \"' )","title":"MSRN"},{"location":"models/msrn/#multi-scale-residual-network-for-image-super-resolution-msrn","text":"","title":"Multi-scale Residual Network for Image Super-Resolution (MSRN)"},{"location":"models/msrn/#overview","text":"The MSRN model proposes a feature extraction structure called the multi-scale residual block. This module can \"adaptively detect image features at different scales\" and \"exploit the potential features of the image\". This model also applies the balanced attention (BAM) method invented by Wang et al. (2021) to further improve the results. It was introduced in the paper Multi-scale Residual Network for Image Super-Resolution by Li et al. (2018) and first released in this repository .","title":"Overview"},{"location":"models/msrn/#msrnconfig","text":"","title":"MsrnConfig"},{"location":"models/msrn/#msrnmodel","text":"","title":"MsrnModel"},{"location":"models/msrn/#super_image.models.msrn.modeling_msrn.MsrnModel.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in super_image\\models\\msrn\\modeling_msrn.py def forward ( self , x ): # x = self.sub_mean(x) x = self . head ( x ) res = x MSRB_out = [] for i in range ( self . n_blocks ): x = self . body [ i ]( x ) MSRB_out . append ( x ) MSRB_out . append ( res ) res = torch . cat ( MSRB_out , 1 ) x = self . tail ( res ) # x = self.add_mean(x) return x","title":"forward()"},{"location":"models/msrn/#super_image.models.msrn.modeling_msrn.MsrnModel.load_state_dict","text":"Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. required strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True True Returns: Type Description ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys Source code in super_image\\models\\msrn\\modeling_msrn.py def load_state_dict ( self , state_dict , strict = True ): own_state = self . state_dict () for name , param in state_dict . items (): if name in own_state : if isinstance ( param , nn . Parameter ): param = param . data try : own_state [ name ] . copy_ ( param ) except Exception : if name . find ( 'tail' ) >= 0 : print ( 'Replace pre-trained upsampler to new one...' ) else : raise RuntimeError ( f 'While copying the parameter named { name } , ' f 'whose dimensions in the model are { own_state [ name ] . size () } and ' f 'whose dimensions in the checkpoint are { param . size () } .' ) elif strict : if name . find ( 'tail' ) == - 1 : raise KeyError ( f 'unexpected key \" { name } \" in state_dict' ) if strict : missing = set ( own_state . keys ()) - set ( state_dict . keys ()) if len ( missing ) > 0 : raise KeyError ( f 'missing keys in state_dict: \" { missing } \"' )","title":"load_state_dict()"},{"location":"reference/cli/","text":"Module that contains the command line application. get_parser () # Return the CLI argument parser. Returns: Type Description ArgumentParser An argparse parser. Source code in super_image\\cli.py def get_parser () -> argparse . ArgumentParser : \"\"\" Return the CLI argument parser. Returns: An argparse parser. \"\"\" return argparse . ArgumentParser ( prog = \"super-image\" ) main ( args = None ) # Run the main program. This function is executed when you type super-image or python -m super_image . Parameters: Name Type Description Default args Optional[List[str]] Arguments passed from the command line. None Returns: Type Description int An exit code. Source code in super_image\\cli.py def main ( args : Optional [ List [ str ]] = None ) -> int : \"\"\" Run the main program. This function is executed when you type `super-image` or `python -m super_image`. Arguments: args: Arguments passed from the command line. Returns: An exit code. \"\"\" parser = get_parser () opts = parser . parse_args ( args = args ) print ( opts ) # noqa: WPS421 (side-effect in main is fine) return 0","title":"CLI"},{"location":"reference/cli/#super_image.cli.get_parser","text":"Return the CLI argument parser. Returns: Type Description ArgumentParser An argparse parser. Source code in super_image\\cli.py def get_parser () -> argparse . ArgumentParser : \"\"\" Return the CLI argument parser. Returns: An argparse parser. \"\"\" return argparse . ArgumentParser ( prog = \"super-image\" )","title":"get_parser()"},{"location":"reference/cli/#super_image.cli.main","text":"Run the main program. This function is executed when you type super-image or python -m super_image . Parameters: Name Type Description Default args Optional[List[str]] Arguments passed from the command line. None Returns: Type Description int An exit code. Source code in super_image\\cli.py def main ( args : Optional [ List [ str ]] = None ) -> int : \"\"\" Run the main program. This function is executed when you type `super-image` or `python -m super_image`. Arguments: args: Arguments passed from the command line. Returns: An exit code. \"\"\" parser = get_parser () opts = parser . parse_args ( args = args ) print ( opts ) # noqa: WPS421 (side-effect in main is fine) return 0","title":"main()"},{"location":"reference/configuration/","text":"Configuration # The base class PretrainedConfig implements the common methods for loading/saving a configuration either from a local file or directory, or from a pretrained model configuration provided by the library (downloaded from HuggingFace's Hub ). PretrainedConfig # to_json_string : str property readonly # Serializes this instance to a JSON string. Returns: Type Description str :obj: str : String containing all the attributes that make up this configuration instance in JSON format. from_dict ( config_dict , ** kwargs ) classmethod # Instantiates a :class: ~super_image.PretrainedConfig from a Python dictionary of parameters. Parameters: Name Type Description Default config_dict Dict[str, Any] obj: Dict[str, Any] ): Dictionary that will be used to instantiate the configuration object. Such a dictionary can be retrieved from a pretrained checkpoint by leveraging the :func: ~super_image.PretrainedConfig.get_config_dict method. required kwargs obj: Dict[str, Any] ): Additional parameters from which to initialize the configuration object. {} Returns: Type Description Tuple[PretrainedConfig, Dict[str, Any]] :class: PretrainedConfig : The configuration object instantiated from those parameters. Source code in super_image\\configuration_utils.py @classmethod def from_dict ( cls , config_dict : Dict [ str , Any ], ** kwargs ) -> Tuple [ \"PretrainedConfig\" , Dict [ str , Any ]]: \"\"\" Instantiates a :class:`~super_image.PretrainedConfig` from a Python dictionary of parameters. Args: config_dict (:obj:`Dict[str, Any]`): Dictionary that will be used to instantiate the configuration object. Such a dictionary can be retrieved from a pretrained checkpoint by leveraging the :func:`~super_image.PretrainedConfig.get_config_dict` method. kwargs (:obj:`Dict[str, Any]`): Additional parameters from which to initialize the configuration object. Returns: :class:`PretrainedConfig`: The configuration object instantiated from those parameters. \"\"\" config = cls ( ** config_dict ) # Update config with kwargs if needed to_remove = [] for key , value in kwargs . items (): if hasattr ( config , key ): setattr ( config , key , value ) to_remove . append ( key ) for key in to_remove : kwargs . pop ( key , None ) logger . info ( f \"Model config { config } \" ) return config , kwargs from_json_file ( json_file ) classmethod # Instantiates a :class: ~super_image.PretrainedConfig from the path to a JSON file of parameters. Parameters: Name Type Description Default json_file Union[str, os.PathLike] obj: str or :obj: os.PathLike ): Path to the JSON file containing the parameters. required Returns: Type Description PretrainedConfig :class: PretrainedConfig : The configuration object instantiated from that JSON file. Source code in super_image\\configuration_utils.py @classmethod def from_json_file ( cls , json_file : Union [ str , os . PathLike ]) -> \"PretrainedConfig\" : \"\"\" Instantiates a :class:`~super_image.PretrainedConfig` from the path to a JSON file of parameters. Args: json_file (:obj:`str` or :obj:`os.PathLike`): Path to the JSON file containing the parameters. Returns: :class:`PretrainedConfig`: The configuration object instantiated from that JSON file. \"\"\" config_dict = cls . _dict_from_json_file ( json_file ) return cls ( ** config_dict ) get_config_dict ( pretrained_model_name_or_path , ** kwargs ) classmethod # From a pretrained_model_name_or_path , resolve to a dictionary of parameters, to be used for instantiating a :class: ~super_image.PretrainedConfig using from_dict . Parameters: Name Type Description Default pretrained_model_name_or_path Union[str, os.PathLike] obj: str or :obj: os.PathLike ): The identifier of the pre-trained checkpoint from which we want the dictionary of parameters. required Returns: Type Description Tuple[Dict[str, Any], Dict[str, Any]] :obj: Tuple[Dict, Dict] : The dictionary(ies) that will be used to instantiate the configuration object. Source code in super_image\\configuration_utils.py @classmethod def get_config_dict ( cls , pretrained_model_name_or_path : Union [ str , os . PathLike ], ** kwargs ) -> Tuple [ Dict [ str , Any ], Dict [ str , Any ]]: \"\"\" From a ``pretrained_model_name_or_path``, resolve to a dictionary of parameters, to be used for instantiating a :class:`~super_image.PretrainedConfig` using ``from_dict``. Parameters: pretrained_model_name_or_path (:obj:`str` or :obj:`os.PathLike`): The identifier of the pre-trained checkpoint from which we want the dictionary of parameters. Returns: :obj:`Tuple[Dict, Dict]`: The dictionary(ies) that will be used to instantiate the configuration object. \"\"\" scale = kwargs . pop ( \"scale\" , None ) cache_dir = kwargs . pop ( \"cache_dir\" , None ) revision = kwargs . pop ( \"revision\" , None ) pretrained_model_name_or_path = str ( pretrained_model_name_or_path ) if os . path . isdir ( pretrained_model_name_or_path ): config_file = os . path . join ( pretrained_model_name_or_path , CONFIG_NAME ) elif os . path . isfile ( pretrained_model_name_or_path ) or is_remote_url ( pretrained_model_name_or_path ): config_file = pretrained_model_name_or_path else : config_file = get_model_url ( pretrained_model_name_or_path , filename = CONFIG_NAME , revision = revision ) try : # Load from URL or cache if already cached resolved_config_file = get_model_path ( config_file , cache_dir = cache_dir ) # Load config dict config_dict = cls . _dict_from_json_file ( resolved_config_file ) if scale is not None : config_dict [ 'scale' ] = scale except EnvironmentError as err : logger . error ( err ) msg = ( f \"Can't load config for ' { pretrained_model_name_or_path } '. Make sure that: \\n\\n \" f \"- ' { pretrained_model_name_or_path } ' is a correct model identifier \\n\\n \" f \"- or ' { pretrained_model_name_or_path } ' is the correct path to a directory containing a { CONFIG_NAME } file \\n\\n \" ) raise EnvironmentError ( msg ) except json . JSONDecodeError : msg = ( f \"Couldn't reach server at ' { config_file } ' to download configuration file or \" \"configuration file is not a valid JSON file. \" f \"Please check network or file content here: { resolved_config_file } .\" ) raise EnvironmentError ( msg ) if resolved_config_file == config_file : logger . info ( f \"loading configuration file { config_file } \" ) else : logger . info ( f \"loading configuration file { config_file } from cache at { resolved_config_file } \" ) return config_dict , kwargs save_pretrained ( self , save_directory ) # Save a configuration object to the directory save_directory , so that it can be re-loaded using the :func: ~super_image.PretrainedConfig.from_pretrained class method. Parameters: Name Type Description Default save_directory Union[str, os.PathLike] obj: str or :obj: os.PathLike ): Directory where the configuration JSON file will be saved (will be created if it does not exist). required Source code in super_image\\configuration_utils.py def save_pretrained ( self , save_directory : Union [ str , os . PathLike ]): \"\"\" Save a configuration object to the directory ``save_directory``, so that it can be re-loaded using the :func:`~super_image.PretrainedConfig.from_pretrained` class method. Args: save_directory (:obj:`str` or :obj:`os.PathLike`): Directory where the configuration JSON file will be saved (will be created if it does not exist). \"\"\" if os . path . isfile ( save_directory ): raise AssertionError ( f \"Provided path ( { save_directory } ) should be a directory, not a file\" ) os . makedirs ( save_directory , exist_ok = True ) # If we save using the predefined names, we can load using `from_pretrained` output_config_file = os . path . join ( save_directory , CONFIG_NAME ) self . to_json_file ( output_config_file ) logger . info ( f \"Configuration saved in { output_config_file } \" ) to_dict ( self ) # Serializes this instance to a Python dictionary. Returns: Type Description Dict[str, Any] :obj: Dict[str, Any] : Dictionary of all the attributes that make up this configuration instance. Source code in super_image\\configuration_utils.py def to_dict ( self ) -> Dict [ str , Any ]: \"\"\" Serializes this instance to a Python dictionary. Returns: :obj:`Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance. \"\"\" output = copy . deepcopy ( self . __dict__ ) if hasattr ( self . __class__ , \"model_type\" ): output [ \"model_type\" ] = self . __class__ . model_type return output to_json_file ( self , json_file_path ) # Save this instance to a JSON file. Parameters: Name Type Description Default json_file_path Union[str, os.PathLike] obj: str or :obj: os.PathLike ): Path to the JSON file in which this configuration instance's parameters will be saved. required Source code in super_image\\configuration_utils.py def to_json_file ( self , json_file_path : Union [ str , os . PathLike ]): \"\"\" Save this instance to a JSON file. Args: json_file_path (:obj:`str` or :obj:`os.PathLike`): Path to the JSON file in which this configuration instance's parameters will be saved. \"\"\" with open ( json_file_path , \"w\" , encoding = \"utf-8\" ) as writer : writer . write ( self . to_json_string )","title":"Configuration"},{"location":"reference/configuration/#configuration","text":"The base class PretrainedConfig implements the common methods for loading/saving a configuration either from a local file or directory, or from a pretrained model configuration provided by the library (downloaded from HuggingFace's Hub ).","title":"Configuration"},{"location":"reference/configuration/#pretrainedconfig","text":"","title":"PretrainedConfig"},{"location":"reference/configuration/#super_image.configuration_utils.PretrainedConfig.to_json_string","text":"Serializes this instance to a JSON string. Returns: Type Description str :obj: str : String containing all the attributes that make up this configuration instance in JSON format.","title":"to_json_string"},{"location":"reference/configuration/#super_image.configuration_utils.PretrainedConfig.from_dict","text":"Instantiates a :class: ~super_image.PretrainedConfig from a Python dictionary of parameters. Parameters: Name Type Description Default config_dict Dict[str, Any] obj: Dict[str, Any] ): Dictionary that will be used to instantiate the configuration object. Such a dictionary can be retrieved from a pretrained checkpoint by leveraging the :func: ~super_image.PretrainedConfig.get_config_dict method. required kwargs obj: Dict[str, Any] ): Additional parameters from which to initialize the configuration object. {} Returns: Type Description Tuple[PretrainedConfig, Dict[str, Any]] :class: PretrainedConfig : The configuration object instantiated from those parameters. Source code in super_image\\configuration_utils.py @classmethod def from_dict ( cls , config_dict : Dict [ str , Any ], ** kwargs ) -> Tuple [ \"PretrainedConfig\" , Dict [ str , Any ]]: \"\"\" Instantiates a :class:`~super_image.PretrainedConfig` from a Python dictionary of parameters. Args: config_dict (:obj:`Dict[str, Any]`): Dictionary that will be used to instantiate the configuration object. Such a dictionary can be retrieved from a pretrained checkpoint by leveraging the :func:`~super_image.PretrainedConfig.get_config_dict` method. kwargs (:obj:`Dict[str, Any]`): Additional parameters from which to initialize the configuration object. Returns: :class:`PretrainedConfig`: The configuration object instantiated from those parameters. \"\"\" config = cls ( ** config_dict ) # Update config with kwargs if needed to_remove = [] for key , value in kwargs . items (): if hasattr ( config , key ): setattr ( config , key , value ) to_remove . append ( key ) for key in to_remove : kwargs . pop ( key , None ) logger . info ( f \"Model config { config } \" ) return config , kwargs","title":"from_dict()"},{"location":"reference/configuration/#super_image.configuration_utils.PretrainedConfig.from_json_file","text":"Instantiates a :class: ~super_image.PretrainedConfig from the path to a JSON file of parameters. Parameters: Name Type Description Default json_file Union[str, os.PathLike] obj: str or :obj: os.PathLike ): Path to the JSON file containing the parameters. required Returns: Type Description PretrainedConfig :class: PretrainedConfig : The configuration object instantiated from that JSON file. Source code in super_image\\configuration_utils.py @classmethod def from_json_file ( cls , json_file : Union [ str , os . PathLike ]) -> \"PretrainedConfig\" : \"\"\" Instantiates a :class:`~super_image.PretrainedConfig` from the path to a JSON file of parameters. Args: json_file (:obj:`str` or :obj:`os.PathLike`): Path to the JSON file containing the parameters. Returns: :class:`PretrainedConfig`: The configuration object instantiated from that JSON file. \"\"\" config_dict = cls . _dict_from_json_file ( json_file ) return cls ( ** config_dict )","title":"from_json_file()"},{"location":"reference/configuration/#super_image.configuration_utils.PretrainedConfig.get_config_dict","text":"From a pretrained_model_name_or_path , resolve to a dictionary of parameters, to be used for instantiating a :class: ~super_image.PretrainedConfig using from_dict . Parameters: Name Type Description Default pretrained_model_name_or_path Union[str, os.PathLike] obj: str or :obj: os.PathLike ): The identifier of the pre-trained checkpoint from which we want the dictionary of parameters. required Returns: Type Description Tuple[Dict[str, Any], Dict[str, Any]] :obj: Tuple[Dict, Dict] : The dictionary(ies) that will be used to instantiate the configuration object. Source code in super_image\\configuration_utils.py @classmethod def get_config_dict ( cls , pretrained_model_name_or_path : Union [ str , os . PathLike ], ** kwargs ) -> Tuple [ Dict [ str , Any ], Dict [ str , Any ]]: \"\"\" From a ``pretrained_model_name_or_path``, resolve to a dictionary of parameters, to be used for instantiating a :class:`~super_image.PretrainedConfig` using ``from_dict``. Parameters: pretrained_model_name_or_path (:obj:`str` or :obj:`os.PathLike`): The identifier of the pre-trained checkpoint from which we want the dictionary of parameters. Returns: :obj:`Tuple[Dict, Dict]`: The dictionary(ies) that will be used to instantiate the configuration object. \"\"\" scale = kwargs . pop ( \"scale\" , None ) cache_dir = kwargs . pop ( \"cache_dir\" , None ) revision = kwargs . pop ( \"revision\" , None ) pretrained_model_name_or_path = str ( pretrained_model_name_or_path ) if os . path . isdir ( pretrained_model_name_or_path ): config_file = os . path . join ( pretrained_model_name_or_path , CONFIG_NAME ) elif os . path . isfile ( pretrained_model_name_or_path ) or is_remote_url ( pretrained_model_name_or_path ): config_file = pretrained_model_name_or_path else : config_file = get_model_url ( pretrained_model_name_or_path , filename = CONFIG_NAME , revision = revision ) try : # Load from URL or cache if already cached resolved_config_file = get_model_path ( config_file , cache_dir = cache_dir ) # Load config dict config_dict = cls . _dict_from_json_file ( resolved_config_file ) if scale is not None : config_dict [ 'scale' ] = scale except EnvironmentError as err : logger . error ( err ) msg = ( f \"Can't load config for ' { pretrained_model_name_or_path } '. Make sure that: \\n\\n \" f \"- ' { pretrained_model_name_or_path } ' is a correct model identifier \\n\\n \" f \"- or ' { pretrained_model_name_or_path } ' is the correct path to a directory containing a { CONFIG_NAME } file \\n\\n \" ) raise EnvironmentError ( msg ) except json . JSONDecodeError : msg = ( f \"Couldn't reach server at ' { config_file } ' to download configuration file or \" \"configuration file is not a valid JSON file. \" f \"Please check network or file content here: { resolved_config_file } .\" ) raise EnvironmentError ( msg ) if resolved_config_file == config_file : logger . info ( f \"loading configuration file { config_file } \" ) else : logger . info ( f \"loading configuration file { config_file } from cache at { resolved_config_file } \" ) return config_dict , kwargs","title":"get_config_dict()"},{"location":"reference/configuration/#super_image.configuration_utils.PretrainedConfig.save_pretrained","text":"Save a configuration object to the directory save_directory , so that it can be re-loaded using the :func: ~super_image.PretrainedConfig.from_pretrained class method. Parameters: Name Type Description Default save_directory Union[str, os.PathLike] obj: str or :obj: os.PathLike ): Directory where the configuration JSON file will be saved (will be created if it does not exist). required Source code in super_image\\configuration_utils.py def save_pretrained ( self , save_directory : Union [ str , os . PathLike ]): \"\"\" Save a configuration object to the directory ``save_directory``, so that it can be re-loaded using the :func:`~super_image.PretrainedConfig.from_pretrained` class method. Args: save_directory (:obj:`str` or :obj:`os.PathLike`): Directory where the configuration JSON file will be saved (will be created if it does not exist). \"\"\" if os . path . isfile ( save_directory ): raise AssertionError ( f \"Provided path ( { save_directory } ) should be a directory, not a file\" ) os . makedirs ( save_directory , exist_ok = True ) # If we save using the predefined names, we can load using `from_pretrained` output_config_file = os . path . join ( save_directory , CONFIG_NAME ) self . to_json_file ( output_config_file ) logger . info ( f \"Configuration saved in { output_config_file } \" )","title":"save_pretrained()"},{"location":"reference/configuration/#super_image.configuration_utils.PretrainedConfig.to_dict","text":"Serializes this instance to a Python dictionary. Returns: Type Description Dict[str, Any] :obj: Dict[str, Any] : Dictionary of all the attributes that make up this configuration instance. Source code in super_image\\configuration_utils.py def to_dict ( self ) -> Dict [ str , Any ]: \"\"\" Serializes this instance to a Python dictionary. Returns: :obj:`Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance. \"\"\" output = copy . deepcopy ( self . __dict__ ) if hasattr ( self . __class__ , \"model_type\" ): output [ \"model_type\" ] = self . __class__ . model_type return output","title":"to_dict()"},{"location":"reference/configuration/#super_image.configuration_utils.PretrainedConfig.to_json_file","text":"Save this instance to a JSON file. Parameters: Name Type Description Default json_file_path Union[str, os.PathLike] obj: str or :obj: os.PathLike ): Path to the JSON file in which this configuration instance's parameters will be saved. required Source code in super_image\\configuration_utils.py def to_json_file ( self , json_file_path : Union [ str , os . PathLike ]): \"\"\" Save this instance to a JSON file. Args: json_file_path (:obj:`str` or :obj:`os.PathLike`): Path to the JSON file in which this configuration instance's parameters will be saved. \"\"\" with open ( json_file_path , \"w\" , encoding = \"utf-8\" ) as writer : writer . write ( self . to_json_string )","title":"to_json_file()"},{"location":"reference/models/","text":"Models # The base class PreTrainedModel implements the common methods for loading/saving a model either from a local file or directory, or from a pretrained model configuration provided by the library (downloaded from HuggingFace's Hub ). PreTrainedModel # save_pretrained ( self , save_directory , save_config = True , state_dict = None , save_function =< function save at 0x000002866B3333A8 > ) # Save a model and its configuration file to a directory, so that it can be re-loaded using the :func: ~super_image.PreTrainedModel.from_pretrained`` class method. Parameters: Name Type Description Default save_directory Union[str, os.PathLike] obj: str or :obj: os.PathLike ): Directory to which to save. Will be created if it doesn't exist. required save_config bool obj: bool , optional , defaults to :obj: True ): Whether or not to save the config of the model. Useful when in distributed training like TPUs and need to call this function on all processes. In this case, set :obj: save_config=True only on the main process to avoid race conditions. True state_dict Optional[dict] obj: torch.Tensor ): The state dictionary of the model to save. Will default to :obj: self.state_dict() , but can be used to only save parts of the model or if special precautions need to be taken when recovering the state dictionary of a model (like when using model parallelism). None save_function Callable obj: Callable ): The function to use to save the state dictionary. When we need to replace :obj: torch.save by another method. <function save at 0x000002866B3333A8> Source code in super_image\\modeling_utils.py def save_pretrained ( self , save_directory : Union [ str , os . PathLike ], save_config : bool = True , state_dict : Optional [ dict ] = None , save_function : Callable = torch . save , ): \"\"\" Save a model and its configuration file to a directory, so that it can be re-loaded using the `:func:`~super_image.PreTrainedModel.from_pretrained`` class method. Arguments: save_directory (:obj:`str` or :obj:`os.PathLike`): Directory to which to save. Will be created if it doesn't exist. save_config (:obj:`bool`, `optional`, defaults to :obj:`True`): Whether or not to save the config of the model. Useful when in distributed training like TPUs and need to call this function on all processes. In this case, set :obj:`save_config=True` only on the main process to avoid race conditions. state_dict (nested dictionary of :obj:`torch.Tensor`): The state dictionary of the model to save. Will default to :obj:`self.state_dict()`, but can be used to only save parts of the model or if special precautions need to be taken when recovering the state dictionary of a model (like when using model parallelism). save_function (:obj:`Callable`): The function to use to save the state dictionary. When we need to replace :obj:`torch.save` by another method. \"\"\" if os . path . isfile ( save_directory ): logger . error ( f \"Provided path ( { save_directory } ) should be a directory, not a file\" ) return os . makedirs ( save_directory , exist_ok = True ) model_to_save = self # Setup scale scale = self . config . scale if scale is not None : weights_name = WEIGHTS_NAME_SCALE . format ( scale = scale ) else : weights_name = WEIGHTS_NAME # Save the config if save_config : model_to_save . config . save_pretrained ( save_directory ) # Save the model if state_dict is None : state_dict = model_to_save . state_dict () # If we save using the predefined names, we can load using `from_pretrained` output_model_file = os . path . join ( save_directory , weights_name ) save_function ( state_dict , output_model_file ) logger . info ( f \"Model weights saved in { output_model_file } \" )","title":"Models"},{"location":"reference/models/#models","text":"The base class PreTrainedModel implements the common methods for loading/saving a model either from a local file or directory, or from a pretrained model configuration provided by the library (downloaded from HuggingFace's Hub ).","title":"Models"},{"location":"reference/models/#pretrainedmodel","text":"","title":"PreTrainedModel"},{"location":"reference/models/#super_image.modeling_utils.PreTrainedModel.save_pretrained","text":"Save a model and its configuration file to a directory, so that it can be re-loaded using the :func: ~super_image.PreTrainedModel.from_pretrained`` class method. Parameters: Name Type Description Default save_directory Union[str, os.PathLike] obj: str or :obj: os.PathLike ): Directory to which to save. Will be created if it doesn't exist. required save_config bool obj: bool , optional , defaults to :obj: True ): Whether or not to save the config of the model. Useful when in distributed training like TPUs and need to call this function on all processes. In this case, set :obj: save_config=True only on the main process to avoid race conditions. True state_dict Optional[dict] obj: torch.Tensor ): The state dictionary of the model to save. Will default to :obj: self.state_dict() , but can be used to only save parts of the model or if special precautions need to be taken when recovering the state dictionary of a model (like when using model parallelism). None save_function Callable obj: Callable ): The function to use to save the state dictionary. When we need to replace :obj: torch.save by another method. <function save at 0x000002866B3333A8> Source code in super_image\\modeling_utils.py def save_pretrained ( self , save_directory : Union [ str , os . PathLike ], save_config : bool = True , state_dict : Optional [ dict ] = None , save_function : Callable = torch . save , ): \"\"\" Save a model and its configuration file to a directory, so that it can be re-loaded using the `:func:`~super_image.PreTrainedModel.from_pretrained`` class method. Arguments: save_directory (:obj:`str` or :obj:`os.PathLike`): Directory to which to save. Will be created if it doesn't exist. save_config (:obj:`bool`, `optional`, defaults to :obj:`True`): Whether or not to save the config of the model. Useful when in distributed training like TPUs and need to call this function on all processes. In this case, set :obj:`save_config=True` only on the main process to avoid race conditions. state_dict (nested dictionary of :obj:`torch.Tensor`): The state dictionary of the model to save. Will default to :obj:`self.state_dict()`, but can be used to only save parts of the model or if special precautions need to be taken when recovering the state dictionary of a model (like when using model parallelism). save_function (:obj:`Callable`): The function to use to save the state dictionary. When we need to replace :obj:`torch.save` by another method. \"\"\" if os . path . isfile ( save_directory ): logger . error ( f \"Provided path ( { save_directory } ) should be a directory, not a file\" ) return os . makedirs ( save_directory , exist_ok = True ) model_to_save = self # Setup scale scale = self . config . scale if scale is not None : weights_name = WEIGHTS_NAME_SCALE . format ( scale = scale ) else : weights_name = WEIGHTS_NAME # Save the config if save_config : model_to_save . config . save_pretrained ( save_directory ) # Save the model if state_dict is None : state_dict = model_to_save . state_dict () # If we save using the predefined names, we can load using `from_pretrained` output_model_file = os . path . join ( save_directory , weights_name ) save_function ( state_dict , output_model_file ) logger . info ( f \"Model weights saved in { output_model_file } \" )","title":"save_pretrained()"},{"location":"reference/trainer/","text":"Trainer # The Trainer class provides an API for feature-complete training in most standard use cases. Before instantiating your Trainer , create a TrainingArguments to access all the points of customization during training. Trainer # Trainer is a simple class implementing the training and eval loop for PyTorch to train a super-image model. Parameters: Name Type Description Default model class: ~super_image.PreTrainedModel or :obj: torch.nn.Module , optional ): The model to train, evaluate or use for predictions. If not provided, a model_init must be passed. .. note:: :class: ~super_image.Trainer is optimized to work with the :class: ~super_image.PreTrainedModel provided by the library. You can still use your own models defined as :obj: torch.nn.Module as long as they work the same way as the super_image models. required args class: ~super_image.TrainingArguments , optional ): The arguments to tweak for training. Will default to a basic instance of :class: ~super_image.TrainingArguments with the output_dir set to a directory named tmp_trainer in the current directory if not provided. required train_dataset obj: torch.utils.data.dataset.Dataset or :obj: torch.utils.data.dataset.IterableDataset ): The dataset to use for training. required eval_dataset obj: torch.utils.data.dataset.Dataset , optional ): The dataset to use for evaluation. required get_eval_dataloader ( self ) # Returns the evaluation :class: ~torch.utils.data.DataLoader . Source code in super_image\\trainer.py def get_eval_dataloader ( self ) -> DataLoader : \"\"\" Returns the evaluation :class:`~torch.utils.data.DataLoader`. \"\"\" eval_dataset = self . eval_dataset if eval_dataset is None : eval_dataset = self . train_dataset return DataLoader ( dataset = eval_dataset , batch_size = 1 , ) get_train_dataloader ( self ) # Returns the training :class: ~torch.utils.data.DataLoader . Source code in super_image\\trainer.py def get_train_dataloader ( self ) -> DataLoader : \"\"\" Returns the training :class:`~torch.utils.data.DataLoader`. \"\"\" if self . train_dataset is None : raise ValueError ( \"Trainer: training requires a train_dataset.\" ) train_dataset = self . train_dataset return DataLoader ( dataset = train_dataset , batch_size = self . args . train_batch_size , shuffle = True , num_workers = self . args . dataloader_num_workers , pin_memory = self . args . dataloader_pin_memory , ) save_model ( self , output_dir = None ) # Will save the model, so you can reload it using :obj: from_pretrained() . Will only save from the main process. Source code in super_image\\trainer.py def save_model ( self , output_dir : Optional [ str ] = None ): \"\"\" Will save the model, so you can reload it using :obj:`from_pretrained()`. Will only save from the main process. \"\"\" output_dir = output_dir if output_dir is not None else self . args . output_dir os . makedirs ( output_dir , exist_ok = True ) if not isinstance ( self . model , PreTrainedModel ): # Setup scale scale = self . model . config . scale if scale is not None : weights_name = WEIGHTS_NAME_SCALE . format ( scale = scale ) else : weights_name = WEIGHTS_NAME weights = copy . deepcopy ( self . model . state_dict ()) torch . save ( weights , os . path . join ( output_dir , weights_name )) else : self . model . save_pretrained ( output_dir ) train ( self , resume_from_checkpoint = None , ** kwargs ) # Main training entry point. Parameters: Name Type Description Default resume_from_checkpoint Union[bool, str] obj: str or :obj: bool , optional ): If a :obj: str , local path to a saved checkpoint as saved by a previous instance of :class: ~super_image.Trainer . If a :obj: bool and equals True , load the last checkpoint in args.output_dir as saved by a previous instance of :class: ~super_image.Trainer . If present, training will resume from the model/optimizer/scheduler states loaded here. None kwargs Additional keyword arguments used to hide deprecated arguments {} Source code in super_image\\trainer.py def train ( self , resume_from_checkpoint : Optional [ Union [ str , bool ]] = None , ** kwargs , ): \"\"\" Main training entry point. Args: resume_from_checkpoint (:obj:`str` or :obj:`bool`, `optional`): If a :obj:`str`, local path to a saved checkpoint as saved by a previous instance of :class:`~super_image.Trainer`. If a :obj:`bool` and equals `True`, load the last checkpoint in `args.output_dir` as saved by a previous instance of :class:`~super_image.Trainer`. If present, training will resume from the model/optimizer/scheduler states loaded here. kwargs: Additional keyword arguments used to hide deprecated arguments \"\"\" args = self . args epochs_trained = 0 device = args . device num_train_epochs = args . num_train_epochs learning_rate = args . learning_rate train_batch_size = args . train_batch_size train_dataset = self . train_dataset train_dataloader = self . get_train_dataloader () step_size = int ( len ( train_dataset ) / train_batch_size * 200 ) # # Load potential model checkpoint # if isinstance(resume_from_checkpoint, bool) and resume_from_checkpoint: # resume_from_checkpoint = get_last_checkpoint(args.output_dir) # if resume_from_checkpoint is None: # raise ValueError(f\"No valid checkpoint found in output directory ({args.output_dir})\") # # if resume_from_checkpoint is not None: # if not os.path.isfile(os.path.join(resume_from_checkpoint, WEIGHTS_NAME)): # raise ValueError(f\"Can't find a valid checkpoint at {resume_from_checkpoint}\") # # logger.info(f\"Loading model from {resume_from_checkpoint}).\") # # if os.path.isfile(os.path.join(resume_from_checkpoint, CONFIG_NAME)): # config = PretrainedConfig.from_json_file(os.path.join(resume_from_checkpoint, CONFIG_NAME)) # # state_dict = torch.load(os.path.join(resume_from_checkpoint, WEIGHTS_NAME), map_location=\"cpu\") # # If the model is on the GPU, it still works! # self._load_state_dict_in_model(state_dict) # # # release memory # del state_dict optimizer = Adam ( self . model . parameters (), lr = learning_rate ) scheduler = lr_scheduler . StepLR ( optimizer , step_size = step_size , gamma = self . args . gamma ) for epoch in range ( epochs_trained , num_train_epochs ): for param_group in optimizer . param_groups : param_group [ 'lr' ] = learning_rate * ( 0.1 ** ( epoch // int ( num_train_epochs * 0.8 ))) self . model . train () epoch_losses = AverageMeter () with tqdm ( total = ( len ( train_dataset ) - len ( train_dataset ) % train_batch_size )) as t : t . set_description ( f 'epoch: { epoch } / { num_train_epochs - 1 } ' ) for data in train_dataloader : inputs , labels = data inputs = inputs . to ( device ) labels = labels . to ( device ) preds = self . model ( inputs ) criterion = nn . L1Loss () loss = criterion ( preds , labels ) epoch_losses . update ( loss . item (), len ( inputs )) optimizer . zero_grad () loss . backward () optimizer . step () scheduler . step () t . set_postfix ( loss = f ' { epoch_losses . avg : .6f } ' ) t . update ( len ( inputs )) self . eval ( epoch ) TrainingArguments # TrainingArguments is the data class of arguments which relate to the training loop itself . Parameters: Name Type Description Default output_dir obj: str ): The output directory where the model predictions and checkpoints will be written. required overwrite_output_dir obj: bool , optional , defaults to :obj: False ): If :obj: True , overwrite the content of the output directory. Use this to continue training if :obj: output_dir points to a checkpoint directory. required learning_rate obj: float , optional , defaults to 1e-4): The initial learning rate for :class: torch.optim.Adam optimizer. required gamma obj: float , optional , defaults to 0.5): The weight decay gamma to apply to the :class: torch.optim.Adam optimizer. required num_train_epochs( obj: int , optional , defaults to 1000): Total number of training epochs to perform. required save_strategy obj: str or :class: ~transformers.trainer_utils.IntervalStrategy , optional , defaults to :obj: \"steps\" ): The checkpoint save strategy to adopt during training. Possible values are: * :obj: \"no\" : No save is done during training. * :obj: \"epoch\" : Save is done at the end of each epoch. * :obj: \"steps\" : Save is done every :obj: save_steps . required save_steps obj: int , optional , defaults to 500): Number of updates steps before two checkpoint saves if :obj: save_strategy=\"steps\" . required save_total_limit obj: int , optional ): If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in :obj: output_dir . required no_cuda obj: bool , optional , defaults to :obj: False ): Whether to not use CUDA even when it is available or not. required seed obj: int , optional , defaults to 42): Random seed that will be set at the beginning of training. required fp16 obj: bool , optional , defaults to :obj: False ): Whether to use 16-bit (mixed) precision training instead of 32-bit training. required per_device_train_batch_size obj: int , optional , defaults to 16): The batch size per GPU/CPU for training. required local_rank obj: int , optional , defaults to -1): Rank of the process during distributed training. required dataloader_num_workers obj: int , optional , defaults to 0): Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. required dataloader_pin_memory obj: bool , optional , defaults to :obj: True ): Whether you want to pin memory in data loaders or not. Will default to :obj: True . required device : torch . device property readonly # The device used by this process. n_gpu property readonly # The number of GPUs used by this process. Note This will only be greater than one when you have multiple GPUs available but are not using distributed training. For distributed training, it will always be 1. train_batch_size : int property readonly # The actual batch size for training (may differ from :obj: per_device_train_batch_size in distributed training).","title":"Trainer"},{"location":"reference/trainer/#trainer","text":"The Trainer class provides an API for feature-complete training in most standard use cases. Before instantiating your Trainer , create a TrainingArguments to access all the points of customization during training.","title":"Trainer"},{"location":"reference/trainer/#trainer_1","text":"Trainer is a simple class implementing the training and eval loop for PyTorch to train a super-image model. Parameters: Name Type Description Default model class: ~super_image.PreTrainedModel or :obj: torch.nn.Module , optional ): The model to train, evaluate or use for predictions. If not provided, a model_init must be passed. .. note:: :class: ~super_image.Trainer is optimized to work with the :class: ~super_image.PreTrainedModel provided by the library. You can still use your own models defined as :obj: torch.nn.Module as long as they work the same way as the super_image models. required args class: ~super_image.TrainingArguments , optional ): The arguments to tweak for training. Will default to a basic instance of :class: ~super_image.TrainingArguments with the output_dir set to a directory named tmp_trainer in the current directory if not provided. required train_dataset obj: torch.utils.data.dataset.Dataset or :obj: torch.utils.data.dataset.IterableDataset ): The dataset to use for training. required eval_dataset obj: torch.utils.data.dataset.Dataset , optional ): The dataset to use for evaluation. required","title":"Trainer"},{"location":"reference/trainer/#super_image.trainer.Trainer.get_eval_dataloader","text":"Returns the evaluation :class: ~torch.utils.data.DataLoader . Source code in super_image\\trainer.py def get_eval_dataloader ( self ) -> DataLoader : \"\"\" Returns the evaluation :class:`~torch.utils.data.DataLoader`. \"\"\" eval_dataset = self . eval_dataset if eval_dataset is None : eval_dataset = self . train_dataset return DataLoader ( dataset = eval_dataset , batch_size = 1 , )","title":"get_eval_dataloader()"},{"location":"reference/trainer/#super_image.trainer.Trainer.get_train_dataloader","text":"Returns the training :class: ~torch.utils.data.DataLoader . Source code in super_image\\trainer.py def get_train_dataloader ( self ) -> DataLoader : \"\"\" Returns the training :class:`~torch.utils.data.DataLoader`. \"\"\" if self . train_dataset is None : raise ValueError ( \"Trainer: training requires a train_dataset.\" ) train_dataset = self . train_dataset return DataLoader ( dataset = train_dataset , batch_size = self . args . train_batch_size , shuffle = True , num_workers = self . args . dataloader_num_workers , pin_memory = self . args . dataloader_pin_memory , )","title":"get_train_dataloader()"},{"location":"reference/trainer/#super_image.trainer.Trainer.save_model","text":"Will save the model, so you can reload it using :obj: from_pretrained() . Will only save from the main process. Source code in super_image\\trainer.py def save_model ( self , output_dir : Optional [ str ] = None ): \"\"\" Will save the model, so you can reload it using :obj:`from_pretrained()`. Will only save from the main process. \"\"\" output_dir = output_dir if output_dir is not None else self . args . output_dir os . makedirs ( output_dir , exist_ok = True ) if not isinstance ( self . model , PreTrainedModel ): # Setup scale scale = self . model . config . scale if scale is not None : weights_name = WEIGHTS_NAME_SCALE . format ( scale = scale ) else : weights_name = WEIGHTS_NAME weights = copy . deepcopy ( self . model . state_dict ()) torch . save ( weights , os . path . join ( output_dir , weights_name )) else : self . model . save_pretrained ( output_dir )","title":"save_model()"},{"location":"reference/trainer/#super_image.trainer.Trainer.train","text":"Main training entry point. Parameters: Name Type Description Default resume_from_checkpoint Union[bool, str] obj: str or :obj: bool , optional ): If a :obj: str , local path to a saved checkpoint as saved by a previous instance of :class: ~super_image.Trainer . If a :obj: bool and equals True , load the last checkpoint in args.output_dir as saved by a previous instance of :class: ~super_image.Trainer . If present, training will resume from the model/optimizer/scheduler states loaded here. None kwargs Additional keyword arguments used to hide deprecated arguments {} Source code in super_image\\trainer.py def train ( self , resume_from_checkpoint : Optional [ Union [ str , bool ]] = None , ** kwargs , ): \"\"\" Main training entry point. Args: resume_from_checkpoint (:obj:`str` or :obj:`bool`, `optional`): If a :obj:`str`, local path to a saved checkpoint as saved by a previous instance of :class:`~super_image.Trainer`. If a :obj:`bool` and equals `True`, load the last checkpoint in `args.output_dir` as saved by a previous instance of :class:`~super_image.Trainer`. If present, training will resume from the model/optimizer/scheduler states loaded here. kwargs: Additional keyword arguments used to hide deprecated arguments \"\"\" args = self . args epochs_trained = 0 device = args . device num_train_epochs = args . num_train_epochs learning_rate = args . learning_rate train_batch_size = args . train_batch_size train_dataset = self . train_dataset train_dataloader = self . get_train_dataloader () step_size = int ( len ( train_dataset ) / train_batch_size * 200 ) # # Load potential model checkpoint # if isinstance(resume_from_checkpoint, bool) and resume_from_checkpoint: # resume_from_checkpoint = get_last_checkpoint(args.output_dir) # if resume_from_checkpoint is None: # raise ValueError(f\"No valid checkpoint found in output directory ({args.output_dir})\") # # if resume_from_checkpoint is not None: # if not os.path.isfile(os.path.join(resume_from_checkpoint, WEIGHTS_NAME)): # raise ValueError(f\"Can't find a valid checkpoint at {resume_from_checkpoint}\") # # logger.info(f\"Loading model from {resume_from_checkpoint}).\") # # if os.path.isfile(os.path.join(resume_from_checkpoint, CONFIG_NAME)): # config = PretrainedConfig.from_json_file(os.path.join(resume_from_checkpoint, CONFIG_NAME)) # # state_dict = torch.load(os.path.join(resume_from_checkpoint, WEIGHTS_NAME), map_location=\"cpu\") # # If the model is on the GPU, it still works! # self._load_state_dict_in_model(state_dict) # # # release memory # del state_dict optimizer = Adam ( self . model . parameters (), lr = learning_rate ) scheduler = lr_scheduler . StepLR ( optimizer , step_size = step_size , gamma = self . args . gamma ) for epoch in range ( epochs_trained , num_train_epochs ): for param_group in optimizer . param_groups : param_group [ 'lr' ] = learning_rate * ( 0.1 ** ( epoch // int ( num_train_epochs * 0.8 ))) self . model . train () epoch_losses = AverageMeter () with tqdm ( total = ( len ( train_dataset ) - len ( train_dataset ) % train_batch_size )) as t : t . set_description ( f 'epoch: { epoch } / { num_train_epochs - 1 } ' ) for data in train_dataloader : inputs , labels = data inputs = inputs . to ( device ) labels = labels . to ( device ) preds = self . model ( inputs ) criterion = nn . L1Loss () loss = criterion ( preds , labels ) epoch_losses . update ( loss . item (), len ( inputs )) optimizer . zero_grad () loss . backward () optimizer . step () scheduler . step () t . set_postfix ( loss = f ' { epoch_losses . avg : .6f } ' ) t . update ( len ( inputs )) self . eval ( epoch )","title":"train()"},{"location":"reference/trainer/#trainingarguments","text":"TrainingArguments is the data class of arguments which relate to the training loop itself . Parameters: Name Type Description Default output_dir obj: str ): The output directory where the model predictions and checkpoints will be written. required overwrite_output_dir obj: bool , optional , defaults to :obj: False ): If :obj: True , overwrite the content of the output directory. Use this to continue training if :obj: output_dir points to a checkpoint directory. required learning_rate obj: float , optional , defaults to 1e-4): The initial learning rate for :class: torch.optim.Adam optimizer. required gamma obj: float , optional , defaults to 0.5): The weight decay gamma to apply to the :class: torch.optim.Adam optimizer. required num_train_epochs( obj: int , optional , defaults to 1000): Total number of training epochs to perform. required save_strategy obj: str or :class: ~transformers.trainer_utils.IntervalStrategy , optional , defaults to :obj: \"steps\" ): The checkpoint save strategy to adopt during training. Possible values are: * :obj: \"no\" : No save is done during training. * :obj: \"epoch\" : Save is done at the end of each epoch. * :obj: \"steps\" : Save is done every :obj: save_steps . required save_steps obj: int , optional , defaults to 500): Number of updates steps before two checkpoint saves if :obj: save_strategy=\"steps\" . required save_total_limit obj: int , optional ): If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in :obj: output_dir . required no_cuda obj: bool , optional , defaults to :obj: False ): Whether to not use CUDA even when it is available or not. required seed obj: int , optional , defaults to 42): Random seed that will be set at the beginning of training. required fp16 obj: bool , optional , defaults to :obj: False ): Whether to use 16-bit (mixed) precision training instead of 32-bit training. required per_device_train_batch_size obj: int , optional , defaults to 16): The batch size per GPU/CPU for training. required local_rank obj: int , optional , defaults to -1): Rank of the process during distributed training. required dataloader_num_workers obj: int , optional , defaults to 0): Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. required dataloader_pin_memory obj: bool , optional , defaults to :obj: True ): Whether you want to pin memory in data loaders or not. Will default to :obj: True . required","title":"TrainingArguments"},{"location":"reference/trainer/#super_image.training_args.TrainingArguments.device","text":"The device used by this process.","title":"device"},{"location":"reference/trainer/#super_image.training_args.TrainingArguments.n_gpu","text":"The number of GPUs used by this process. Note This will only be greater than one when you have multiple GPUs available but are not using distributed training. For distributed training, it will always be 1.","title":"n_gpu"},{"location":"reference/trainer/#super_image.training_args.TrainingArguments.train_batch_size","text":"The actual batch size for training (may differ from :obj: per_device_train_batch_size in distributed training).","title":"train_batch_size"},{"location":"coverage/","text":".md-content { max-width: none !important; } article h1, article > a { display: none; } var coviframe = document.getElementById(\"coviframe\"); function resizeIframe() { coviframe.style.height = coviframe.contentWindow.document.documentElement.offsetHeight + 'px'; } coviframe.contentWindow.document.body.onclick = function() { coviframe.contentWindow.location.reload(); }","title":"Coverage report"}]}